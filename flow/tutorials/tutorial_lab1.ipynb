{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial ACS_UPB_LAB1: Running Sumo Simulations\n",
    "\n",
    "__Credits: most of the credits for this ipynb goes to https://github.com/flow-project/flow/tree/master/tutorials__\n",
    "\n",
    "This tutorial walks through the process of running non-RL traffic simulations in Flow. Simulations of this form act as non-autonomous baselines and depict the behavior of human dynamics on a network. Similar simulations may also be used to evaluate the performance of hand-designed controllers on a network. This tutorial focuses primarily on the former use case, while an example of the latter may be found in `exercise07_controllers.ipynb`.\n",
    "\n",
    "In this exercise, we simulate a initially perturbed single lane ring road. We witness in simulation that as time advances the initially perturbations do not dissipate, but instead propagates and expands until vehicles are forced to periodically stop and accelerate. For more information on this behavior, we refer the reader to the following article [1].\n",
    "\n",
    "## 1.1 Components of a Simulation\n",
    "All simulations, both in the presence and absence of RL, require two components: a *network*, and an *environment*. Networks describe the features of the transportation network used in simulation. This includes the positions and properties of nodes and edges constituting the lanes and junctions, as well as properties of the vehicles, traffic lights, inflows, etc. in the network. Environments, on the other hand, initialize, reset, and advance simulations, and act the primary interface between the reinforcement learning algorithm and the network. Moreover, custom environments may be used to modify the dynamical features of an network.\n",
    "\n",
    "## 1.2 Setting up the environment of current lab (ENV1)\n",
    "Load configurations for lab 1.\n",
    "\n",
    "## 2. Setting up a Network\n",
    "Flow contains a plethora of pre-designed networks used to replicate highways, intersections, and merges in both closed and open settings. All these networks are located in flow/networks. In order to recreate a ring road network, we begin by importing the network `RingNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FigureEightNetwork\n"
     ]
    }
   ],
   "source": [
    "from flow.envs.nemodrive_lab import ENV2 as ENV\n",
    "\n",
    "# from flow.networks.figure_eight import FigureEightNetwork\n",
    "network_name = ENV[\"NETWORK\"]\n",
    "print(network_name.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This network, as well as all other networks in Flow, is parametrized by the following arguments: \n",
    "* name\n",
    "* vehicles\n",
    "* net_params\n",
    "* initial_config\n",
    "* traffic_lights\n",
    "\n",
    "These parameters allow a single network to be recycled for a multitude of different network settings. For example, `RingNetwork` may be used to create ring roads of variable length with a variable number of lanes and vehicles.\n",
    "\n",
    "### 2.1 Name\n",
    "The `name` argument is a string variable depicting the name of the network. This has no effect on the type of network created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = network_name.__name__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 VehicleParams\n",
    "The `VehicleParams` class stores state information on all vehicles in the network. This class is used to identify the dynamical behavior of a vehicle and whether it is controlled by a reinforcement learning agent. Morover, information pertaining to the observations and reward function can be collected from various get methods within this class.\n",
    "\n",
    "The initial configuration of this class describes the number of vehicles in the network at the start of every simulation, as well as the properties of these vehicles. We begin by creating an empty `VehicleParams` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicles = ENV[\"VEHICLES\"]()\n",
    "\n",
    "# code in get_vehicles \n",
    "# from flow.core.params import VehicleParams\n",
    "\n",
    "# vehicles = VehicleParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this object is created, vehicles may be introduced using the `add` method. This method specifies the types and quantities of vehicles at the start of a simulation rollout. For a description of the various arguements associated with the `add` method, we refer the reader to the following documentation ([VehicleParams.add](https://flow.readthedocs.io/en/latest/flow.core.html?highlight=vehicleparam#flow.core.params.VehicleParams)).\n",
    "\n",
    "When adding vehicles, their dynamical behaviors may be specified either by the simulator (default), or by user-generated models. For longitudinal (acceleration) dynamics, several prominent car-following models are implemented in Flow. For this example, the acceleration behavior of all vehicles will be defined by the Intelligent Driver Model (IDM) [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in get_vehicles \n",
    "# from flow.controllers.car_following_models import IDMController"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another controller we define is for the vehicle's routing behavior. For closed network where the route for any vehicle is repeated, the `ContinuousRouter` controller is used to perpetually reroute all vehicles to the initial set route."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in get_vehicles \n",
    "# from flow.controllers.routing_controllers import ContinuousRouter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we add 22 vehicles of type \"human\" with the above acceleration and routing behavior into the `Vehicles` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (E.g. code in get_vehicles)\n",
    "# vehicles.add(\"human\",\n",
    "#              acceleration_controller=(IDMController, {}),\n",
    "#              routing_controller=(ContinuousRouter, {}),\n",
    "#              num_vehicles=22)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 NetParams\n",
    "\n",
    "`NetParams` are network-specific parameters used to define the shape and properties of a network. Unlike most other parameters, `NetParams` may vary drastically depending on the specific network configuration, and accordingly most of its parameters are stored in `additional_params`. In order to determine which `additional_params` variables may be needed for a specific network, we refer to the `ADDITIONAL_NET_PARAMS` variable located in the network file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'radius_ring': 60, 'lanes': 2, 'speed_limit': 30, 'resolution': 40}\n"
     ]
    }
   ],
   "source": [
    "# from flow.networks.ring import ADDITIONAL_NET_PARAMS\n",
    "\n",
    "ADDITIONAL_NET_PARAMS = ENV[\"ADDITIONAL_NET_PARAMS\"]\n",
    "\n",
    "print(ADDITIONAL_NET_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `ADDITIONAL_NET_PARAMS` dict from the ring road network, we see that the required parameters are:\n",
    "\n",
    "* **length**: length of the ring road\n",
    "* **lanes**: number of lanes\n",
    "* **speed**: speed limit for all edges\n",
    "* **resolution**: resolution of the curves on the ring. Setting this value to 1 converts the ring to a diamond.\n",
    "\n",
    "\n",
    "At times, other inputs may be needed from `NetParams` to recreate proper network features/behavior. These requirements can be founded in the network's documentation. For the ring road, no attributes are needed aside from the `additional_params` terms. Furthermore, for this exercise, we use the network's default parameters when creating the `NetParams` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import NetParams\n",
    "\n",
    "net_params = NetParams(additional_params=ADDITIONAL_NET_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 InitialConfig\n",
    "\n",
    "`InitialConfig` specifies parameters that affect the positioning of vehicle in the network at the start of a simulation. These parameters can be used to limit the edges and number of lanes vehicles originally occupy, and provide a means of adding randomness to the starting positions of vehicles. In order to introduce a small initial disturbance to the system of vehicles in the network, we set the `perturbation` term in `InitialConfig` to 1m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spacing': 'random', 'perturbation': 50}\n"
     ]
    }
   ],
   "source": [
    "from flow.core.params import InitialConfig\n",
    "initial_config_param = ENV[\"INITIAL_CONFIG_PARAMS\"]\n",
    "print(initial_config_param)\n",
    "\n",
    "initial_config = InitialConfig(**initial_config_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 TrafficLightParams\n",
    "\n",
    "`TrafficLightParams` are used to describe the positions and types of traffic lights in the network. These inputs are outside the scope of this tutorial, and instead are covered in `exercise06_traffic_lights.ipynb`. For our example, we create an empty `TrafficLightParams` object, thereby ensuring that none are placed on any nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import TrafficLightParams\n",
    "\n",
    "traffic_lights = TrafficLightParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Setting up an Environment\n",
    "\n",
    "Several envionrments in Flow exist to train autonomous agents of different forms (e.g. autonomous vehicles, traffic lights) to perform a variety of different tasks. These environments are often network or task specific; however, some can be deployed on an ambiguous set of networks as well. One such environment, `AccelEnv`, may be used to train a variable number of vehicles in a fully observable network with a *static* number of vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'flow.envs.nemodrive_lab.env2_lab.LaneChangeAccelEnv2'>\n"
     ]
    }
   ],
   "source": [
    "# from flow.envs.nemodrive_lab.env1_lab import LaneChangeAccelEnv1\n",
    "env_name = ENV[\"ENVIRONMENT\"]\n",
    "print(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we will not be training any autonomous agents in this exercise, the use of an environment allows us to view the cumulative reward simulation rollouts receive in the absence of autonomy.\n",
    "\n",
    "Envrionments in Flow are parametrized by three components:\n",
    "* `EnvParams`\n",
    "* `SumoParams`\n",
    "* `Network`\n",
    "\n",
    "### 3.1 SumoParams\n",
    "`SumoParams` specifies simulation-specific variables. These variables include the length a simulation step (in seconds) and whether to render the GUI when running the experiment. For this example, we consider a simulation step length of 0.1s and activate the GUI.\n",
    "\n",
    "Another useful parameter is `emission_path`, which is used to specify the path where the emissions output will be generated. They contain a lot of information about the simulation, for instance the position and speed of each car at each time step. If you do not specify any emission path, the emission file will not be generated. More on this in Section 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import SumoParams\n",
    "\n",
    "sumo_params = SumoParams(sim_step=0.1, render=True, emission_path='data', restart_instance=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 EnvParams\n",
    "\n",
    "`EnvParams` specify environment and experiment-specific parameters that either affect the training process or the dynamics of various components within the network. Much like `NetParams`, the attributes associated with this parameter are mostly environment specific, and can be found in the environment's `ADDITIONAL_ENV_PARAMS` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_accel': 3, 'max_decel': 3, 'lane_change_duration': 0, 'target_velocity': 10, 'sort_vehicles': False, 'forward_progress_gain': 0.1, 'collision_reward': -1, 'lane_change_reward': -0.1, 'frontal_collision_distance': 2.0, 'lateral_collision_distance': 3.0, 'action_space_box': False, 'pos_noise_std': [0.5, 2], 'pos_noise_steps_reset': 100, 'speed_noise_std': [0.2, 0.8], 'acc_noise_std': [0.2, 0.4]}\n"
     ]
    }
   ],
   "source": [
    "# from flow.envs.nemodrive_lab.env1_lab import ADDITIONAL_ENV1_PARAMS\n",
    "ADDITIONAL_ENV_PARAMS = ENV[\"ADDITIONAL_ENV_PARAMS\"]\n",
    "\n",
    "print(ADDITIONAL_ENV_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the `ADDITIONAL_ENV_PARAMS` variable, we see that it consists of only one entry, \"target_velocity\", which is used when computing the reward function associated with the environment. We use this default value when generating the `EnvParams` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.params import EnvParams\n",
    "\n",
    "env_params = EnvParams(additional_params=ADDITIONAL_ENV_PARAMS, horizon=ENV[\"HORIZON\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting up and Running the Experiment\n",
    "Once the inputs to the network and environment classes are ready, we are ready to set up a `Experiment` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flow.core.experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These objects may be used to simulate rollouts in the absence of reinforcement learning agents, as well as acquire behaviors and rewards that may be used as a baseline with which to compare the performance of the learning agent. In this case, we choose to run our experiment for one rollout consisting of 3000 steps (300 s).\n",
    "\n",
    "**Note**: When executing the below code, remeber to click on the    <img style=\"display:inline;\" src=\"img/play_button.png\"> Play button after the GUI is rendered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the network object\n",
    "network = network_name(name=\"ring_example\",\n",
    "                       vehicles=vehicles,\n",
    "                       net_params=net_params,\n",
    "                       initial_config=initial_config,\n",
    "                       traffic_lights=traffic_lights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FatalTraCIError",
     "evalue": "connection closed by SUMO",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFatalTraCIError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4070cc562e07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# create the experiment object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_to_csv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Projects/AAIT/flow/flow/core/experiment.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, num_runs, num_steps, rl_actions, convert_to_csv)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrl_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m                 vel[j] = np.mean(\n\u001b[1;32m    122\u001b[0m                     self.env.k.vehicle.get_speed(self.env.k.vehicle.get_ids()))\n",
      "\u001b[0;32m~/Projects/AAIT/flow/flow/envs/base.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, rl_actions)\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# advance the simulation in the simulator by one step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# store new observations in the vehicles and traffic lights class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/AAIT/flow/flow/core/kernel/simulation/traci.py\u001b[0m in \u001b[0;36msimulation_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msimulation_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;34m\"\"\"See parent class.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulationStep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36msimulationStep\u001b[0;34m(self, step)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_string\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!BBd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCMD_SIMSTEP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sendExact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubscriptionResults\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_subscriptionMapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0msubscriptionResults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Projects/sumo/tools/traci/connection.py\u001b[0m in \u001b[0;36m_sendExact\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mFatalTraCIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"connection closed by SUMO\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcommand\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"!BBB\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFatalTraCIError\u001b[0m: connection closed by SUMO"
     ]
    }
   ],
   "source": [
    "# create the environment object\n",
    "sumo_params.render = True\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "# create the experiment object\n",
    "exp = Experiment(env)\n",
    "_ = exp.run(1, 3000, convert_to_csv=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run still agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: -3000.0\n",
      "Average, std return: -3000.0, 0.0\n",
      "Average, std speed: 4.8421383321199025, 0.0\n"
     ]
    }
   ],
   "source": [
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "# create the experiment object\n",
    "exp = Experiment(env)\n",
    "\n",
    "rl_actions = lambda state: [0, 0]\n",
    "\n",
    "_ = exp.run(1, 3000, convert_to_csv=True, rl_actions=rl_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run random agent.\n",
    "\n",
    "Use __FullExperiment__ to test agent that expects _state, reward, done, info_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: -213.70367132378917\n",
      "Round 1, return: -1128.7147838285787\n",
      "Round 2, return: -627.554034545858\n",
      "Round 3, return: -681.9083110341325\n",
      "Round 4, return: -119.14582699761802\n",
      "Round 5, return: -446.7533413121279\n",
      "Round 6, return: -425.7369412641068\n",
      "Round 7, return: -475.6252823138319\n",
      "Round 8, return: -716.9215183259731\n",
      "Round 9, return: -841.8928955496672\n",
      "Average, std return: -567.7956606495684, 282.592630864858\n",
      "Average, std speed: 5.680333411899924, 0.7670237487883679\n"
     ]
    }
   ],
   "source": [
    "from flow.core.experiment_with_reward import FullExperiment\n",
    "import numpy as np\n",
    "\n",
    "class RandomAgent():\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.max_decel = env.env_params.additional_params[\"max_decel\"]\n",
    "        self.max_accel = env.env_params.additional_params[\"max_accel\"]\n",
    "        self.change_lane_step_freq = 1\n",
    "        self.num_steps = 0\n",
    "        \n",
    "    def act(self, state, reward, done, info):\n",
    "        self.num_steps += 1\n",
    "        d = 0\n",
    "        if self.num_steps % self.change_lane_step_freq == 0:\n",
    "            d = np.random.randint(3)\n",
    "\n",
    "        acc = np.random.uniform(-self.max_decel, self.max_accel)\n",
    "        action =  np.array([acc, d])\n",
    "        yield action\n",
    "\n",
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "exp = FullExperiment(env)\n",
    "\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "_ = exp.run(10, 3000, convert_to_csv=True, rl_actions=agent.act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results RandomRun:\n",
    "\n",
    "Round 0, return: -213.70367132378917\n",
    "\n",
    "Round 1, return: -1128.7147838285787\n",
    "\n",
    "Round 2, return: -627.554034545858\n",
    "\n",
    "Round 3, return: -681.9083110341325\n",
    "\n",
    "Round 4, return: -119.14582699761802\n",
    "\n",
    "Round 5, return: -446.7533413121279\n",
    "\n",
    "Round 6, return: -425.7369412641068\n",
    "\n",
    "Round 7, return: -475.6252823138319\n",
    "\n",
    "Round 8, return: -716.9215183259731\n",
    "\n",
    "Round 9, return: -841.8928955496672\n",
    "\n",
    "Average, std return: -567.7956606495684, 282.592630864858\n",
    "\n",
    "Average, std speed: 5.680333411899924, 0.7670237487883679\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: 368.22066219162355\n",
      "Round 1, return: 169.9672823130634\n",
      "Round 2, return: 393.0464766466746\n",
      "Round 3, return: 483.6021698201408\n",
      "Round 4, return: 686.6617658491106\n",
      "Round 5, return: 169.01411824244326\n",
      "Round 6, return: 175.21081685515858\n",
      "Round 7, return: 207.65770702442296\n",
      "Round 8, return: 243.2925879221222\n",
      "Round 9, return: 452.8956719968303\n",
      "Average, std return: 334.95692588615896, 164.03261325573877\n",
      "Average, std speed: 6.327870921677684, 0.22107667209877344\n"
     ]
    }
   ],
   "source": [
    "from flow.core.experiment_with_reward import FullExperiment\n",
    "import numpy as np\n",
    "\n",
    "class PIDAgent():\n",
    "    def __init__(self, Kp, Ki, Kd, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.max_decel = env.env_params.additional_params[\"max_decel\"]\n",
    "        self.max_accel = env.env_params.additional_params[\"max_accel\"]\n",
    "        self.change_lane_step_freq = 2\n",
    "        self.num_steps = 0\n",
    "        self.Ki = Ki\n",
    "        self.Kp = Kp\n",
    "        self.Kd = Kd\n",
    "        self.vd = env.env_params.additional_params[\"target_velocity\"]\n",
    "        self.env = env\n",
    "        \n",
    "        self.distance_check = 6\n",
    "        self.prev_speed = 0\n",
    "        self.sum_err = 0\n",
    "    @property   \n",
    "    def vehicle_id(self):\n",
    "        return \"rl_0\"\n",
    "    @property\n",
    "    def lane(self):\n",
    "        return self.env.k.vehicle.get_lane(self.vehicle_id)\n",
    "\n",
    "    def lane_change_check(self):\n",
    "        \n",
    "        closest_dist = self.env.k.vehicle.get_headway(self.vehicle_id)\n",
    "        safe_distance = self.env.env_params.additional_params[\"frontal_collision_distance\"] * self.distance_check\n",
    "        \n",
    "        \n",
    "        myedge = self.env.k.vehicle.get_edge(self.vehicle_id)        \n",
    "        nr_lanes = self.env.k.network.num_lanes(myedge)\n",
    "        \n",
    "        if (nr_lanes > 1) and (closest_dist > 0) and (closest_dist < safe_distance):\n",
    "            #check other lanes \n",
    "            #import pdb; pdb.set_trace()\n",
    "            otherlanes = self.env.k.vehicle.get_lane_headways(self.vehicle_id)\n",
    "            otherlane = self.lane+1 if self.lane<nr_lanes-1 else self.lane-1\n",
    "            dist = otherlanes[otherlane]\n",
    "            \n",
    "            if closest_dist < dist and dist>safe_distance:\n",
    "                return 1 if self.lane == nr_lanes-1 else 2\n",
    "       \n",
    "        return 0 # no lane change\n",
    "        \n",
    "    def act(self, state, reward, done, info):\n",
    "        self.num_steps += 1\n",
    "\n",
    "        v = self.env.k.vehicle.get_speed(self.vehicle_id)\n",
    "        \n",
    "        dv = self.prev_speed - v\n",
    "        \n",
    "        self.sum_err += self.vd - v\n",
    "        \n",
    "        acc = self.Kp*(self.vd - v) + self.Kd*dv + self.Ki*self.sum_err\n",
    "        \n",
    "        acc = np.clip(acc, -self.max_decel, self.max_accel)\n",
    "        \n",
    "        self.prev_speed = v\n",
    "        d = 0\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if self.num_steps % self.change_lane_step_freq == 0:\n",
    "            d = self.lane_change_check()\n",
    "\n",
    "        action =  np.array([acc, d])\n",
    "        yield action\n",
    "\n",
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "exp = FullExperiment(env)\n",
    "\n",
    "agent = PIDAgent(env=env,\n",
    "                Ki=0.004,\n",
    "                Kp=0.2,\n",
    "                Kd=0.3)\n",
    "\n",
    "_ = exp.run(10, 3000, convert_to_csv=True, rl_actions=agent.act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results PID Run:\n",
    "\n",
    "Round 0, return: 368.22066219162355\n",
    "\n",
    "Round 1, return: 169.9672823130634\n",
    "\n",
    "Round 2, return: 393.0464766466746\n",
    "\n",
    "Round 3, return: 483.6021698201408\n",
    "\n",
    "Round 4, return: 686.6617658491106\n",
    "\n",
    "Round 5, return: 169.01411824244326\n",
    "\n",
    "Round 6, return: 175.21081685515858\n",
    "\n",
    "Round 7, return: 207.65770702442296\n",
    "\n",
    "Round 8, return: 243.2925879221222\n",
    "\n",
    "Round 9, return: 452.8956719968303\n",
    "\n",
    "Average, std return: 334.95692588615896, 164.03261325573877\n",
    "\n",
    "Average, std speed: 6.327870921677684, 0.22107667209877344"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<flow.core.params.EnvParams at 0x7f9b01f7c5c0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0, return: -194.25226637140568\n",
      "Round 1, return: 645.1726348031408\n",
      "Round 2, return: -8.057047273020647\n",
      "Round 3, return: 127.11625496919676\n",
      "Round 4, return: 131.45050575089874\n",
      "Round 5, return: 75.54286546257498\n",
      "Round 6, return: 675.3547626437601\n",
      "Round 7, return: -341.11844498315025\n",
      "Round 8, return: -774.20189448934\n",
      "Round 9, return: 308.22323390325136\n",
      "Average, std return: 64.52306044159063, 414.537672709776\n",
      "Average, std speed: 7.426823532001501, 0.2581054114423834\n"
     ]
    }
   ],
   "source": [
    "from flow.core.experiment_with_reward import FullExperiment\n",
    "import numpy as np\n",
    "\n",
    "class PIDAgent():\n",
    "    def __init__(self, Kp, Ki, Kd, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.max_decel = env.env_params.additional_params[\"max_decel\"]\n",
    "        self.max_accel = env.env_params.additional_params[\"max_accel\"]\n",
    "        self.change_lane_step_freq = 2\n",
    "        self.num_steps = 0\n",
    "        self.Ki = Ki\n",
    "        self.Kp = Kp\n",
    "        self.Kd = Kd\n",
    "        self.vd = env.env_params.additional_params[\"target_velocity\"]\n",
    "        self.env = env\n",
    "        \n",
    "        self.distance_check = 6\n",
    "        self.prev_speed = 0\n",
    "        self.sum_err = 0\n",
    "    @property   \n",
    "    def vehicle_id(self):\n",
    "        return \"rl_0\"\n",
    "    @property\n",
    "    def lane(self):\n",
    "        return self.env.k.vehicle.get_lane(self.vehicle_id)\n",
    "\n",
    "    def lane_change_check(self):\n",
    "        \n",
    "        closest_dist = self.env.k.vehicle.get_headway(self.vehicle_id)\n",
    "        safe_distance = self.env.env_params.additional_params[\"frontal_collision_distance\"] * self.distance_check\n",
    "        \n",
    "        \n",
    "        myedge = self.env.k.vehicle.get_edge(self.vehicle_id)        \n",
    "        nr_lanes = self.env.k.network.num_lanes(myedge)\n",
    "        \n",
    "        if (nr_lanes > 1) and (closest_dist > 0) and (closest_dist < safe_distance):\n",
    "            #check other lanes \n",
    "            #import pdb; pdb.set_trace()\n",
    "            otherlanes = self.env.k.vehicle.get_lane_headways(self.vehicle_id)\n",
    "            otherlane = self.lane+1 if self.lane<nr_lanes-1 else self.lane-1\n",
    "            dist = otherlanes[otherlane]\n",
    "            \n",
    "            if closest_dist < dist and dist>safe_distance:\n",
    "                return 1 if self.lane == nr_lanes-1 else 2\n",
    "       \n",
    "        return 0 # no lane change\n",
    "        \n",
    "    def act(self, state, reward, done, info):\n",
    "        self.num_steps += 1\n",
    "\n",
    "        v = self.env.k.vehicle.get_speed(self.vehicle_id)\n",
    "        \n",
    "        dv = self.prev_speed - v\n",
    "        \n",
    "        self.sum_err += self.vd - v\n",
    "        \n",
    "        acc = self.Kp*(self.vd - v) + self.Kd*dv + self.Ki*self.sum_err\n",
    "        \n",
    "        acc = np.clip(acc, -self.max_decel, self.max_accel)\n",
    "        \n",
    "        self.prev_speed = v\n",
    "        d = 0\n",
    "        #import pdb; pdb.set_trace()\n",
    "        if self.num_steps % self.change_lane_step_freq == 0:\n",
    "            d = self.lane_change_check()\n",
    "\n",
    "        action =  np.array([acc, d])\n",
    "        yield action\n",
    "\n",
    "sumo_params.render = False\n",
    "env = env_name(env_params, sumo_params, network)\n",
    "\n",
    "exp = FullExperiment(env)\n",
    "\n",
    "agent = PIDAgent(env=env,\n",
    "                Ki=0.004,\n",
    "                Kp=0.2,\n",
    "                Kd=0.3)\n",
    "\n",
    "_ = exp.run(10, 3000, convert_to_csv=True, rl_actions=agent.act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env2\n",
    "\n",
    "Round 0, return: -194.25226637140568\n",
    "\n",
    "Round 1, return: 645.1726348031408\n",
    "\n",
    "Round 2, return: -8.057047273020647\n",
    "\n",
    "Round 3, return: 127.11625496919676\n",
    "\n",
    "Round 4, return: 131.45050575089874\n",
    "\n",
    "Round 5, return: 75.54286546257498\n",
    "\n",
    "Round 6, return: 675.3547626437601\n",
    "\n",
    "Round 7, return: -341.11844498315025\n",
    "\n",
    "Round 8, return: -774.20189448934\n",
    "\n",
    "Round 9, return: 308.22323390325136\n",
    "\n",
    "Average, std return: 64.52306044159063, 414.537672709776\n",
    "\n",
    "Average, std speed: 7.426823532001501, 0.2581054114423834\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Feel free to experiment with all these problems and more!\n",
    "\n",
    "## Bibliography\n",
    "[1] Sugiyama, Yuki, et al. \"Traffic jams without bottlenecksâ€”experimental evidence for the physical mechanism of the formation of a jam.\" New journal of physics 10.3 (2008): 033001.\n",
    "\n",
    "[2] Treiber, Martin, Ansgar Hennecke, and Dirk Helbing. \"Congested traffic states in empirical observations and microscopic simulations.\" Physical review E 62.2 (2000): 1805."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Setting up Flow Parameters\n",
    "\n",
    "RLlib experiments both generate a `params.json` file for each experiment run. For RLlib experiments, the parameters defining the Flow network and environment must be stored as well. As such, in this section we define the dictionary `flow_params`, which contains the variables required by the utility function `make_create_env`. `make_create_env` is a higher-order function which returns a function `create_env` that initializes a Gym environment corresponding to the Flow network specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating flow_params. Make sure the dictionary keys are as specified. \n",
    "sumo_params.render = False\n",
    "sumo_params.print_warnings=False\n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=name,\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=env_name,\n",
    "    # name of the network class the experiment uses\n",
    "    network=network_name,\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=sumo_params,\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=env_params,\n",
    "    # network-related parameters (see flow.core.params.NetParams and\n",
    "    # the network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "    # vehicles to be placed in the network at the start of a rollout \n",
    "    # (see flow.core.vehicles.Vehicles)\n",
    "    veh=vehicles,\n",
    "    # (optional) parameters affecting the positioning of vehicles upon \n",
    "    # initialization/reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exp_tag': 'FigureEightNetwork',\n",
       " 'env_name': flow.envs.nemodrive_lab.env1_lab.LaneChangeAccelEnv1,\n",
       " 'network': flow.networks.figure_eight.FigureEightNetwork,\n",
       " 'simulator': 'traci',\n",
       " 'sim': <flow.core.params.SumoParams at 0x7f4eb0406f28>,\n",
       " 'env': <flow.core.params.EnvParams at 0x7f4eb0412128>,\n",
       " 'net': <flow.core.params.NetParams at 0x7f4eb0406780>,\n",
       " 'veh': <flow.core.params.VehicleParams at 0x7f4ee4654390>,\n",
       " 'initial': <flow.core.params.InitialConfig at 0x7f4eb0406cc0>}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow_params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Running RL experiments in Ray\n",
    "\n",
    "### 4.1 Import \n",
    "\n",
    "First, we must import modules required to run experiments in Ray. The `json` package is required to store the Flow experiment parameters in the `params.json` file, as is `FlowParamsEncoder`. Ray-related imports are required: the PPO algorithm agent, `ray.tune`'s experiment runner, and environment helper methods `register_env` and `make_create_env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "import ray\n",
    "try:\n",
    "    from ray.rllib.agents.agent import get_agent_class\n",
    "except ImportError:\n",
    "    from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Initializing Ray\n",
    "Here, we initialize Ray and experiment-based constant variables specifying parallelism in the experiment as well as experiment batch size in terms of number of rollouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 19:32:16,913\tINFO node.py:498 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-12-07_19-32-16_912746_21740/logs.\n",
      "2019-12-07 19:32:17,078\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:45340 to respond...\n",
      "2019-12-07 19:32:17,231\tINFO services.py:409 -- Waiting for redis server at 127.0.0.1:14915 to respond...\n",
      "2019-12-07 19:32:17,235\tINFO services.py:809 -- Starting Redis shard with 1.67 GB max memory.\n",
      "2019-12-07 19:32:17,352\tINFO node.py:512 -- Process STDOUT and STDERR is being redirected to /tmp/ray/session_2019-12-07_19-32-16_912746_21740/logs.\n",
      "2019-12-07 19:32:17,369\tINFO services.py:1475 -- Starting the Plasma object store with 2.5 GB memory using /dev/shm.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.188',\n",
       " 'redis_address': '192.168.1.188:45340',\n",
       " 'object_store_address': '/tmp/ray/session_2019-12-07_19-32-16_912746_21740/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2019-12-07_19-32-16_912746_21740/sockets/raylet',\n",
       " 'webui_url': None,\n",
       " 'session_dir': '/tmp/ray/session_2019-12-07_19-32-16_912746_21740'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of parallel workers\n",
    "N_CPUS = 8\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 20\n",
    "\n",
    "ray.init(num_cpus=N_CPUS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Configuration and Setup\n",
    "Here, we copy and modify the default configuration for the [PPO algorithm](https://arxiv.org/abs/1707.06347). The agent has the number of parallel workers specified, a batch size corresponding to `N_ROLLOUTS` rollouts (each of which has length `HORIZON` steps), a discount rate $\\gamma$ of 0.999, two hidden layers of size 16, uses Generalized Advantage Estimation, $\\lambda$ of 0.97, and other parameters as set below.\n",
    "\n",
    "Once `config` contains the desired parameters, a JSON string corresponding to the `flow_params` specified in section 3 is generated. The `FlowParamsEncoder` maps objects to string representations so that the experiment can be reproduced later. That string representation is stored within the `env_config` section of the `config` dictionary. Later, `config` is written out to the file `params.json`. \n",
    "\n",
    "Next, we call `make_create_env` and pass in the `flow_params` to return a function we can use to register our Flow environment with Gym. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The algorithm or model to train. This may refer to \"\n",
    "#      \"the name of a built-on algorithm (e.g. RLLib's DQN \"\n",
    "#      \"or PPO), or a user-defined trainable function or \"\n",
    "#      \"class registered in the tune registry.\")\n",
    "alg_run = \"PPO\"\n",
    "HORIZON = 100\n",
    "\n",
    "agent_cls = get_agent_class(alg_run)\n",
    "config = agent_cls._default_config.copy()\n",
    "config[\"num_workers\"] = N_CPUS - 1  # number of parallel workers\n",
    "config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS  # batch size\n",
    "config[\"gamma\"] = 0.999  # discount rate\n",
    "config[\"model\"].update({\"fcnet_hiddens\": [16, 16]})  # size of hidden layers in network\n",
    "config[\"use_gae\"] = True  # using generalized advantage estimation\n",
    "config[\"lambda\"] = 0.97  \n",
    "config[\"sgd_minibatch_size\"] = min(16 * 1024, config[\"train_batch_size\"])  # stochastic gradient descent\n",
    "config[\"kl_target\"] = 0.02  # target KL divergence\n",
    "config[\"num_sgd_iter\"] = 500  # number of SGD iterations\n",
    "config[\"horizon\"] = HORIZON  # rollout horizon\n",
    "\n",
    "# save the flow params for replay\n",
    "flow_json = json.dumps(flow_params, cls=FlowParamsEncoder, sort_keys=True,\n",
    "                       indent=4)  # generating a string version of flow_params\n",
    "config['env_config']['flow_params'] = flow_json  # adding the flow_params to config dict\n",
    "config['env_config']['run'] = alg_run\n",
    "\n",
    "# Call the utility function make_create_env to be able to \n",
    "# register the Flow env for this experiment\n",
    "create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "# Register as rllib env with Gym\n",
    "register_env(gym_name, create_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Running Experiments\n",
    "\n",
    "Here, we use the `run_experiments` function from `ray.tune`. The function takes a dictionary with one key, a name corresponding to the experiment, and one value, itself a dictionary containing parameters for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 19:32:48,152\tINFO trial_runner.py:176 -- Starting a new experiment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.2/8.3 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 19:32:48,815\tWARNING signature.py:108 -- The function with_updates has a **kwargs argument, which is currently not supported.\n",
      "2019-12-07 19:32:48,833\tERROR log_sync.py:34 -- Log sync requires cluster to be setup with `ray up`.\n",
      "2019-12-07 19:32:48,924\tWARNING util.py:145 -- The `start_trial` operation took 0.11248040199279785 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 3.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:32:52,357\tWARNING ppo.py:143 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:00,577\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:00.581416: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:00,805\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:03,703\tINFO rollout_worker.py:742 -- Built policy map: {'default_policy': <ray.rllib.policy.tf_policy_template.PPOTFPolicy object at 0x7fa296a20c88>}\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:03,703\tINFO rollout_worker.py:743 -- Built preprocessor map: {'default_policy': <ray.rllib.models.preprocessors.NoPreprocessor object at 0x7fa296a20940>}\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:03,703\tINFO rollout_worker.py:356 -- Built filter map: {'default_policy': <ray.rllib.utils.filter.NoFilter object at 0x7fa296a207f0>}\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:04,214\tINFO multi_gpu_optimizer.py:93 -- LocalMultiGPUOptimizer devices ['/cpu:0']\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:33:34,841\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 1 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:33:34.969192: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:33:37,838\tINFO dynamic_tf_policy.py:324 -- Initializing loss function with dummy input:\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m { 'action_prob': <tf.Tensor 'default_policy/action_prob:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'actions': <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'advantages': <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'behaviour_logits': <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'dones': <tf.Tensor 'default_policy/dones:0' shape=(?,) dtype=bool>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'new_obs': <tf.Tensor 'default_policy/new_obs:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'obs': <tf.Tensor 'default_policy/observation:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'prev_actions': <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'prev_rewards': <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'rewards': <tf.Tensor 'default_policy/rewards:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'value_targets': <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'vf_preds': <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m 2019-12-07 19:33:42,078\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 2 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m 2019-12-07 19:33:42.219435: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m 2019-12-07 19:33:47,793\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 3 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m 2019-12-07 19:33:47.935668: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m 2019-12-07 19:33:53,980\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 4 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m 2019-12-07 19:33:54.101937: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:54,980\tINFO trainable.py:105 -- _setup took 63.316 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:33:54,981\tWARNING util.py:47 -- Install gputil for GPU system monitoring.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m 2019-12-07 19:34:00,802\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 5 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m 2019-12-07 19:34:01.205922: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:07,030\tINFO rollout_worker.py:451 -- Generating sample batch of size 200\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m 2019-12-07 19:34:08,281\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 6 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m 2019-12-07 19:34:08.431158: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m 2019-12-07 19:34:15,449\tINFO rollout_worker.py:319 -- Creating policy evaluation worker 7 on CPU (please ignore any CUDA init errors)\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m 2019-12-07 19:34:15.697468: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:18,632\tINFO sampler.py:304 -- Raw obs from env: { 0: { 'agent0': np.ndarray((69,), dtype=float64, min=0.0, max=0.982, mean=0.184)}}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:18,644\tINFO sampler.py:305 -- Info return from env: {0: {'agent0': None}}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:18,645\tINFO sampler.py:403 -- Preprocessed obs: np.ndarray((69,), dtype=float64, min=0.0, max=0.982, mean=0.184)\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:18,645\tINFO sampler.py:407 -- Filtered obs: np.ndarray((69,), dtype=float64, min=0.0, max=0.982, mean=0.184)\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:18,664\tINFO sampler.py:521 -- Inputs to compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m { 'default_policy': [ { 'data': { 'agent_id': 'agent0',\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                   'env_id': 0,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                   'info': None,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                   'obs': np.ndarray((69,), dtype=float64, min=0.0, max=0.982, mean=0.184),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                   'prev_action': np.ndarray((2,), dtype=float64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                   'prev_reward': 0.0,\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                   'rnn_state': []},\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'type': 'PolicyEvalData'}]}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:18,676\tINFO tf_run_builder.py:92 -- Executing TF run without tracing. To dump TF timeline traces to disk, set the TF_TIMELINE_DIR environment variable.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:19,692\tINFO sampler.py:548 -- Outputs of compute_actions():\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m { 'default_policy': ( { 'data': { 'batches': [ np.ndarray((1, 1), dtype=float32, min=0.859, max=0.859, mean=0.859),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                                                np.ndarray((1,), dtype=int64, min=0.0, max=0.0, mean=0.0)]},\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'type': 'TupleActions'},\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                       [],\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                       { 'action_prob': np.ndarray((1,), dtype=float32, min=0.092, max=0.092, mean=0.092),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'behaviour_logits': np.ndarray((1, 5), dtype=float32, min=-0.003, max=0.002, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'vf_preds': np.ndarray((1,), dtype=float32, min=0.001, max=0.001, mean=0.001)})}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:64: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m WARNING:tensorflow:From /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/ray/rllib/models/tf/tf_action_dist.py:69: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Instructions for updating:\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m keep_dims is deprecated, use keepdims instead\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m /home/osboxes/anaconda3/envs/flow/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:100: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m   \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:34:45,535\tINFO sample_batch_builder.py:161 -- Trajectory fragment after postprocess_trajectory():\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m { 'agent0': { 'data': { 'action_prob': np.ndarray((100,), dtype=float32, min=0.007, max=0.133, mean=0.097),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'actions': np.ndarray((100, 2), dtype=float32, min=-2.163, max=2.452, mean=0.579),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'advantages': np.ndarray((100,), dtype=float32, min=-10.364, max=0.187, mean=-7.098),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'agent_index': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'behaviour_logits': np.ndarray((100, 5), dtype=float32, min=-0.003, max=0.002, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'dones': np.ndarray((100,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'eps_id': np.ndarray((100,), dtype=int64, min=1905261938.0, max=1905261938.0, mean=1905261938.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'infos': np.ndarray((100,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'new_obs': np.ndarray((100, 69), dtype=float32, min=0.0, max=0.985, mean=0.197),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'obs': np.ndarray((100, 69), dtype=float32, min=0.0, max=0.985, mean=0.197),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'prev_actions': np.ndarray((100, 2), dtype=float32, min=-2.163, max=2.452, mean=0.571),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'prev_rewards': np.ndarray((100,), dtype=float32, min=-0.991, max=0.109, mean=-0.298),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'rewards': np.ndarray((100,), dtype=float32, min=-0.991, max=0.109, mean=-0.297),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         't': np.ndarray((100,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'unroll_id': np.ndarray((100,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'value_targets': np.ndarray((100,), dtype=float32, min=-10.362, max=0.19, mean=-7.096),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m                         'vf_preds': np.ndarray((100,), dtype=float32, min=-0.0, max=0.003, mean=0.002)},\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m               'type': 'SampleBatch'}}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m 2019-12-07 19:35:44,853\tINFO rollout_worker.py:485 -- Completed sample batch:\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m { 'data': { 'action_prob': np.ndarray((200,), dtype=float32, min=0.007, max=0.133, mean=0.096),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'actions': np.ndarray((200, 2), dtype=float32, min=-2.163, max=2.452, mean=0.501),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'advantages': np.ndarray((200,), dtype=float32, min=-12.066, max=0.187, mean=-7.119),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'agent_index': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'behaviour_logits': np.ndarray((200, 5), dtype=float32, min=-0.006, max=0.003, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'dones': np.ndarray((200,), dtype=bool, min=0.0, max=1.0, mean=0.01),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'eps_id': np.ndarray((200,), dtype=int64, min=1225675454.0, max=1905261938.0, mean=1565468696.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'infos': np.ndarray((200,), dtype=object, head={}),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'new_obs': np.ndarray((200, 69), dtype=float32, min=0.0, max=0.985, mean=0.236),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'obs': np.ndarray((200, 69), dtype=float32, min=0.0, max=0.985, mean=0.235),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'prev_actions': np.ndarray((200, 2), dtype=float32, min=-2.163, max=2.452, mean=0.493),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'prev_rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=0.109, mean=-0.318),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'rewards': np.ndarray((200,), dtype=float32, min=-1.0, max=0.109, mean=-0.318),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             't': np.ndarray((200,), dtype=int64, min=0.0, max=99.0, mean=49.5),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'unroll_id': np.ndarray((200,), dtype=int64, min=0.0, max=0.0, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'value_targets': np.ndarray((200,), dtype=float32, min=-12.068, max=0.19, mean=-7.119),\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m             'vf_preds': np.ndarray((200,), dtype=float32, min=-0.002, max=0.003, mean=-0.0)},\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m   'type': 'SampleBatch'}\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,851\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/kernel:0' shape=(69, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,851\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,851\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,851\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/kernel:0' shape=(16, 5) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/default_model/fc_out/bias:0' shape=(5,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/kernel:0' shape=(69, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/kernel:0' shape=(16, 16) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/kernel:0' shape=(16, 1) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,852\tINFO tf_policy.py:355 -- Optimizing variable <tf.Variable 'default_policy/value_function/fc_out/bias:0' shape=(1,) dtype=float32_ref>\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,855\tINFO multi_gpu_impl.py:146 -- Training on concatenated sample batches:\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m { 'inputs': [ np.ndarray((2000, 2), dtype=float32, min=-3.091, max=3.899, mean=0.45),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-1.0, max=0.207, mean=-0.318),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000, 69), dtype=float32, min=0.0, max=0.985, mean=0.258),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000, 2), dtype=float32, min=-3.091, max=3.899, mean=0.456),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-3.568, max=2.033, mean=0.0),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000, 5), dtype=float32, min=-0.008, max=0.007, mean=-0.0),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-20.378, max=0.302, mean=-7.206),\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m               np.ndarray((2000,), dtype=float32, min=-0.003, max=0.005, mean=-0.0)],\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'placeholders': [ <tf.Tensor 'default_policy/action:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/prev_reward:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/observation:0' shape=(?, 69) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/actions:0' shape=(?, 2) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/advantages:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/behaviour_logits:0' shape=(?, 5) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/value_targets:0' shape=(?,) dtype=float32>,\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m                     <tf.Tensor 'default_policy/vf_preds:0' shape=(?,) dtype=float32>],\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m   'state_inputs': []}\n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m \n",
      "\u001b[2m\u001b[36m(pid=23396)\u001b[0m 2019-12-07 19:36:51,855\tINFO multi_gpu_impl.py:191 -- Divided 2000 rollout sequences, each of length 1, among 1 devices.\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-37-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -14.224646468641934\n",
      "  episode_reward_mean: -32.01789314320183\n",
      "  episode_reward_min: -52.13366844831764\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 20\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 11570.798\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4924705028533936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02504133991897106\n",
      "        policy_loss: -0.029688503593206406\n",
      "        total_loss: 59.7205696105957\n",
      "        vf_explained_var: -0.002446413040161133\n",
      "        vf_loss: 59.74525833129883\n",
      "    load_time_ms: 75.887\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 2000\n",
      "    sample_time_ms: 167534.006\n",
      "    update_time_ms: 8901.673\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.11933085501859\n",
      "    ram_util_percent: 58.36245353159851\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 283.7044816690335\n",
      "    mean_inference_ms: 26.1546387704785\n",
      "    mean_processing_ms: 129.04268279785364\n",
      "  time_since_restore: 188.16306519508362\n",
      "  time_this_iter_s: 188.16306519508362\n",
      "  time_total_s: 188.16306519508362\n",
      "  timestamp: 1575765423\n",
      "  timesteps_since_restore: 2000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 1\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 188 s, 1 iter, 2000 ts, -32 rew\n",
      "\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23543)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23519)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23523)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23539)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/types_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/nodes_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/edges_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23527)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23531)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "\u001b[2m\u001b[36m(pid=23557)\u001b[0m Warning: Cannot find local schema '/home/osboxes/sumo_binaries/bin/data/xsd/connections_file.xsd', will try website lookup.\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-38-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -14.224646468641934\n",
      "  episode_reward_mean: -30.50436162097567\n",
      "  episode_reward_min: -52.13366844831764\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 40\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10937.183\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4200057983398438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0234555434435606\n",
      "        policy_loss: -0.018557768315076828\n",
      "        total_loss: 45.12467956542969\n",
      "        vf_explained_var: -0.0026913881301879883\n",
      "        vf_loss: 45.1385383605957\n",
      "    load_time_ms: 39.239\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "    sample_time_ms: 134570.202\n",
      "    update_time_ms: 4477.327\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 93.63875\n",
      "    ram_util_percent: 58.895624999999995\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 247.7308370017492\n",
      "    mean_inference_ms: 22.460845727378\n",
      "    mean_processing_ms: 126.55555253412804\n",
      "  time_since_restore: 300.1787598133087\n",
      "  time_this_iter_s: 112.0156946182251\n",
      "  time_total_s: 300.1787598133087\n",
      "  timestamp: 1575765535\n",
      "  timesteps_since_restore: 4000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 2\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 300 s, 2 iter, 4000 ts, -30.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -12.299503828147051\n",
      "  episode_reward_mean: -27.656399348786426\n",
      "  episode_reward_min: -52.13366844831764\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 60\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 9879.998\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3115200996398926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024454429745674133\n",
      "        policy_loss: -0.022569071501493454\n",
      "        total_loss: 26.5166015625\n",
      "        vf_explained_var: -0.001073598861694336\n",
      "        vf_loss: 26.534286499023438\n",
      "    load_time_ms: 27.056\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 6000\n",
      "    sample_time_ms: 116604.787\n",
      "    update_time_ms: 3001.643\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.03412698412698\n",
      "    ram_util_percent: 58.293650793650805\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 232.9420831136583\n",
      "    mean_inference_ms: 20.868440745765994\n",
      "    mean_processing_ms: 114.15235003348963\n",
      "  time_since_restore: 388.7097096443176\n",
      "  time_this_iter_s: 88.53094983100891\n",
      "  time_total_s: 388.7097096443176\n",
      "  timestamp: 1575765624\n",
      "  timesteps_since_restore: 6000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 3\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 388 s, 3 iter, 6000 ts, -27.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-41-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: -6.132801110263463\n",
      "  episode_reward_mean: -24.045751834942788\n",
      "  episode_reward_min: -52.13366844831764\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 80\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10096.549\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.157823085784912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027324272319674492\n",
      "        policy_loss: -0.03281307965517044\n",
      "        total_loss: 10.655037879943848\n",
      "        vf_explained_var: -0.0017729997634887695\n",
      "        vf_loss: 10.68238639831543\n",
      "    load_time_ms: 20.945\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "    sample_time_ms: 106807.388\n",
      "    update_time_ms: 2264.382\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.2015873015873\n",
      "    ram_util_percent: 58.20238095238097\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 224.74528501296737\n",
      "    mean_inference_ms: 20.00123029728092\n",
      "    mean_processing_ms: 103.60307844818394\n",
      "  time_since_restore: 477.24585580825806\n",
      "  time_this_iter_s: 88.53614616394043\n",
      "  time_total_s: 477.24585580825806\n",
      "  timestamp: 1575765712\n",
      "  timesteps_since_restore: 8000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 4\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 477 s, 4 iter, 8000 ts, -24 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-44-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 8.363570009813943\n",
      "  episode_reward_mean: -20.002603890692225\n",
      "  episode_reward_min: -52.13366844831764\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 11965.982\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.059645175933838\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015812411904335022\n",
      "        policy_loss: -0.0188547745347023\n",
      "        total_loss: 5.870800971984863\n",
      "        vf_explained_var: -0.0004514455795288086\n",
      "        vf_loss: 5.886494159698486\n",
      "    load_time_ms: 20.73\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 10000\n",
      "    sample_time_ms: 109659.004\n",
      "    update_time_ms: 1825.811\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.79651741293532\n",
      "    ram_util_percent: 58.94925373134328\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 222.83251160834763\n",
      "    mean_inference_ms: 19.680703926891457\n",
      "    mean_processing_ms: 95.72381640959458\n",
      "  time_since_restore: 617.8999090194702\n",
      "  time_this_iter_s: 140.65405321121216\n",
      "  time_total_s: 617.8999090194702\n",
      "  timestamp: 1575765853\n",
      "  timesteps_since_restore: 10000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 5\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 617 s, 5 iter, 10000 ts, -20 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-45-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 13.434306491129473\n",
      "  episode_reward_mean: -13.725957260884984\n",
      "  episode_reward_min: -48.72357946042106\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 11456.764\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9373111724853516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013286377303302288\n",
      "        policy_loss: -0.016295531764626503\n",
      "        total_loss: 5.1499786376953125\n",
      "        vf_explained_var: -0.0006184577941894531\n",
      "        vf_loss: 5.163614749908447\n",
      "    load_time_ms: 20.406\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "    sample_time_ms: 105471.711\n",
      "    update_time_ms: 1540.081\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.90225563909775\n",
      "    ram_util_percent: 58.8984962406015\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 208.52403002318962\n",
      "    mean_inference_ms: 18.033932699692194\n",
      "    mean_processing_ms: 81.54953019271797\n",
      "  time_since_restore: 711.5131657123566\n",
      "  time_this_iter_s: 93.61325669288635\n",
      "  time_total_s: 711.5131657123566\n",
      "  timestamp: 1575765947\n",
      "  timesteps_since_restore: 12000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 6\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 711 s, 6 iter, 12000 ts, -13.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-47-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 15.166571515345954\n",
      "  episode_reward_mean: -6.682798321847096\n",
      "  episode_reward_min: -36.81173437332036\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10952.305\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8412277698516846\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02045278623700142\n",
      "        policy_loss: -0.02504359371960163\n",
      "        total_loss: 8.17495346069336\n",
      "        vf_explained_var: -0.012569189071655273\n",
      "        vf_loss: 8.195908546447754\n",
      "    load_time_ms: 17.918\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 14000\n",
      "    sample_time_ms: 106106.971\n",
      "    update_time_ms: 1327.024\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82619047619048\n",
      "    ram_util_percent: 60.158333333333324\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 210.2950337724785\n",
      "    mean_inference_ms: 18.047975367266574\n",
      "    mean_processing_ms: 67.5748307421462\n",
      "  time_since_restore: 829.4481194019318\n",
      "  time_this_iter_s: 117.9349536895752\n",
      "  time_total_s: 829.4481194019318\n",
      "  timestamp: 1575766065\n",
      "  timesteps_since_restore: 14000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 7\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 829 s, 7 iter, 14000 ts, -6.68 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-48-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 23.19543622837194\n",
      "  episode_reward_mean: 0.594111089119307\n",
      "  episode_reward_min: -28.83405435694752\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10563.261\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7021064758300781\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02014092169702053\n",
      "        policy_loss: -0.021384701132774353\n",
      "        total_loss: 16.18514060974121\n",
      "        vf_explained_var: 0.017057955265045166\n",
      "        vf_loss: 16.202495574951172\n",
      "    load_time_ms: 16.119\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "    sample_time_ms: 100175.339\n",
      "    update_time_ms: 1167.019\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.18105263157895\n",
      "    ram_util_percent: 59.19157894736842\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 212.60848100229504\n",
      "    mean_inference_ms: 18.130313726718047\n",
      "    mean_processing_ms: 60.171233883307224\n",
      "  time_since_restore: 896.0335264205933\n",
      "  time_this_iter_s: 66.5854070186615\n",
      "  time_total_s: 896.0335264205933\n",
      "  timestamp: 1575766131\n",
      "  timesteps_since_restore: 16000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 8\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 896 s, 8 iter, 16000 ts, 0.594 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-49-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 34.8265090130965\n",
      "  episode_reward_mean: 7.8265159338659975\n",
      "  episode_reward_min: -22.46986644337636\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10299.742\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6421501636505127\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012838245369493961\n",
      "        policy_loss: -0.015103115700185299\n",
      "        total_loss: 37.10443878173828\n",
      "        vf_explained_var: 0.00472944974899292\n",
      "        vf_loss: 37.116981506347656\n",
      "    load_time_ms: 14.766\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 18000\n",
      "    sample_time_ms: 95650.361\n",
      "    update_time_ms: 1043.116\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59587628865978\n",
      "    ram_util_percent: 59.25257731958763\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 213.15436440656447\n",
      "    mean_inference_ms: 18.06493327073698\n",
      "    mean_processing_ms: 55.28608900773077\n",
      "  time_since_restore: 963.773699760437\n",
      "  time_this_iter_s: 67.74017333984375\n",
      "  time_total_s: 963.773699760437\n",
      "  timestamp: 1575766199\n",
      "  timesteps_since_restore: 18000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 9\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 963 s, 9 iter, 18000 ts, 7.83 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-51-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 43.10677328590816\n",
      "  episode_reward_mean: 14.564841174100225\n",
      "  episode_reward_min: -10.408518691243177\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10067.353\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6374377012252808\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012703397311270237\n",
      "        policy_loss: -0.013427856378257275\n",
      "        total_loss: 61.6630744934082\n",
      "        vf_explained_var: 0.001544177532196045\n",
      "        vf_loss: 61.67396545410156\n",
      "    load_time_ms: 13.552\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "    sample_time_ms: 92018.502\n",
      "    update_time_ms: 943.613\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.875\n",
      "    ram_util_percent: 59.283333333333324\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 209.70581748095708\n",
      "    mean_inference_ms: 17.699096815181914\n",
      "    mean_processing_ms: 51.47006646040577\n",
      "  time_since_restore: 1031.186753988266\n",
      "  time_this_iter_s: 67.41305422782898\n",
      "  time_total_s: 1031.186753988266\n",
      "  timestamp: 1575766267\n",
      "  timesteps_since_restore: 20000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 10\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1031 s, 10 iter, 20000 ts, 14.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-52-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 43.41705772109079\n",
      "  episode_reward_mean: 20.718553353277013\n",
      "  episode_reward_min: -3.6947316489241597\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 9846.065\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.569421410560608\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014115422032773495\n",
      "        policy_loss: -0.01601826213300228\n",
      "        total_loss: 64.10859680175781\n",
      "        vf_explained_var: 0.0007213354110717773\n",
      "        vf_loss: 64.12178802490234\n",
      "    load_time_ms: 6.57\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 22000\n",
      "    sample_time_ms: 81593.687\n",
      "    update_time_ms: 58.814\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.18846153846152\n",
      "    ram_util_percent: 58.9769230769231\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 205.81443580212385\n",
      "    mean_inference_ms: 17.342916946070318\n",
      "    mean_processing_ms: 48.33584843888927\n",
      "  time_since_restore: 1103.9385323524475\n",
      "  time_this_iter_s: 72.75177836418152\n",
      "  time_total_s: 1103.9385323524475\n",
      "  timestamp: 1575766339\n",
      "  timesteps_since_restore: 22000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 11\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1103 s, 11 iter, 22000 ts, 20.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-53-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 46.79212408306969\n",
      "  episode_reward_mean: 25.5255624228259\n",
      "  episode_reward_min: -3.6947316489241597\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 9592.701\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6438056230545044\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00737013528123498\n",
      "        policy_loss: -0.012772771529853344\n",
      "        total_loss: 68.81432342529297\n",
      "        vf_explained_var: 0.00010210275650024414\n",
      "        vf_loss: 68.82562255859375\n",
      "    load_time_ms: 6.623\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "    sample_time_ms: 77434.286\n",
      "    update_time_ms: 57.5\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.08969072164949\n",
      "    ram_util_percent: 58.452577319587625\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 199.08628120437402\n",
      "    mean_inference_ms: 16.703648464746685\n",
      "    mean_processing_ms: 45.58292480984076\n",
      "  time_since_restore: 1171.8088235855103\n",
      "  time_this_iter_s: 67.87029123306274\n",
      "  time_total_s: 1171.8088235855103\n",
      "  timestamp: 1575766407\n",
      "  timesteps_since_restore: 24000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 12\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1171 s, 12 iter, 24000 ts, 25.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-54-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 55.05279965226599\n",
      "  episode_reward_mean: 30.813383694301464\n",
      "  episode_reward_min: 8.154648478056949\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 9638.058\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6496349573135376\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02100077085196972\n",
      "        policy_loss: -0.02527853660285473\n",
      "        total_loss: 116.3188247680664\n",
      "        vf_explained_var: 0.0006277561187744141\n",
      "        vf_loss: 116.34202575683594\n",
      "    load_time_ms: 6.656\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 26000\n",
      "    sample_time_ms: 75572.497\n",
      "    update_time_ms: 58.008\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.944\n",
      "    ram_util_percent: 58.42599999999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 193.35724538192318\n",
      "    mean_inference_ms: 16.18257693603094\n",
      "    mean_processing_ms: 42.88943005890045\n",
      "  time_since_restore: 1242.1992588043213\n",
      "  time_this_iter_s: 70.39043521881104\n",
      "  time_total_s: 1242.1992588043213\n",
      "  timestamp: 1575766478\n",
      "  timesteps_since_restore: 26000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 13\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1242 s, 13 iter, 26000 ts, 30.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-56-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 56.41744708590311\n",
      "  episode_reward_mean: 33.66022923626966\n",
      "  episode_reward_min: -3.355474409195453\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10932.362\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6465222835540771\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02661876380443573\n",
      "        policy_loss: -0.02903638780117035\n",
      "        total_loss: 104.3424072265625\n",
      "        vf_explained_var: 0.0001087188720703125\n",
      "        vf_loss: 104.36876678466797\n",
      "    load_time_ms: 7.086\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "    sample_time_ms: 78518.9\n",
      "    update_time_ms: 58.292\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 58.94919786096256\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 190.84426789217758\n",
      "    mean_inference_ms: 15.949118809013974\n",
      "    mean_processing_ms: 41.27088007818841\n",
      "  time_since_restore: 1372.9803104400635\n",
      "  time_this_iter_s: 130.7810516357422\n",
      "  time_total_s: 1372.9803104400635\n",
      "  timestamp: 1575766609\n",
      "  timesteps_since_restore: 28000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 14\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1372 s, 14 iter, 28000 ts, 33.7 rew\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 19:56:49,513\tWARNING util.py:145 -- The `on_step_begin` operation took 0.1186361312866211 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 69.50867156616505\n",
      "  episode_reward_mean: 35.634418287290224\n",
      "  episode_reward_min: -3.355474409195453\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 9760.744\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.687409520149231\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01927327737212181\n",
      "        policy_loss: -0.023318080231547356\n",
      "        total_loss: 111.45574951171875\n",
      "        vf_explained_var: 0.00014859437942504883\n",
      "        vf_loss: 111.47715759277344\n",
      "    load_time_ms: 5.43\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 30000\n",
      "    sample_time_ms: 74735.513\n",
      "    update_time_ms: 62.578\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.0176923076923\n",
      "    ram_util_percent: 59.043846153846154\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 189.65468161256592\n",
      "    mean_inference_ms: 15.825476582671962\n",
      "    mean_processing_ms: 39.87545134640668\n",
      "  time_since_restore: 1464.103214263916\n",
      "  time_this_iter_s: 91.12290382385254\n",
      "  time_total_s: 1464.103214263916\n",
      "  timestamp: 1575766700\n",
      "  timesteps_since_restore: 30000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 15\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1464 s, 15 iter, 30000 ts, 35.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_19-59-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 70.10293160795386\n",
      "  episode_reward_mean: 37.934240461850266\n",
      "  episode_reward_min: -10.32828043686163\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10310.44\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7528635263442993\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016448017209768295\n",
      "        policy_loss: -0.02168537676334381\n",
      "        total_loss: 132.54896545410156\n",
      "        vf_explained_var: 0.0002701282501220703\n",
      "        vf_loss: 132.56903076171875\n",
      "    load_time_ms: 4.46\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "    sample_time_ms: 72100.165\n",
      "    update_time_ms: 56.913\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.95288461538462\n",
      "    ram_util_percent: 59.01153846153847\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 188.22623614648745\n",
      "    mean_inference_ms: 15.70573019859396\n",
      "    mean_processing_ms: 38.6085933807016\n",
      "  time_since_restore: 1536.7968037128448\n",
      "  time_this_iter_s: 72.69358944892883\n",
      "  time_total_s: 1536.7968037128448\n",
      "  timestamp: 1575766773\n",
      "  timesteps_since_restore: 32000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 16\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1536 s, 16 iter, 32000 ts, 37.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-00-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 73.98961190383628\n",
      "  episode_reward_mean: 40.80127358152761\n",
      "  episode_reward_min: -10.32828043686163\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 10308.293\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.585311770439148\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020316001027822495\n",
      "        policy_loss: -0.025694265961647034\n",
      "        total_loss: 144.38925170898438\n",
      "        vf_explained_var: -1.6689300537109375e-06\n",
      "        vf_loss: 144.41290283203125\n",
      "    load_time_ms: 4.509\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 34000\n",
      "    sample_time_ms: 67575.607\n",
      "    update_time_ms: 57.275\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.73495145631068\n",
      "    ram_util_percent: 59.16213592233013\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 188.2685857263668\n",
      "    mean_inference_ms: 15.696681685862707\n",
      "    mean_processing_ms: 37.73264040019018\n",
      "  time_since_restore: 1609.5171608924866\n",
      "  time_this_iter_s: 72.72035717964172\n",
      "  time_total_s: 1609.5171608924866\n",
      "  timestamp: 1575766845\n",
      "  timesteps_since_restore: 34000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 17\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 4.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1609 s, 17 iter, 34000 ts, 40.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:04:24,673\tWARNING util.py:145 -- The `process_trial` operation took 0.11617708206176758 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-04-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 73.98961190383628\n",
      "  episode_reward_mean: 41.657861975101724\n",
      "  episode_reward_min: -10.32828043686163\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 13436.795\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6526142358779907\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02459453046321869\n",
      "        policy_loss: -0.031037544831633568\n",
      "        total_loss: 140.8939208984375\n",
      "        vf_explained_var: 4.035234451293945e-05\n",
      "        vf_loss: 140.92250061035156\n",
      "    load_time_ms: 6.003\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "    sample_time_ms: 79629.71\n",
      "    update_time_ms: 57.017\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 64.90482315112541\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 190.7822902895618\n",
      "    mean_inference_ms: 15.91753271438919\n",
      "    mean_processing_ms: 37.19104523307118\n",
      "  time_since_restore: 1828.1187608242035\n",
      "  time_this_iter_s: 218.60159993171692\n",
      "  time_total_s: 1828.1187608242035\n",
      "  timestamp: 1575767064\n",
      "  timesteps_since_restore: 36000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 18\n",
      "  trial_id: 42ddae0c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:04:24,913\tWARNING util.py:145 -- The `experiment_checkpoint` operation took 0.1885209083557129 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 1828 s, 18 iter, 36000 ts, 41.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:07:26,554\tWARNING util.py:145 -- The `process_trial` operation took 0.10054779052734375 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-07-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 75.3036598274489\n",
      "  episode_reward_mean: 43.180380028661894\n",
      "  episode_reward_min: -13.505895989818127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 14852.505\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10000000149011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.78208589553833\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06268581002950668\n",
      "        policy_loss: -0.04571098834276199\n",
      "        total_loss: 152.60423278808594\n",
      "        vf_explained_var: 7.861852645874023e-05\n",
      "        vf_loss: 152.64369201660156\n",
      "    load_time_ms: 6.824\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 38000\n",
      "    sample_time_ms: 89550.569\n",
      "    update_time_ms: 94.39\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 65.94208494208495\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 194.24468386957477\n",
      "    mean_inference_ms: 16.22292629383553\n",
      "    mean_processing_ms: 36.705429971713784\n",
      "  time_since_restore: 2009.7111690044403\n",
      "  time_this_iter_s: 181.59240818023682\n",
      "  time_total_s: 2009.7111690044403\n",
      "  timestamp: 1575767246\n",
      "  timesteps_since_restore: 38000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 19\n",
      "  trial_id: 42ddae0c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:07:26,813\tWARNING util.py:145 -- The `on_step_begin` operation took 0.12042474746704102 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 2009 s, 19 iter, 38000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-10-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.4252730148665\n",
      "  episode_reward_mean: 46.03513157153842\n",
      "  episode_reward_min: -13.505895989818127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 16019.727\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5401828289031982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022949233651161194\n",
      "        policy_loss: -0.028050273656845093\n",
      "        total_loss: 196.53468322753906\n",
      "        vf_explained_var: 4.124641418457031e-05\n",
      "        vf_loss: 196.5592498779297\n",
      "    load_time_ms: 7.23\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "    sample_time_ms: 99347.627\n",
      "    update_time_ms: 107.216\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 71.06996047430829\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 198.65814551766255\n",
      "    mean_inference_ms: 16.664644264257245\n",
      "    mean_processing_ms: 36.335922251525574\n",
      "  time_since_restore: 2186.969117164612\n",
      "  time_this_iter_s: 177.2579481601715\n",
      "  time_total_s: 2186.969117164612\n",
      "  timestamp: 1575767423\n",
      "  timesteps_since_restore: 40000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 20\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 2186 s, 20 iter, 40000 ts, 46 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.4252730148665\n",
      "  episode_reward_mean: 49.257620858438116\n",
      "  episode_reward_min: -13.505895989818127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 17111.013\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5983933210372925\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.032549500465393066\n",
      "        policy_loss: -0.029083505272865295\n",
      "        total_loss: 213.6141815185547\n",
      "        vf_explained_var: -1.2278556823730469e-05\n",
      "        vf_loss: 213.63841247558594\n",
      "    load_time_ms: 6.905\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 42000\n",
      "    sample_time_ms: 107108.05\n",
      "    update_time_ms: 115.556\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 69.87521739130435\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 205.07884746199224\n",
      "    mean_inference_ms: 17.28824911388856\n",
      "    mean_processing_ms: 36.3657154649454\n",
      "  time_since_restore: 2348.378718852997\n",
      "  time_this_iter_s: 161.409601688385\n",
      "  time_total_s: 2348.378718852997\n",
      "  timestamp: 1575767585\n",
      "  timesteps_since_restore: 42000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 21\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 2348 s, 21 iter, 42000 ts, 49.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:15:44,856\tWARNING util.py:145 -- The `process_trial` operation took 0.11901593208312988 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-15-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.4252730148665\n",
      "  episode_reward_mean: 51.38459438736306\n",
      "  episode_reward_min: -13.505895989818127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 19236.471\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5613056421279907\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018155183643102646\n",
      "        policy_loss: -0.021425001323223114\n",
      "        total_loss: 210.87547302246094\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 210.89414978027344\n",
      "    load_time_ms: 8.177\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 44000\n",
      "    sample_time_ms: 114083.767\n",
      "    update_time_ms: 128.513\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 63.21106194690266\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 211.96868462063424\n",
      "    mean_inference_ms: 17.986783458155827\n",
      "    mean_processing_ms: 36.39332502745758\n",
      "  time_since_restore: 2507.567563056946\n",
      "  time_this_iter_s: 159.18884420394897\n",
      "  time_total_s: 2507.567563056946\n",
      "  timestamp: 1575767744\n",
      "  timesteps_since_restore: 44000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 22\n",
      "  trial_id: 42ddae0c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:15:45,037\tWARNING util.py:145 -- The `experiment_checkpoint` operation took 0.15218448638916016 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 2507 s, 22 iter, 44000 ts, 51.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-18-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.4252730148665\n",
      "  episode_reward_mean: 54.07624869744975\n",
      "  episode_reward_min: -13.505895989818127\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 21285.376\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5739854574203491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02502565085887909\n",
      "        policy_loss: -0.03397645801305771\n",
      "        total_loss: 213.88125610351562\n",
      "        vf_explained_var: -4.410743713378906e-06\n",
      "        vf_loss: 213.91143798828125\n",
      "    load_time_ms: 9.213\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 46000\n",
      "    sample_time_ms: 121912.439\n",
      "    update_time_ms: 155.1\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 63.28719008264463\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 217.48505115026867\n",
      "    mean_inference_ms: 18.577924944112127\n",
      "    mean_processing_ms: 36.393304263639536\n",
      "  time_since_restore: 2677.059534549713\n",
      "  time_this_iter_s: 169.49197149276733\n",
      "  time_total_s: 2677.059534549713\n",
      "  timestamp: 1575767914\n",
      "  timesteps_since_restore: 46000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 23\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 2677 s, 23 iter, 46000 ts, 54.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:22:14,865\tWARNING util.py:145 -- The `process_trial` operation took 0.14519643783569336 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-22-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.4252730148665\n",
      "  episode_reward_mean: 53.5694804593051\n",
      "  episode_reward_min: -6.308523354474603\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 21724.106\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6964528560638428\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026512622833251953\n",
      "        policy_loss: -0.030397556722164154\n",
      "        total_loss: 135.02236938476562\n",
      "        vf_explained_var: 2.390146255493164e-05\n",
      "        vf_loss: 135.04879760742188\n",
      "    load_time_ms: 9.375\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 48000\n",
      "    sample_time_ms: 130378.14\n",
      "    update_time_ms: 165.962\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 65.32172523961661\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 224.3174315386064\n",
      "    mean_inference_ms: 19.234383756635125\n",
      "    mean_processing_ms: 36.60041858258469\n",
      "  time_since_restore: 2897.151837348938\n",
      "  time_this_iter_s: 220.09230279922485\n",
      "  time_total_s: 2897.151837348938\n",
      "  timestamp: 1575768134\n",
      "  timesteps_since_restore: 48000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 24\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 2897 s, 24 iter, 48000 ts, 53.6 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 20:25:06,034\tWARNING util.py:145 -- The `process_trial` operation took 0.1398451328277588 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-25-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 77.3488925976265\n",
      "  episode_reward_mean: 54.31941474793829\n",
      "  episode_reward_min: -6.308523354474603\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 23088.26\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.489917516708374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025259025394916534\n",
      "        policy_loss: -0.03300931677222252\n",
      "        total_loss: 203.27394104003906\n",
      "        vf_explained_var: -4.5299530029296875e-06\n",
      "        vf_loss: 203.30316162109375\n",
      "    load_time_ms: 12.446\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 50000\n",
      "    sample_time_ms: 136968.725\n",
      "    update_time_ms: 168.816\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 66.25\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 230.04202846792657\n",
      "    mean_inference_ms: 19.795391532866166\n",
      "    mean_processing_ms: 36.81374723366642\n",
      "  time_since_restore: 3068.080664396286\n",
      "  time_this_iter_s: 170.92882704734802\n",
      "  time_total_s: 3068.080664396286\n",
      "  timestamp: 1575768305\n",
      "  timesteps_since_restore: 50000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 25\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3068 s, 25 iter, 50000 ts, 54.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-27-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 77.3488925976265\n",
      "  episode_reward_mean: 54.72867247463548\n",
      "  episode_reward_min: -6.308523354474603\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 22441.317\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6200802326202393\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030555400997400284\n",
      "        policy_loss: -0.03672168403863907\n",
      "        total_loss: 209.50401306152344\n",
      "        vf_explained_var: -3.5762786865234375e-06\n",
      "        vf_loss: 209.5360870361328\n",
      "    load_time_ms: 11.814\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 52000\n",
      "    sample_time_ms: 143136.021\n",
      "    update_time_ms: 198.477\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.53606557377049\n",
      "    ram_util_percent: 64.42295081967212\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 235.5214248144301\n",
      "    mean_inference_ms: 20.28999015561767\n",
      "    mean_processing_ms: 36.92999871536122\n",
      "  time_since_restore: 3196.266042947769\n",
      "  time_this_iter_s: 128.18537855148315\n",
      "  time_total_s: 3196.266042947769\n",
      "  timestamp: 1575768434\n",
      "  timesteps_since_restore: 52000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 26\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3196 s, 26 iter, 52000 ts, 54.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 77.3488925976265\n",
      "  episode_reward_mean: 51.99392898237054\n",
      "  episode_reward_min: -37.231996282539214\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 22447.902\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8210383653640747\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028791749849915504\n",
      "        policy_loss: -0.029694823548197746\n",
      "        total_loss: 135.69859313964844\n",
      "        vf_explained_var: -8.344650268554688e-07\n",
      "        vf_loss: 135.72393798828125\n",
      "    load_time_ms: 11.734\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 54000\n",
      "    sample_time_ms: 142960.856\n",
      "    update_time_ms: 198.658\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.39108910891089\n",
      "    ram_util_percent: 63.36930693069308\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 239.0393147947912\n",
      "    mean_inference_ms: 20.61176231569366\n",
      "    mean_processing_ms: 36.91688257808161\n",
      "  time_since_restore: 3267.2633781433105\n",
      "  time_this_iter_s: 70.99733519554138\n",
      "  time_total_s: 3267.2633781433105\n",
      "  timestamp: 1575768505\n",
      "  timesteps_since_restore: 54000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 27\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3267 s, 27 iter, 54000 ts, 52 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-29-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.51617137012865\n",
      "  episode_reward_mean: 52.7207494935018\n",
      "  episode_reward_min: -37.231996282539214\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 19281.445\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.549930453300476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021603792905807495\n",
      "        policy_loss: -0.026959702372550964\n",
      "        total_loss: 238.5221405029297\n",
      "        vf_explained_var: -4.76837158203125e-07\n",
      "        vf_loss: 238.5458526611328\n",
      "    load_time_ms: 10.15\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 56000\n",
      "    sample_time_ms: 130740.229\n",
      "    update_time_ms: 200.872\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.81720430107526\n",
      "    ram_util_percent: 62.744086021505396\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 241.53301420286945\n",
      "    mean_inference_ms: 20.81103169020923\n",
      "    mean_processing_ms: 36.9295231158712\n",
      "  time_since_restore: 3331.8178009986877\n",
      "  time_this_iter_s: 64.5544228553772\n",
      "  time_total_s: 3331.8178009986877\n",
      "  timestamp: 1575768569\n",
      "  timesteps_since_restore: 56000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 28\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3331 s, 28 iter, 56000 ts, 52.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-30-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.51617137012865\n",
      "  episode_reward_mean: 54.32095837765093\n",
      "  episode_reward_min: -37.231996282539214\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 17841.682\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8054516315460205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018558336421847343\n",
      "        policy_loss: -0.027242479845881462\n",
      "        total_loss: 186.7657928466797\n",
      "        vf_explained_var: 2.682209014892578e-06\n",
      "        vf_loss: 186.79025268554688\n",
      "    load_time_ms: 9.661\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 58000\n",
      "    sample_time_ms: 120478.162\n",
      "    update_time_ms: 163.335\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.40549450549452\n",
      "    ram_util_percent: 62.854945054945055\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 240.79235527111464\n",
      "    mean_inference_ms: 20.7471080426369\n",
      "    mean_processing_ms: 36.50315437690445\n",
      "  time_since_restore: 3396.005171060562\n",
      "  time_this_iter_s: 64.18737006187439\n",
      "  time_total_s: 3396.005171060562\n",
      "  timestamp: 1575768634\n",
      "  timesteps_since_restore: 58000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 29\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3396 s, 29 iter, 58000 ts, 54.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-31-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.51617137012865\n",
      "  episode_reward_mean: 53.60566833533607\n",
      "  episode_reward_min: -37.231996282539214\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 16795.315\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8036072254180908\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03289502114057541\n",
      "        policy_loss: -0.03524046018719673\n",
      "        total_loss: 191.4442138671875\n",
      "        vf_explained_var: -1.430511474609375e-06\n",
      "        vf_loss: 191.47454833984375\n",
      "    load_time_ms: 9.246\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 60000\n",
      "    sample_time_ms: 110249.688\n",
      "    update_time_ms: 157.358\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.68695652173913\n",
      "    ram_util_percent: 62.97065217391304\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 238.17150603360943\n",
      "    mean_inference_ms: 20.508982737283468\n",
      "    mean_processing_ms: 36.02364441990521\n",
      "  time_since_restore: 3460.3749372959137\n",
      "  time_this_iter_s: 64.36976623535156\n",
      "  time_total_s: 3460.3749372959137\n",
      "  timestamp: 1575768698\n",
      "  timesteps_since_restore: 60000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 30\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3460 s, 30 iter, 60000 ts, 53.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.51617137012865\n",
      "  episode_reward_mean: 52.49461028004252\n",
      "  episode_reward_min: -37.231996282539214\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 15541.719\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6872316598892212\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03380541875958443\n",
      "        policy_loss: -0.030577557161450386\n",
      "        total_loss: 176.961181640625\n",
      "        vf_explained_var: -9.5367431640625e-07\n",
      "        vf_loss: 176.9866943359375\n",
      "    load_time_ms: 9.401\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 62000\n",
      "    sample_time_ms: 101795.643\n",
      "    update_time_ms: 148.335\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.17499999999998\n",
      "    ram_util_percent: 62.8358695652174\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 234.82060845762737\n",
      "    mean_inference_ms: 20.20008271789118\n",
      "    mean_processing_ms: 35.55932837266772\n",
      "  time_since_restore: 3524.5482416152954\n",
      "  time_this_iter_s: 64.17330431938171\n",
      "  time_total_s: 3524.5482416152954\n",
      "  timestamp: 1575768762\n",
      "  timesteps_since_restore: 62000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 31\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3524 s, 31 iter, 62000 ts, 52.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-33-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.51617137012865\n",
      "  episode_reward_mean: 54.29318980894605\n",
      "  episode_reward_min: -9.497789835107787\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 13373.076\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15000000596046448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.614007830619812\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04207416623830795\n",
      "        policy_loss: -0.03554631769657135\n",
      "        total_loss: 179.09939575195312\n",
      "        vf_explained_var: -9.5367431640625e-07\n",
      "        vf_loss: 179.12860107421875\n",
      "    load_time_ms: 8.075\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 64000\n",
      "    sample_time_ms: 94530.275\n",
      "    update_time_ms: 137.561\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.65217391304348\n",
      "    ram_util_percent: 62.932608695652185\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 231.89202498562466\n",
      "    mean_inference_ms: 19.918833074897403\n",
      "    mean_processing_ms: 35.1704488647712\n",
      "  time_since_restore: 3589.1016776561737\n",
      "  time_this_iter_s: 64.5534360408783\n",
      "  time_total_s: 3589.1016776561737\n",
      "  timestamp: 1575768827\n",
      "  timesteps_since_restore: 64000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 32\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3589 s, 32 iter, 64000 ts, 54.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-34-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.67277664489625\n",
      "  episode_reward_mean: 50.276029126096994\n",
      "  episode_reward_min: -9.497789835107787\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 11237.562\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22499999403953552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7926490306854248\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0456790067255497\n",
      "        policy_loss: -0.044820427894592285\n",
      "        total_loss: 142.63052368164062\n",
      "        vf_explained_var: 2.384185791015625e-07\n",
      "        vf_loss: 142.66510009765625\n",
      "    load_time_ms: 7.022\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 66000\n",
      "    sample_time_ms: 86075.114\n",
      "    update_time_ms: 110.679\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86999999999999\n",
      "    ram_util_percent: 63.035555555555554\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 228.38497777163437\n",
      "    mean_inference_ms: 19.61096016763066\n",
      "    mean_processing_ms: 34.67853786168371\n",
      "  time_since_restore: 3652.3404195308685\n",
      "  time_this_iter_s: 63.238741874694824\n",
      "  time_total_s: 3652.3404195308685\n",
      "  timestamp: 1575768890\n",
      "  timesteps_since_restore: 66000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 33\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3652 s, 33 iter, 66000 ts, 50.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-35-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.67277664489625\n",
      "  episode_reward_mean: 49.86959592625737\n",
      "  episode_reward_min: -1.3232712806725793\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 9161.826\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.645668864250183\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018479295074939728\n",
      "        policy_loss: -0.03002830035984516\n",
      "        total_loss: 152.67515563964844\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 152.69900512695312\n",
      "    load_time_ms: 6.442\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 68000\n",
      "    sample_time_ms: 72484.551\n",
      "    update_time_ms: 99.687\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.12666666666667\n",
      "    ram_util_percent: 63.089999999999996\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 224.5505645122842\n",
      "    mean_inference_ms: 19.283389022532166\n",
      "    mean_processing_ms: 34.14152495652837\n",
      "  time_since_restore: 3715.3847765922546\n",
      "  time_this_iter_s: 63.04435706138611\n",
      "  time_total_s: 3715.3847765922546\n",
      "  timestamp: 1575768953\n",
      "  timesteps_since_restore: 68000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 34\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3715 s, 34 iter, 68000 ts, 49.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-36-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.64736843778286\n",
      "  episode_reward_mean: 48.77275371165407\n",
      "  episode_reward_min: -6.031724177681029\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7768.664\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7138267755508423\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020019100978970528\n",
      "        policy_loss: -0.029885463416576385\n",
      "        total_loss: 161.32162475585938\n",
      "        vf_explained_var: 2.980232238769531e-07\n",
      "        vf_loss: 161.34481811523438\n",
      "    load_time_ms: 3.365\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 70000\n",
      "    sample_time_ms: 63126.585\n",
      "    update_time_ms: 90.704\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.49555555555555\n",
      "    ram_util_percent: 63.13444444444444\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 223.03284446511054\n",
      "    mean_inference_ms: 19.108812145462608\n",
      "    mean_processing_ms: 33.880077432853255\n",
      "  time_since_restore: 3778.485930919647\n",
      "  time_this_iter_s: 63.10115432739258\n",
      "  time_total_s: 3778.485930919647\n",
      "  timestamp: 1575769016\n",
      "  timesteps_since_restore: 70000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 35\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3778 s, 35 iter, 70000 ts, 48.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-38-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.64736843778286\n",
      "  episode_reward_mean: 49.980237244584906\n",
      "  episode_reward_min: -6.031724177681029\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7718.93\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6786054372787476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014037860557436943\n",
      "        policy_loss: -0.024873947724699974\n",
      "        total_loss: 213.00770568847656\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 213.0278778076172\n",
      "    load_time_ms: 3.401\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 72000\n",
      "    sample_time_ms: 56699.813\n",
      "    update_time_ms: 60.189\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.52666666666669\n",
      "    ram_util_percent: 63.188888888888876\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 220.10970946927947\n",
      "    mean_inference_ms: 18.855322090696763\n",
      "    mean_processing_ms: 33.48125972769546\n",
      "  time_since_restore: 3841.613574028015\n",
      "  time_this_iter_s: 63.12764310836792\n",
      "  time_total_s: 3841.613574028015\n",
      "  timestamp: 1575769080\n",
      "  timesteps_since_restore: 72000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 36\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3841 s, 36 iter, 72000 ts, 50 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-39-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.64736843778286\n",
      "  episode_reward_mean: 49.148080146569875\n",
      "  episode_reward_min: -8.040620435736798\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7799.288\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3375000059604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7949758768081665\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04463130608201027\n",
      "        policy_loss: -0.057393305003643036\n",
      "        total_loss: 173.87771606445312\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 173.92002868652344\n",
      "    load_time_ms: 3.485\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 74000\n",
      "    sample_time_ms: 56074.571\n",
      "    update_time_ms: 59.23\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.34255319148937\n",
      "    ram_util_percent: 63.21063829787232\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 218.56154238509828\n",
      "    mean_inference_ms: 18.689516806402967\n",
      "    mean_processing_ms: 33.156253683619866\n",
      "  time_since_restore: 3907.2455098629\n",
      "  time_this_iter_s: 65.63193583488464\n",
      "  time_total_s: 3907.2455098629\n",
      "  timestamp: 1575769145\n",
      "  timesteps_since_restore: 74000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 37\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3907 s, 37 iter, 74000 ts, 49.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-40-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.64736843778286\n",
      "  episode_reward_mean: 49.50120430898462\n",
      "  episode_reward_min: -8.040620435736798\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7919.927\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7826213836669922\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020395103842020035\n",
      "        policy_loss: -0.034823477268218994\n",
      "        total_loss: 149.57801818847656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 149.6024627685547\n",
      "    load_time_ms: 3.537\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 76000\n",
      "    sample_time_ms: 55888.607\n",
      "    update_time_ms: 60.13\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.69010989010988\n",
      "    ram_util_percent: 63.11648351648351\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 216.52734405680474\n",
      "    mean_inference_ms: 18.49627759236882\n",
      "    mean_processing_ms: 32.75408176312297\n",
      "  time_since_restore: 3971.171575307846\n",
      "  time_this_iter_s: 63.92606544494629\n",
      "  time_total_s: 3971.171575307846\n",
      "  timestamp: 1575769209\n",
      "  timesteps_since_restore: 76000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 38\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 3971 s, 38 iter, 76000 ts, 49.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-41-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.64736843778286\n",
      "  episode_reward_mean: 51.60317233396206\n",
      "  episode_reward_min: -8.040620435736798\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7858.763\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.655550241470337\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02145541086792946\n",
      "        policy_loss: -0.03698539733886719\n",
      "        total_loss: 195.42408752441406\n",
      "        vf_explained_var: -3.5762786865234375e-07\n",
      "        vf_loss: 195.45013427734375\n",
      "    load_time_ms: 3.101\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 78000\n",
      "    sample_time_ms: 55829.509\n",
      "    update_time_ms: 60.024\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.45222222222223\n",
      "    ram_util_percent: 63.14555555555556\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 214.50184049544262\n",
      "    mean_inference_ms: 18.31812000969941\n",
      "    mean_processing_ms: 32.478536809846155\n",
      "  time_since_restore: 4034.057569026947\n",
      "  time_this_iter_s: 62.88599371910095\n",
      "  time_total_s: 4034.057569026947\n",
      "  timestamp: 1575769272\n",
      "  timesteps_since_restore: 78000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 39\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4034 s, 39 iter, 78000 ts, 51.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-42-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.16272024898407\n",
      "  episode_reward_mean: 52.00816146407146\n",
      "  episode_reward_min: -8.040620435736798\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7683.89\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.708380937576294\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014799034222960472\n",
      "        policy_loss: -0.026051687076687813\n",
      "        total_loss: 174.98695373535156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 175.0054931640625\n",
      "    load_time_ms: 3.17\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 80000\n",
      "    sample_time_ms: 55906.618\n",
      "    update_time_ms: 53.181\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.43296703296703\n",
      "    ram_util_percent: 63.17252747252748\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 211.09278752424268\n",
      "    mean_inference_ms: 18.043942430174813\n",
      "    mean_processing_ms: 31.99883854985691\n",
      "  time_since_restore: 4097.382869005203\n",
      "  time_this_iter_s: 63.325299978256226\n",
      "  time_total_s: 4097.382869005203\n",
      "  timestamp: 1575769336\n",
      "  timesteps_since_restore: 80000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 40\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4097 s, 40 iter, 80000 ts, 52 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-43-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.16272024898407\n",
      "  episode_reward_mean: 50.74342731572747\n",
      "  episode_reward_min: -8.040620435736798\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7630.545\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5740396976470947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03202720731496811\n",
      "        policy_loss: -0.044959671795368195\n",
      "        total_loss: 169.371337890625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 169.40013122558594\n",
      "    load_time_ms: 3.015\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 82000\n",
      "    sample_time_ms: 55885.265\n",
      "    update_time_ms: 53.433\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.27333333333331\n",
      "    ram_util_percent: 63.19666666666665\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 208.96058799129037\n",
      "    mean_inference_ms: 17.85545804614547\n",
      "    mean_processing_ms: 31.701128344331465\n",
      "  time_since_restore: 4160.80811715126\n",
      "  time_this_iter_s: 63.42524814605713\n",
      "  time_total_s: 4160.80811715126\n",
      "  timestamp: 1575769399\n",
      "  timesteps_since_restore: 82000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 41\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4160 s, 41 iter, 82000 ts, 50.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-44-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.19162104012554\n",
      "  episode_reward_mean: 51.231319426652405\n",
      "  episode_reward_min: -7.84875012041276\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7624.085\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7970551252365112\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014929836615920067\n",
      "        policy_loss: -0.02979731187224388\n",
      "        total_loss: 158.82623291015625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 158.8484649658203\n",
      "    load_time_ms: 3.012\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 84000\n",
      "    sample_time_ms: 55832.383\n",
      "    update_time_ms: 51.843\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.91868131868132\n",
      "    ram_util_percent: 63.24175824175824\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 206.41066190662735\n",
      "    mean_inference_ms: 17.63883990502078\n",
      "    mean_processing_ms: 31.42239582290098\n",
      "  time_since_restore: 4224.764100313187\n",
      "  time_this_iter_s: 63.95598316192627\n",
      "  time_total_s: 4224.764100313187\n",
      "  timestamp: 1575769463\n",
      "  timesteps_since_restore: 84000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 42\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4224 s, 42 iter, 84000 ts, 51.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-45-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.19162104012554\n",
      "  episode_reward_mean: 52.588264069129146\n",
      "  episode_reward_min: -6.014618920433955\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7602.364\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5244992971420288\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020967811346054077\n",
      "        policy_loss: -0.038214776664972305\n",
      "        total_loss: 174.27468872070312\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 174.30223083496094\n",
      "    load_time_ms: 3.03\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 86000\n",
      "    sample_time_ms: 55826.899\n",
      "    update_time_ms: 51.027\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.7488888888889\n",
      "    ram_util_percent: 63.35666666666667\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 205.03879283790425\n",
      "    mean_inference_ms: 17.500965751605637\n",
      "    mean_processing_ms: 31.226479199955307\n",
      "  time_since_restore: 4287.735979557037\n",
      "  time_this_iter_s: 62.97187924385071\n",
      "  time_total_s: 4287.735979557037\n",
      "  timestamp: 1575769526\n",
      "  timesteps_since_restore: 86000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 43\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4287 s, 43 iter, 86000 ts, 52.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-46-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.47051415939144\n",
      "  episode_reward_mean: 48.03991494885011\n",
      "  episode_reward_min: -6.014618920433955\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7600.333\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0467123985290527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018787315115332603\n",
      "        policy_loss: -0.03168006241321564\n",
      "        total_loss: 128.80609130859375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 128.8282470703125\n",
      "    load_time_ms: 3.033\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 88000\n",
      "    sample_time_ms: 55825.439\n",
      "    update_time_ms: 50.635\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.39111111111112\n",
      "    ram_util_percent: 63.395555555555546\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 202.8386847628151\n",
      "    mean_inference_ms: 17.311713339058333\n",
      "    mean_processing_ms: 30.915783328297408\n",
      "  time_since_restore: 4350.75217962265\n",
      "  time_this_iter_s: 63.01620006561279\n",
      "  time_total_s: 4350.75217962265\n",
      "  timestamp: 1575769589\n",
      "  timesteps_since_restore: 88000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 44\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4350 s, 44 iter, 88000 ts, 48 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-47-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.47051415939144\n",
      "  episode_reward_mean: 47.86642969715226\n",
      "  episode_reward_min: -5.951778018378718\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7684.834\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7508783340454102\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019662562757730484\n",
      "        policy_loss: -0.03248932585120201\n",
      "        total_loss: 155.9644317626953\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 155.98699951171875\n",
      "    load_time_ms: 2.984\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 90000\n",
      "    sample_time_ms: 55962.027\n",
      "    update_time_ms: 50.683\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.21935483870969\n",
      "    ram_util_percent: 63.275268817204285\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 201.02661101510049\n",
      "    mean_inference_ms: 17.160914848465026\n",
      "    mean_processing_ms: 30.634889332259462\n",
      "  time_since_restore: 4416.147807359695\n",
      "  time_this_iter_s: 65.39562773704529\n",
      "  time_total_s: 4416.147807359695\n",
      "  timestamp: 1575769655\n",
      "  timesteps_since_restore: 90000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 45\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4416 s, 45 iter, 90000 ts, 47.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-48-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.47051415939144\n",
      "  episode_reward_mean: 47.83747661208392\n",
      "  episode_reward_min: -5.951778018378718\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7803.783\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9015662670135498\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028675008565187454\n",
      "        policy_loss: -0.050629258155822754\n",
      "        total_loss: 165.36134338378906\n",
      "        vf_explained_var: -3.5762786865234375e-07\n",
      "        vf_loss: 165.3974151611328\n",
      "    load_time_ms: 2.972\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 92000\n",
      "    sample_time_ms: 55829.049\n",
      "    update_time_ms: 59.066\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.46043956043958\n",
      "    ram_util_percent: 63.296703296703285\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 200.2025388176524\n",
      "    mean_inference_ms: 17.068296869298187\n",
      "    mean_processing_ms: 30.49874698372413\n",
      "  time_since_restore: 4479.21129322052\n",
      "  time_this_iter_s: 63.063485860824585\n",
      "  time_total_s: 4479.21129322052\n",
      "  timestamp: 1575769718\n",
      "  timesteps_since_restore: 92000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 46\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4479 s, 46 iter, 92000 ts, 47.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-49-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.35085068570154\n",
      "  episode_reward_mean: 48.62954640706156\n",
      "  episode_reward_min: -5.951778018378718\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7788.436\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9834158420562744\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01644003763794899\n",
      "        policy_loss: -0.033508267253637314\n",
      "        total_loss: 189.29124450683594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 189.31646728515625\n",
      "    load_time_ms: 2.909\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 94000\n",
      "    sample_time_ms: 55562.343\n",
      "    update_time_ms: 59.669\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.61348314606742\n",
      "    ram_util_percent: 63.366292134831454\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 198.85643317985355\n",
      "    mean_inference_ms: 16.94237888093146\n",
      "    mean_processing_ms: 30.29644146317758\n",
      "  time_since_restore: 4541.92019701004\n",
      "  time_this_iter_s: 62.708903789520264\n",
      "  time_total_s: 4541.92019701004\n",
      "  timestamp: 1575769781\n",
      "  timesteps_since_restore: 94000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 47\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4541 s, 47 iter, 94000 ts, 48.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-50-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.35085068570154\n",
      "  episode_reward_mean: 48.86068454803692\n",
      "  episode_reward_min: -6.2143488544168\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7638.041\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6787561178207397\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02538137696683407\n",
      "        policy_loss: -0.0447610579431057\n",
      "        total_loss: 174.58238220214844\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 174.6142120361328\n",
      "    load_time_ms: 2.872\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 96000\n",
      "    sample_time_ms: 55702.212\n",
      "    update_time_ms: 57.684\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.88021978021979\n",
      "    ram_util_percent: 63.403296703296704\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 196.360129205683\n",
      "    mean_inference_ms: 16.74859201958631\n",
      "    mean_processing_ms: 30.022776384862826\n",
      "  time_since_restore: 4605.705939292908\n",
      "  time_this_iter_s: 63.78574228286743\n",
      "  time_total_s: 4605.705939292908\n",
      "  timestamp: 1575769844\n",
      "  timesteps_since_restore: 96000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 48\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4605 s, 48 iter, 96000 ts, 48.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.35085068570154\n",
      "  episode_reward_mean: 48.20301381873816\n",
      "  episode_reward_min: -10.749621032788893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7704.95\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3712432384490967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023583631962537766\n",
      "        policy_loss: -0.03806040436029434\n",
      "        total_loss: 119.20085144042969\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 119.22697448730469\n",
      "    load_time_ms: 2.926\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 98000\n",
      "    sample_time_ms: 55835.991\n",
      "    update_time_ms: 58.27\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.87849462365593\n",
      "    ram_util_percent: 63.42795698924732\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 194.96578264350893\n",
      "    mean_inference_ms: 16.619507672435773\n",
      "    mean_processing_ms: 29.8173790711831\n",
      "  time_since_restore: 4670.597114801407\n",
      "  time_this_iter_s: 64.89117550849915\n",
      "  time_total_s: 4670.597114801407\n",
      "  timestamp: 1575769909\n",
      "  timesteps_since_restore: 98000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 49\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4670 s, 49 iter, 98000 ts, 48.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-52-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.35085068570154\n",
      "  episode_reward_mean: 48.59532948130836\n",
      "  episode_reward_min: -10.749621032788893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7699.936\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8730573654174805\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0353139229118824\n",
      "        policy_loss: -0.05016040429472923\n",
      "        total_loss: 178.3033905029297\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 178.33563232421875\n",
      "    load_time_ms: 3.003\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 100000\n",
      "    sample_time_ms: 56465.765\n",
      "    update_time_ms: 59.515\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.37676767676767\n",
      "    ram_util_percent: 63.51717171717171\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 193.67918882242847\n",
      "    mean_inference_ms: 16.50123803746105\n",
      "    mean_processing_ms: 29.638863607404275\n",
      "  time_since_restore: 4740.168894290924\n",
      "  time_this_iter_s: 69.57177948951721\n",
      "  time_total_s: 4740.168894290924\n",
      "  timestamp: 1575769979\n",
      "  timesteps_since_restore: 100000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 50\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4740 s, 50 iter, 100000 ts, 48.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-54-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.33561781363962\n",
      "  episode_reward_mean: 48.24956636024685\n",
      "  episode_reward_min: -10.749621032788893\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7697.867\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9936840534210205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030041975900530815\n",
      "        policy_loss: -0.04931657388806343\n",
      "        total_loss: 178.07321166992188\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 178.10728454589844\n",
      "    load_time_ms: 2.98\n",
      "    num_steps_sampled: 102000\n",
      "    num_steps_trained: 102000\n",
      "    sample_time_ms: 56492.256\n",
      "    update_time_ms: 59.404\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.09670329670328\n",
      "    ram_util_percent: 63.56373626373627\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 191.5933768217315\n",
      "    mean_inference_ms: 16.333683714077893\n",
      "    mean_processing_ms: 29.34133117569847\n",
      "  time_since_restore: 4803.837470531464\n",
      "  time_this_iter_s: 63.66857624053955\n",
      "  time_total_s: 4803.837470531464\n",
      "  timestamp: 1575770043\n",
      "  timesteps_since_restore: 102000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 102000\n",
      "  training_iteration: 51\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4803 s, 51 iter, 102000 ts, 48.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-55-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.66370443616285\n",
      "  episode_reward_mean: 48.25944760878443\n",
      "  episode_reward_min: -16.697542867403396\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7850.271\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.135395050048828\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023228954523801804\n",
      "        policy_loss: -0.0419948436319828\n",
      "        total_loss: 195.88412475585938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 195.91444396972656\n",
      "    load_time_ms: 3.306\n",
      "    num_steps_sampled: 104000\n",
      "    num_steps_trained: 104000\n",
      "    sample_time_ms: 56575.924\n",
      "    update_time_ms: 60.138\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.9557894736842\n",
      "    ram_util_percent: 63.445263157894736\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 189.92994897474776\n",
      "    mean_inference_ms: 16.204438974212866\n",
      "    mean_processing_ms: 29.110170459943912\n",
      "  time_since_restore: 4870.159162044525\n",
      "  time_this_iter_s: 66.32169151306152\n",
      "  time_total_s: 4870.159162044525\n",
      "  timestamp: 1575770109\n",
      "  timesteps_since_restore: 104000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 104000\n",
      "  training_iteration: 52\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4870 s, 52 iter, 104000 ts, 48.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-56-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.66370443616285\n",
      "  episode_reward_mean: 47.07836427942835\n",
      "  episode_reward_min: -16.697542867403396\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7869.784\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.028040885925293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015530748292803764\n",
      "        policy_loss: -0.029409203678369522\n",
      "        total_loss: 159.13768005371094\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 159.15931701660156\n",
      "    load_time_ms: 3.264\n",
      "    num_steps_sampled: 106000\n",
      "    num_steps_trained: 106000\n",
      "    sample_time_ms: 56683.46\n",
      "    update_time_ms: 62.398\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.83736263736265\n",
      "    ram_util_percent: 63.48791208791207\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 189.38501539020007\n",
      "    mean_inference_ms: 16.14180114896736\n",
      "    mean_processing_ms: 28.9551118909187\n",
      "  time_since_restore: 4934.420809984207\n",
      "  time_this_iter_s: 64.261647939682\n",
      "  time_total_s: 4934.420809984207\n",
      "  timestamp: 1575770173\n",
      "  timesteps_since_restore: 106000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 106000\n",
      "  training_iteration: 53\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4934 s, 53 iter, 106000 ts, 47.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-57-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.66370443616285\n",
      "  episode_reward_mean: 50.597522559835376\n",
      "  episode_reward_min: -16.697542867403396\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7889.669\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9917211532592773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01691988855600357\n",
      "        policy_loss: -0.02880462072789669\n",
      "        total_loss: 188.16665649414062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 188.1868133544922\n",
      "    load_time_ms: 3.246\n",
      "    num_steps_sampled: 108000\n",
      "    num_steps_trained: 108000\n",
      "    sample_time_ms: 56696.575\n",
      "    update_time_ms: 62.731\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.62087912087912\n",
      "    ram_util_percent: 63.54725274725274\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 188.34546803386024\n",
      "    mean_inference_ms: 16.047439328359737\n",
      "    mean_processing_ms: 28.79429478555945\n",
      "  time_since_restore: 4997.755643129349\n",
      "  time_this_iter_s: 63.3348331451416\n",
      "  time_total_s: 4997.755643129349\n",
      "  timestamp: 1575770237\n",
      "  timesteps_since_restore: 108000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 108000\n",
      "  training_iteration: 54\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 4997 s, 54 iter, 108000 ts, 50.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-58-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.66370443616285\n",
      "  episode_reward_mean: 50.07087170257942\n",
      "  episode_reward_min: -16.697542867403396\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7784.041\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.855654001235962\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016788242384791374\n",
      "        policy_loss: -0.03317710757255554\n",
      "        total_loss: 160.01039123535156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 160.03509521484375\n",
      "    load_time_ms: 3.276\n",
      "    num_steps_sampled: 110000\n",
      "    num_steps_trained: 110000\n",
      "    sample_time_ms: 56571.586\n",
      "    update_time_ms: 63.125\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.79111111111112\n",
      "    ram_util_percent: 63.59777777777778\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 187.81997894403946\n",
      "    mean_inference_ms: 15.983372708692563\n",
      "    mean_processing_ms: 28.671785512665792\n",
      "  time_since_restore: 5060.7570123672485\n",
      "  time_this_iter_s: 63.00136923789978\n",
      "  time_total_s: 5060.7570123672485\n",
      "  timestamp: 1575770300\n",
      "  timesteps_since_restore: 110000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 110000\n",
      "  training_iteration: 55\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5060 s, 55 iter, 110000 ts, 50.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_20-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.66370443616285\n",
      "  episode_reward_mean: 48.078830857132004\n",
      "  episode_reward_min: -16.697542867403396\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7630.281\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.205984354019165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01811269298195839\n",
      "        policy_loss: -0.03322366997599602\n",
      "        total_loss: 160.85302734375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 160.87705993652344\n",
      "    load_time_ms: 3.22\n",
      "    num_steps_sampled: 112000\n",
      "    num_steps_trained: 112000\n",
      "    sample_time_ms: 56764.139\n",
      "    update_time_ms: 55.743\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.97111111111113\n",
      "    ram_util_percent: 63.65888888888888\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 187.8095928766167\n",
      "    mean_inference_ms: 15.954149927225727\n",
      "    mean_processing_ms: 28.582940225916662\n",
      "  time_since_restore: 5124.139492034912\n",
      "  time_this_iter_s: 63.382479667663574\n",
      "  time_total_s: 5124.139492034912\n",
      "  timestamp: 1575770363\n",
      "  timesteps_since_restore: 112000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 112000\n",
      "  training_iteration: 56\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5124 s, 56 iter, 112000 ts, 48.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-00-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.0441756891148\n",
      "  episode_reward_mean: 46.631628666685145\n",
      "  episode_reward_min: -4.168450914084143\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7490.328\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3847100734710693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015691976994276047\n",
      "        policy_loss: -0.030252976343035698\n",
      "        total_loss: 154.73056030273438\n",
      "        vf_explained_var: -2.384185791015625e-07\n",
      "        vf_loss: 154.7528533935547\n",
      "    load_time_ms: 3.27\n",
      "    num_steps_sampled: 114000\n",
      "    num_steps_trained: 114000\n",
      "    sample_time_ms: 57101.983\n",
      "    update_time_ms: 56.275\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.77065217391304\n",
      "    ram_util_percent: 63.73152173913041\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 187.2282324595973\n",
      "    mean_inference_ms: 15.889703567161064\n",
      "    mean_processing_ms: 28.469534430990116\n",
      "  time_since_restore: 5188.844676017761\n",
      "  time_this_iter_s: 64.70518398284912\n",
      "  time_total_s: 5188.844676017761\n",
      "  timestamp: 1575770428\n",
      "  timesteps_since_restore: 114000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 114000\n",
      "  training_iteration: 57\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5188 s, 57 iter, 114000 ts, 46.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-01-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.0441756891148\n",
      "  episode_reward_mean: 46.81895544873351\n",
      "  episode_reward_min: -22.29804725229849\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7493.104\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.321352958679199\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017717787995934486\n",
      "        policy_loss: -0.027881192043423653\n",
      "        total_loss: 184.40292358398438\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 184.4218292236328\n",
      "    load_time_ms: 3.258\n",
      "    num_steps_sampled: 116000\n",
      "    num_steps_trained: 116000\n",
      "    sample_time_ms: 57033.533\n",
      "    update_time_ms: 56.501\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.78888888888889\n",
      "    ram_util_percent: 63.7711111111111\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 186.1001590159198\n",
      "    mean_inference_ms: 15.794180706074153\n",
      "    mean_processing_ms: 28.35206194019208\n",
      "  time_since_restore: 5251.9740653038025\n",
      "  time_this_iter_s: 63.12938928604126\n",
      "  time_total_s: 5251.9740653038025\n",
      "  timestamp: 1575770491\n",
      "  timesteps_since_restore: 116000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 116000\n",
      "  training_iteration: 58\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5251 s, 58 iter, 116000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-02-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.0441756891148\n",
      "  episode_reward_mean: 47.78715334183507\n",
      "  episode_reward_min: -22.29804725229849\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7580.16\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7859549522399902\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029190300032496452\n",
      "        policy_loss: -0.04911893233656883\n",
      "        total_loss: 198.65765380859375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 198.6919708251953\n",
      "    load_time_ms: 3.178\n",
      "    num_steps_sampled: 118000\n",
      "    num_steps_trained: 118000\n",
      "    sample_time_ms: 57022.955\n",
      "    update_time_ms: 55.217\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.24148936170214\n",
      "    ram_util_percent: 63.65531914893619\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 185.62345170888653\n",
      "    mean_inference_ms: 15.73633801696762\n",
      "    mean_processing_ms: 28.270442325281056\n",
      "  time_since_restore: 5317.62344956398\n",
      "  time_this_iter_s: 65.64938426017761\n",
      "  time_total_s: 5317.62344956398\n",
      "  timestamp: 1575770557\n",
      "  timesteps_since_restore: 118000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 118000\n",
      "  training_iteration: 59\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5317 s, 59 iter, 118000 ts, 47.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.0441756891148\n",
      "  episode_reward_mean: 48.22548840135791\n",
      "  episode_reward_min: -22.29804725229849\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7729.129\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9324836730957031\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024521468207240105\n",
      "        policy_loss: -0.03957507386803627\n",
      "        total_loss: 178.91119384765625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 178.93829345703125\n",
      "    load_time_ms: 3.036\n",
      "    num_steps_sampled: 120000\n",
      "    num_steps_trained: 120000\n",
      "    sample_time_ms: 56257.743\n",
      "    update_time_ms: 53.213\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59120879120879\n",
      "    ram_util_percent: 63.70000000000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 184.67607252241345\n",
      "    mean_inference_ms: 15.649854239013976\n",
      "    mean_processing_ms: 28.155007850059498\n",
      "  time_since_restore: 5381.032531261444\n",
      "  time_this_iter_s: 63.40908169746399\n",
      "  time_total_s: 5381.032531261444\n",
      "  timestamp: 1575770620\n",
      "  timesteps_since_restore: 120000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 120000\n",
      "  training_iteration: 60\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5381 s, 60 iter, 120000 ts, 48.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-04-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.0441756891148\n",
      "  episode_reward_mean: 50.281592974249534\n",
      "  episode_reward_min: -22.29804725229849\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7730.527\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8858407735824585\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014049515128135681\n",
      "        policy_loss: -0.030894970521330833\n",
      "        total_loss: 182.50555419921875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 182.52935791015625\n",
      "    load_time_ms: 3.109\n",
      "    num_steps_sampled: 122000\n",
      "    num_steps_trained: 122000\n",
      "    sample_time_ms: 56247.858\n",
      "    update_time_ms: 53.781\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.35\n",
      "    ram_util_percent: 63.728888888888896\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 182.86699618627085\n",
      "    mean_inference_ms: 15.512699268509362\n",
      "    mean_processing_ms: 27.969610341258036\n",
      "  time_since_restore: 5444.631598234177\n",
      "  time_this_iter_s: 63.599066972732544\n",
      "  time_total_s: 5444.631598234177\n",
      "  timestamp: 1575770684\n",
      "  timesteps_since_restore: 122000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 122000\n",
      "  training_iteration: 61\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5444 s, 61 iter, 122000 ts, 50.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-05-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.7978546533665\n",
      "  episode_reward_mean: 51.51781737959357\n",
      "  episode_reward_min: -22.29804725229849\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7571.987\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.164309501647949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026055363938212395\n",
      "        policy_loss: -0.04471633583307266\n",
      "        total_loss: 182.9186248779297\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 182.9500274658203\n",
      "    load_time_ms: 2.781\n",
      "    num_steps_sampled: 124000\n",
      "    num_steps_trained: 124000\n",
      "    sample_time_ms: 56083.41\n",
      "    update_time_ms: 53.525\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56\n",
      "    ram_util_percent: 63.817777777777785\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 181.62792628299098\n",
      "    mean_inference_ms: 15.40865444746596\n",
      "    mean_processing_ms: 27.783713068497754\n",
      "  time_since_restore: 5507.7180716991425\n",
      "  time_this_iter_s: 63.08647346496582\n",
      "  time_total_s: 5507.7180716991425\n",
      "  timestamp: 1575770747\n",
      "  timesteps_since_restore: 124000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 124000\n",
      "  training_iteration: 62\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5507 s, 62 iter, 124000 ts, 51.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-06-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.25049911159103\n",
      "  episode_reward_mean: 51.224205995184505\n",
      "  episode_reward_min: -4.374390042562869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7551.866\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.023334264755249\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022646695375442505\n",
      "        policy_loss: -0.035115502774715424\n",
      "        total_loss: 171.32437133789062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 171.3480224609375\n",
      "    load_time_ms: 2.821\n",
      "    num_steps_sampled: 126000\n",
      "    num_steps_trained: 126000\n",
      "    sample_time_ms: 56138.919\n",
      "    update_time_ms: 52.063\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.83225806451613\n",
      "    ram_util_percent: 63.86129032258067\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 181.61464450099413\n",
      "    mean_inference_ms: 15.380227728616214\n",
      "    mean_processing_ms: 27.775831332788346\n",
      "  time_since_restore: 5572.304714679718\n",
      "  time_this_iter_s: 64.58664298057556\n",
      "  time_total_s: 5572.304714679718\n",
      "  timestamp: 1575770812\n",
      "  timesteps_since_restore: 126000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 126000\n",
      "  training_iteration: 63\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5572 s, 63 iter, 126000 ts, 51.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-07-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.34174921367604\n",
      "  episode_reward_mean: 49.50457130892077\n",
      "  episode_reward_min: -4.374390042562869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7524.118\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.165459156036377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011370760388672352\n",
      "        policy_loss: -0.02318781241774559\n",
      "        total_loss: 185.44969177246094\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 185.4671630859375\n",
      "    load_time_ms: 2.929\n",
      "    num_steps_sampled: 128000\n",
      "    num_steps_trained: 128000\n",
      "    sample_time_ms: 56160.262\n",
      "    update_time_ms: 51.652\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.79222222222221\n",
      "    ram_util_percent: 63.93000000000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 180.76576720129785\n",
      "    mean_inference_ms: 15.306638665335186\n",
      "    mean_processing_ms: 27.649639144823496\n",
      "  time_since_restore: 5635.573718070984\n",
      "  time_this_iter_s: 63.26900339126587\n",
      "  time_total_s: 5635.573718070984\n",
      "  timestamp: 1575770875\n",
      "  timesteps_since_restore: 128000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 128000\n",
      "  training_iteration: 64\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5635 s, 64 iter, 128000 ts, 49.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-08-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.24218977878039\n",
      "  episode_reward_mean: 50.30873029253699\n",
      "  episode_reward_min: -4.374390042562869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7518.358\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7860897779464722\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02099217288196087\n",
      "        policy_loss: -0.036605317145586014\n",
      "        total_loss: 191.45529174804688\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 191.4812774658203\n",
      "    load_time_ms: 2.894\n",
      "    num_steps_sampled: 130000\n",
      "    num_steps_trained: 130000\n",
      "    sample_time_ms: 56246.772\n",
      "    update_time_ms: 51.853\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.84175824175824\n",
      "    ram_util_percent: 63.97142857142859\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 179.84812095859135\n",
      "    mean_inference_ms: 15.22828863782263\n",
      "    mean_processing_ms: 27.53064303391483\n",
      "  time_since_restore: 5699.389853954315\n",
      "  time_this_iter_s: 63.8161358833313\n",
      "  time_total_s: 5699.389853954315\n",
      "  timestamp: 1575770939\n",
      "  timesteps_since_restore: 130000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 130000\n",
      "  training_iteration: 65\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5699 s, 65 iter, 130000 ts, 50.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-07 21:10:05,135\tWARNING util.py:145 -- The `process_trial` operation took 0.1101689338684082 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-10-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.24218977878039\n",
      "  episode_reward_mean: 47.08847134731819\n",
      "  episode_reward_min: -18.980767290147917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7631.684\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.459232807159424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014644804410636425\n",
      "        policy_loss: -0.025912748649716377\n",
      "        total_loss: 183.94227600097656\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 183.96072387695312\n",
      "    load_time_ms: 2.911\n",
      "    num_steps_sampled: 132000\n",
      "    num_steps_trained: 132000\n",
      "    sample_time_ms: 56363.599\n",
      "    update_time_ms: 50.965\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.43617021276596\n",
      "    ram_util_percent: 63.8691489361702\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 179.3939233296713\n",
      "    mean_inference_ms: 15.184875021987777\n",
      "    mean_processing_ms: 27.46228780803405\n",
      "  time_since_restore: 5765.124932527542\n",
      "  time_this_iter_s: 65.73507857322693\n",
      "  time_total_s: 5765.124932527542\n",
      "  timestamp: 1575771005\n",
      "  timesteps_since_restore: 132000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 132000\n",
      "  training_iteration: 66\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5765 s, 66 iter, 132000 ts, 47.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-11-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.92964470740597\n",
      "  episode_reward_mean: 46.368922045689814\n",
      "  episode_reward_min: -18.980767290147917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7783.757\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0092554092407227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01930001750588417\n",
      "        policy_loss: -0.03434484452009201\n",
      "        total_loss: 174.3424835205078\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 174.36708068847656\n",
      "    load_time_ms: 2.831\n",
      "    num_steps_sampled: 134000\n",
      "    num_steps_trained: 134000\n",
      "    sample_time_ms: 56119.88\n",
      "    update_time_ms: 61.032\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.73846153846155\n",
      "    ram_util_percent: 63.90329670329671\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 178.77615055634138\n",
      "    mean_inference_ms: 15.12432025948106\n",
      "    mean_processing_ms: 27.37909305972103\n",
      "  time_since_restore: 5829.019098520279\n",
      "  time_this_iter_s: 63.894165992736816\n",
      "  time_total_s: 5829.019098520279\n",
      "  timestamp: 1575771069\n",
      "  timesteps_since_restore: 134000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 134000\n",
      "  training_iteration: 67\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5829 s, 67 iter, 134000 ts, 46.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-12-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.16748340462463\n",
      "  episode_reward_mean: 46.806866071573175\n",
      "  episode_reward_min: -18.980767290147917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7796.426\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8262748718261719\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016727739945054054\n",
      "        policy_loss: -0.03246826305985451\n",
      "        total_loss: 192.73410034179688\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 192.75811767578125\n",
      "    load_time_ms: 3.54\n",
      "    num_steps_sampled: 136000\n",
      "    num_steps_trained: 136000\n",
      "    sample_time_ms: 56109.397\n",
      "    update_time_ms: 59.913\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.38888888888889\n",
      "    ram_util_percent: 63.964444444444446\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 176.90172729152908\n",
      "    mean_inference_ms: 14.993020479622885\n",
      "    mean_processing_ms: 27.115990630824598\n",
      "  time_since_restore: 5892.1733655929565\n",
      "  time_this_iter_s: 63.15426707267761\n",
      "  time_total_s: 5892.1733655929565\n",
      "  timestamp: 1575771132\n",
      "  timesteps_since_restore: 136000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 136000\n",
      "  training_iteration: 68\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5892 s, 68 iter, 136000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.16748340462463\n",
      "  episode_reward_mean: 46.42489931979466\n",
      "  episode_reward_min: -18.980767290147917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7629.728\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.206300735473633\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018292255699634552\n",
      "        policy_loss: -0.03436294570565224\n",
      "        total_loss: 204.46591186523438\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 204.49105834960938\n",
      "    load_time_ms: 3.701\n",
      "    num_steps_sampled: 138000\n",
      "    num_steps_trained: 138000\n",
      "    sample_time_ms: 55937.488\n",
      "    update_time_ms: 60.137\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.81685393258428\n",
      "    ram_util_percent: 63.98314606741573\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 175.7430262597676\n",
      "    mean_inference_ms: 14.903745290759213\n",
      "    mean_processing_ms: 26.959900761893167\n",
      "  time_since_restore: 5954.446606397629\n",
      "  time_this_iter_s: 62.27324080467224\n",
      "  time_total_s: 5954.446606397629\n",
      "  timestamp: 1575771194\n",
      "  timesteps_since_restore: 138000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 138000\n",
      "  training_iteration: 69\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 5954 s, 69 iter, 138000 ts, 46.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.16748340462463\n",
      "  episode_reward_mean: 45.76730427007175\n",
      "  episode_reward_min: -18.980767290147917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7464.415\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1412298679351807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020820491015911102\n",
      "        policy_loss: -0.030213749036192894\n",
      "        total_loss: 204.62115478515625\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 204.64085388183594\n",
      "    load_time_ms: 4.467\n",
      "    num_steps_sampled: 140000\n",
      "    num_steps_trained: 140000\n",
      "    sample_time_ms: 56092.931\n",
      "    update_time_ms: 60.61\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16111111111111\n",
      "    ram_util_percent: 64.06555555555556\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 175.6306269086009\n",
      "    mean_inference_ms: 14.884305714408551\n",
      "    mean_processing_ms: 26.886997139331246\n",
      "  time_since_restore: 6017.748252391815\n",
      "  time_this_iter_s: 63.3016459941864\n",
      "  time_total_s: 6017.748252391815\n",
      "  timestamp: 1575771257\n",
      "  timesteps_since_restore: 140000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 140000\n",
      "  training_iteration: 70\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6017 s, 70 iter, 140000 ts, 45.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-15-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.16748340462463\n",
      "  episode_reward_mean: 46.47876722968883\n",
      "  episode_reward_min: -17.552413194559364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7459.662\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1107959747314453\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02545967884361744\n",
      "        policy_loss: -0.04355715215206146\n",
      "        total_loss: 199.13645935058594\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 199.16714477539062\n",
      "    load_time_ms: 4.408\n",
      "    num_steps_sampled: 142000\n",
      "    num_steps_trained: 142000\n",
      "    sample_time_ms: 56243.336\n",
      "    update_time_ms: 60.45\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.77849462365593\n",
      "    ram_util_percent: 64.1483870967742\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 175.63600132246313\n",
      "    mean_inference_ms: 14.86241325036567\n",
      "    mean_processing_ms: 26.835562746150508\n",
      "  time_since_restore: 6082.8032393455505\n",
      "  time_this_iter_s: 65.05498695373535\n",
      "  time_total_s: 6082.8032393455505\n",
      "  timestamp: 1575771322\n",
      "  timesteps_since_restore: 142000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 142000\n",
      "  training_iteration: 71\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6082 s, 71 iter, 142000 ts, 46.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-16-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.16748340462463\n",
      "  episode_reward_mean: 46.57058776354383\n",
      "  episode_reward_min: -17.552413194559364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7457.474\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.139725685119629\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01915030926465988\n",
      "        policy_loss: -0.03615814447402954\n",
      "        total_loss: 185.89199829101562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 185.91844177246094\n",
      "    load_time_ms: 4.571\n",
      "    num_steps_sampled: 144000\n",
      "    num_steps_trained: 144000\n",
      "    sample_time_ms: 56257.905\n",
      "    update_time_ms: 60.712\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56111111111112\n",
      "    ram_util_percent: 64.20444444444443\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 175.14506519301466\n",
      "    mean_inference_ms: 14.81551043523883\n",
      "    mean_processing_ms: 26.712338651541\n",
      "  time_since_restore: 6146.012430429459\n",
      "  time_this_iter_s: 63.20919108390808\n",
      "  time_total_s: 6146.012430429459\n",
      "  timestamp: 1575771386\n",
      "  timesteps_since_restore: 144000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 144000\n",
      "  training_iteration: 72\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6146 s, 72 iter, 144000 ts, 46.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-17-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.85412821543655\n",
      "  episode_reward_mean: 46.939677838395085\n",
      "  episode_reward_min: -17.552413194559364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7461.081\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8921555280685425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019578827545046806\n",
      "        policy_loss: -0.032241255044937134\n",
      "        total_loss: 189.96963500976562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 189.99203491210938\n",
      "    load_time_ms: 4.629\n",
      "    num_steps_sampled: 146000\n",
      "    num_steps_trained: 146000\n",
      "    sample_time_ms: 56114.204\n",
      "    update_time_ms: 60.988\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.65054945054946\n",
      "    ram_util_percent: 64.04395604395606\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 174.80928484940833\n",
      "    mean_inference_ms: 14.773140482009563\n",
      "    mean_processing_ms: 26.657583949375056\n",
      "  time_since_restore: 6209.213937759399\n",
      "  time_this_iter_s: 63.201507329940796\n",
      "  time_total_s: 6209.213937759399\n",
      "  timestamp: 1575771449\n",
      "  timesteps_since_restore: 146000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 146000\n",
      "  training_iteration: 73\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6209 s, 73 iter, 146000 ts, 46.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-18-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.85412821543655\n",
      "  episode_reward_mean: 46.86476293402952\n",
      "  episode_reward_min: -17.552413194559364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7565.36\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3276185989379883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01313687302172184\n",
      "        policy_loss: -0.020852407440543175\n",
      "        total_loss: 171.6986541748047\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 171.71287536621094\n",
      "    load_time_ms: 4.519\n",
      "    num_steps_sampled: 148000\n",
      "    num_steps_trained: 148000\n",
      "    sample_time_ms: 56250.13\n",
      "    update_time_ms: 61.663\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.42021276595744\n",
      "    ram_util_percent: 64.08191489361704\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 174.2442408798293\n",
      "    mean_inference_ms: 14.726173212997413\n",
      "    mean_processing_ms: 26.561537576957218\n",
      "  time_since_restore: 6274.991410493851\n",
      "  time_this_iter_s: 65.7774727344513\n",
      "  time_total_s: 6274.991410493851\n",
      "  timestamp: 1575771515\n",
      "  timesteps_since_restore: 148000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 148000\n",
      "  training_iteration: 74\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6274 s, 74 iter, 148000 ts, 46.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-19-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.70331352263474\n",
      "  episode_reward_mean: 47.26238078973411\n",
      "  episode_reward_min: -16.524477121419245\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7707.039\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1140151023864746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02104637585580349\n",
      "        policy_loss: -0.036154523491859436\n",
      "        total_loss: 203.08018493652344\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 203.10568237304688\n",
      "    load_time_ms: 4.494\n",
      "    num_steps_sampled: 150000\n",
      "    num_steps_trained: 150000\n",
      "    sample_time_ms: 56117.019\n",
      "    update_time_ms: 67.423\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.62857142857143\n",
      "    ram_util_percent: 64.12747252747253\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 172.81059961239313\n",
      "    mean_inference_ms: 14.62118101091826\n",
      "    mean_processing_ms: 26.411970340501203\n",
      "  time_since_restore: 6338.940133810043\n",
      "  time_this_iter_s: 63.94872331619263\n",
      "  time_total_s: 6338.940133810043\n",
      "  timestamp: 1575771579\n",
      "  timesteps_since_restore: 150000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 150000\n",
      "  training_iteration: 75\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6338 s, 75 iter, 150000 ts, 47.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-20-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.70331352263474\n",
      "  episode_reward_mean: 48.326100461363815\n",
      "  episode_reward_min: -10.150447193688912\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7683.42\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.017751455307007\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02413717284798622\n",
      "        policy_loss: -0.04152660816907883\n",
      "        total_loss: 195.08978271484375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 195.11911010742188\n",
      "    load_time_ms: 4.915\n",
      "    num_steps_sampled: 152000\n",
      "    num_steps_trained: 152000\n",
      "    sample_time_ms: 55859.741\n",
      "    update_time_ms: 66.319\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.12444444444445\n",
      "    ram_util_percent: 64.22\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 171.36331869386683\n",
      "    mean_inference_ms: 14.51519142594962\n",
      "    mean_processing_ms: 26.250770192119504\n",
      "  time_since_restore: 6401.797427177429\n",
      "  time_this_iter_s: 62.857293367385864\n",
      "  time_total_s: 6401.797427177429\n",
      "  timestamp: 1575771642\n",
      "  timesteps_since_restore: 152000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 152000\n",
      "  training_iteration: 76\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6401 s, 76 iter, 152000 ts, 48.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-21-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.70331352263474\n",
      "  episode_reward_mean: 48.041227584593045\n",
      "  episode_reward_min: -7.733037714102455\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7534.288\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.410301685333252\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021635161712765694\n",
      "        policy_loss: -0.03026692382991314\n",
      "        total_loss: 192.4880828857422\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 192.5074005126953\n",
      "    load_time_ms: 5.321\n",
      "    num_steps_sampled: 154000\n",
      "    num_steps_trained: 154000\n",
      "    sample_time_ms: 56019.225\n",
      "    update_time_ms: 55.601\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.15934065934066\n",
      "    ram_util_percent: 64.23846153846152\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 170.70027659298387\n",
      "    mean_inference_ms: 14.46069717483974\n",
      "    mean_processing_ms: 26.164244357658564\n",
      "  time_since_restore: 6465.679480552673\n",
      "  time_this_iter_s: 63.88205337524414\n",
      "  time_total_s: 6465.679480552673\n",
      "  timestamp: 1575771706\n",
      "  timesteps_since_restore: 154000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 154000\n",
      "  training_iteration: 77\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6465 s, 77 iter, 154000 ts, 48 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-22-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.70331352263474\n",
      "  episode_reward_mean: 48.96048519029396\n",
      "  episode_reward_min: -7.733037714102455\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7534.356\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7613862752914429\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01796845719218254\n",
      "        policy_loss: -0.030725523829460144\n",
      "        total_loss: 204.56857299804688\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 204.59019470214844\n",
      "    load_time_ms: 4.708\n",
      "    num_steps_sampled: 156000\n",
      "    num_steps_trained: 156000\n",
      "    sample_time_ms: 56022.98\n",
      "    update_time_ms: 56.238\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.88888888888889\n",
      "    ram_util_percent: 64.25111111111111\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 170.2277587936297\n",
      "    mean_inference_ms: 14.420088849445456\n",
      "    mean_processing_ms: 26.096159085119726\n",
      "  time_since_restore: 6528.877123832703\n",
      "  time_this_iter_s: 63.1976432800293\n",
      "  time_total_s: 6528.877123832703\n",
      "  timestamp: 1575771769\n",
      "  timesteps_since_restore: 156000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 156000\n",
      "  training_iteration: 78\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6528 s, 78 iter, 156000 ts, 49 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-23-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 93.70331352263474\n",
      "  episode_reward_mean: 48.385744281810055\n",
      "  episode_reward_min: -12.302137436912638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7521.5\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.176201581954956\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019064927473664284\n",
      "        policy_loss: -0.03424440696835518\n",
      "        total_loss: 198.17196655273438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 198.19656372070312\n",
      "    load_time_ms: 4.568\n",
      "    num_steps_sampled: 158000\n",
      "    num_steps_trained: 158000\n",
      "    sample_time_ms: 56090.391\n",
      "    update_time_ms: 58.096\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59333333333333\n",
      "    ram_util_percent: 64.36\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 170.46357748972193\n",
      "    mean_inference_ms: 14.415962972485518\n",
      "    mean_processing_ms: 26.102440351453197\n",
      "  time_since_restore: 6591.69678235054\n",
      "  time_this_iter_s: 62.819658517837524\n",
      "  time_total_s: 6591.69678235054\n",
      "  timestamp: 1575771832\n",
      "  timesteps_since_restore: 158000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 158000\n",
      "  training_iteration: 79\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6591 s, 79 iter, 158000 ts, 48.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.10561038395478\n",
      "  episode_reward_mean: 46.13053039352693\n",
      "  episode_reward_min: -12.302137436912638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7519.892\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5451672077178955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020283477380871773\n",
      "        policy_loss: -0.032079119235277176\n",
      "        total_loss: 203.64955139160156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 203.6713409423828\n",
      "    load_time_ms: 3.798\n",
      "    num_steps_sampled: 160000\n",
      "    num_steps_trained: 160000\n",
      "    sample_time_ms: 56314.515\n",
      "    update_time_ms: 59.205\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.60645161290323\n",
      "    ram_util_percent: 64.26451612903227\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 169.94487654771672\n",
      "    mean_inference_ms: 14.36971319620102\n",
      "    mean_processing_ms: 26.036740627171376\n",
      "  time_since_restore: 6657.240712404251\n",
      "  time_this_iter_s: 65.54393005371094\n",
      "  time_total_s: 6657.240712404251\n",
      "  timestamp: 1575771897\n",
      "  timesteps_since_restore: 160000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 160000\n",
      "  training_iteration: 80\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6657 s, 80 iter, 160000 ts, 46.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.10561038395478\n",
      "  episode_reward_mean: 46.66629150356483\n",
      "  episode_reward_min: -12.302137436912638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7535.18\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3593590259552\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017283970490098\n",
      "        policy_loss: -0.032490551471710205\n",
      "        total_loss: 216.27862548828125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.3023223876953\n",
      "    load_time_ms: 4.592\n",
      "    num_steps_sampled: 162000\n",
      "    num_steps_trained: 162000\n",
      "    sample_time_ms: 56103.742\n",
      "    update_time_ms: 58.671\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.84\n",
      "    ram_util_percent: 64.23555555555555\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 169.87457332810828\n",
      "    mean_inference_ms: 14.351548725214016\n",
      "    mean_processing_ms: 25.97197033444866\n",
      "  time_since_restore: 6720.3397805690765\n",
      "  time_this_iter_s: 63.09906816482544\n",
      "  time_total_s: 6720.3397805690765\n",
      "  timestamp: 1575771960\n",
      "  timesteps_since_restore: 162000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 162000\n",
      "  training_iteration: 81\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6720 s, 81 iter, 162000 ts, 46.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.78595931073588\n",
      "  episode_reward_mean: 43.45063305659596\n",
      "  episode_reward_min: -17.52998631716785\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7693.952\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3977725505828857\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014885216020047665\n",
      "        policy_loss: -0.026955226436257362\n",
      "        total_loss: 204.91677856445312\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 204.93621826171875\n",
      "    load_time_ms: 4.437\n",
      "    num_steps_sampled: 164000\n",
      "    num_steps_trained: 164000\n",
      "    sample_time_ms: 56149.12\n",
      "    update_time_ms: 57.839\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.40645161290321\n",
      "    ram_util_percent: 64.3010752688172\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 169.90918452006733\n",
      "    mean_inference_ms: 14.344932710820201\n",
      "    mean_processing_ms: 25.987252882321286\n",
      "  time_since_restore: 6785.580473423004\n",
      "  time_this_iter_s: 65.24069285392761\n",
      "  time_total_s: 6785.580473423004\n",
      "  timestamp: 1575772026\n",
      "  timesteps_since_restore: 164000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 164000\n",
      "  training_iteration: 82\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6785 s, 82 iter, 164000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-28-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.78595931073588\n",
      "  episode_reward_mean: 41.63786649989031\n",
      "  episode_reward_min: -17.52998631716785\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7844.617\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4828007221221924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017978930845856667\n",
      "        policy_loss: -0.033433809876441956\n",
      "        total_loss: 195.67800903320312\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 195.7023468017578\n",
      "    load_time_ms: 4.336\n",
      "    num_steps_sampled: 166000\n",
      "    num_steps_trained: 166000\n",
      "    sample_time_ms: 55960.571\n",
      "    update_time_ms: 57.458\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.45222222222223\n",
      "    ram_util_percent: 64.38555555555554\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 169.0560076963796\n",
      "    mean_inference_ms: 14.281322943252562\n",
      "    mean_processing_ms: 25.87884345959432\n",
      "  time_since_restore: 6848.388823270798\n",
      "  time_this_iter_s: 62.80834984779358\n",
      "  time_total_s: 6848.388823270798\n",
      "  timestamp: 1575772089\n",
      "  timesteps_since_restore: 166000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 166000\n",
      "  training_iteration: 83\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6848 s, 83 iter, 166000 ts, 41.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-29-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.78595931073588\n",
      "  episode_reward_mean: 43.15189939061207\n",
      "  episode_reward_min: -17.52998631716785\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7878.802\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9225273132324219\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016636891290545464\n",
      "        policy_loss: -0.029277153313159943\n",
      "        total_loss: 188.54302978515625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 188.5638427734375\n",
      "    load_time_ms: 4.336\n",
      "    num_steps_sampled: 168000\n",
      "    num_steps_trained: 168000\n",
      "    sample_time_ms: 55624.598\n",
      "    update_time_ms: 55.925\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.64494382022471\n",
      "    ram_util_percent: 64.42584269662922\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 167.87155255610494\n",
      "    mean_inference_ms: 14.198304093870476\n",
      "    mean_processing_ms: 25.753272911896687\n",
      "  time_since_restore: 6911.035389184952\n",
      "  time_this_iter_s: 62.64656591415405\n",
      "  time_total_s: 6911.035389184952\n",
      "  timestamp: 1575772151\n",
      "  timesteps_since_restore: 168000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 168000\n",
      "  training_iteration: 84\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6911 s, 84 iter, 168000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-30-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.78595931073588\n",
      "  episode_reward_mean: 43.48802747955068\n",
      "  episode_reward_min: -17.52998631716785\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7742.398\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.87409245967865\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021627862006425858\n",
      "        policy_loss: -0.03677026182413101\n",
      "        total_loss: 209.7379150390625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.7637481689453\n",
      "    load_time_ms: 4.342\n",
      "    num_steps_sampled: 170000\n",
      "    num_steps_trained: 170000\n",
      "    sample_time_ms: 55700.565\n",
      "    update_time_ms: 50.062\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.0010989010989\n",
      "    ram_util_percent: 64.48791208791208\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 167.4678627689153\n",
      "    mean_inference_ms: 14.157997389323928\n",
      "    mean_processing_ms: 25.702558948735554\n",
      "  time_since_restore: 6974.319425106049\n",
      "  time_this_iter_s: 63.2840359210968\n",
      "  time_total_s: 6974.319425106049\n",
      "  timestamp: 1575772215\n",
      "  timesteps_since_restore: 170000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 170000\n",
      "  training_iteration: 85\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 6974 s, 85 iter, 170000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-31-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.78893100459013\n",
      "  episode_reward_mean: 41.40772436760371\n",
      "  episode_reward_min: -17.52998631716785\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7656.358\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5932207107543945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02854846976697445\n",
      "        policy_loss: -0.04189920797944069\n",
      "        total_loss: 209.1806182861328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 209.20806884765625\n",
      "    load_time_ms: 3.934\n",
      "    num_steps_sampled: 172000\n",
      "    num_steps_trained: 172000\n",
      "    sample_time_ms: 55861.613\n",
      "    update_time_ms: 52.114\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.00219780219781\n",
      "    ram_util_percent: 64.56593406593406\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 167.13758910603204\n",
      "    mean_inference_ms: 14.12878420052897\n",
      "    mean_processing_ms: 25.695438772126554\n",
      "  time_since_restore: 7037.952380180359\n",
      "  time_this_iter_s: 63.6329550743103\n",
      "  time_total_s: 7037.952380180359\n",
      "  timestamp: 1575772278\n",
      "  timesteps_since_restore: 172000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 172000\n",
      "  training_iteration: 86\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7037 s, 86 iter, 172000 ts, 41.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-32-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.78893100459013\n",
      "  episode_reward_mean: 43.61305929169419\n",
      "  episode_reward_min: -9.377285105823796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7645.961\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2962310314178467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02711661532521248\n",
      "        policy_loss: -0.040080003440380096\n",
      "        total_loss: 202.6308135986328\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 202.6571807861328\n",
      "    load_time_ms: 3.533\n",
      "    num_steps_sampled: 174000\n",
      "    num_steps_trained: 174000\n",
      "    sample_time_ms: 56038.215\n",
      "    update_time_ms: 51.719\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.84731182795699\n",
      "    ram_util_percent: 64.47634408602151\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 166.3830003305239\n",
      "    mean_inference_ms: 14.073395565388124\n",
      "    mean_processing_ms: 25.599379541448045\n",
      "  time_since_restore: 7103.486832380295\n",
      "  time_this_iter_s: 65.53445219993591\n",
      "  time_total_s: 7103.486832380295\n",
      "  timestamp: 1575772344\n",
      "  timesteps_since_restore: 174000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 174000\n",
      "  training_iteration: 87\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7103 s, 87 iter, 174000 ts, 43.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-33-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.74079759202482\n",
      "  episode_reward_mean: 43.152832620777836\n",
      "  episode_reward_min: -9.377285105823796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7628.755\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4776248931884766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03129849210381508\n",
      "        policy_loss: -0.050763390958309174\n",
      "        total_loss: 178.3253631591797\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 178.3602752685547\n",
      "    load_time_ms: 3.416\n",
      "    num_steps_sampled: 176000\n",
      "    num_steps_trained: 176000\n",
      "    sample_time_ms: 56184.047\n",
      "    update_time_ms: 51.67\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.39565217391304\n",
      "    ram_util_percent: 64.50434782608698\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 166.2979623745679\n",
      "    mean_inference_ms: 14.055478572935533\n",
      "    mean_processing_ms: 25.567127610469743\n",
      "  time_since_restore: 7167.958133935928\n",
      "  time_this_iter_s: 64.47130155563354\n",
      "  time_total_s: 7167.958133935928\n",
      "  timestamp: 1575772408\n",
      "  timesteps_since_restore: 176000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 176000\n",
      "  training_iteration: 88\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7167 s, 88 iter, 176000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-34-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.96684113857889\n",
      "  episode_reward_mean: 43.59448052378011\n",
      "  episode_reward_min: -9.377285105823796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7650.595\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8964247703552246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023258840665221214\n",
      "        policy_loss: -0.03734581917524338\n",
      "        total_loss: 208.11422729492188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 208.13978576660156\n",
      "    load_time_ms: 3.442\n",
      "    num_steps_sampled: 178000\n",
      "    num_steps_trained: 178000\n",
      "    sample_time_ms: 56315.139\n",
      "    update_time_ms: 51.517\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.59782608695652\n",
      "    ram_util_percent: 64.53804347826089\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 166.51239474503728\n",
      "    mean_inference_ms: 14.059534389112379\n",
      "    mean_processing_ms: 25.57244379287321\n",
      "  time_since_restore: 7232.316523313522\n",
      "  time_this_iter_s: 64.358389377594\n",
      "  time_total_s: 7232.316523313522\n",
      "  timestamp: 1575772473\n",
      "  timesteps_since_restore: 178000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 178000\n",
      "  training_iteration: 89\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7232 s, 89 iter, 178000 ts, 43.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-35-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.96684113857889\n",
      "  episode_reward_mean: 42.49449094868311\n",
      "  episode_reward_min: -9.377285105823796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7801.352\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6812314987182617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01962077058851719\n",
      "        policy_loss: -0.0361526720225811\n",
      "        total_loss: 211.18780517578125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 211.2140655517578\n",
      "    load_time_ms: 3.479\n",
      "    num_steps_sampled: 180000\n",
      "    num_steps_trained: 180000\n",
      "    sample_time_ms: 56100.015\n",
      "    update_time_ms: 50.785\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.18804347826087\n",
      "    ram_util_percent: 64.60434782608696\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 166.36323265718914\n",
      "    mean_inference_ms: 14.039686502038757\n",
      "    mean_processing_ms: 25.522528547160146\n",
      "  time_since_restore: 7297.20529961586\n",
      "  time_this_iter_s: 64.88877630233765\n",
      "  time_total_s: 7297.20529961586\n",
      "  timestamp: 1575772538\n",
      "  timesteps_since_restore: 180000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 180000\n",
      "  training_iteration: 90\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7297 s, 90 iter, 180000 ts, 42.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-36-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.1888860706286\n",
      "  episode_reward_mean: 43.5932945624266\n",
      "  episode_reward_min: -9.377285105823796\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7857.283\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5808346271514893\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028688110411167145\n",
      "        policy_loss: -0.03951107710599899\n",
      "        total_loss: 220.46026611328125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 220.48524475097656\n",
      "    load_time_ms: 3.201\n",
      "    num_steps_sampled: 182000\n",
      "    num_steps_trained: 182000\n",
      "    sample_time_ms: 56142.903\n",
      "    update_time_ms: 51.908\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.88369565217393\n",
      "    ram_util_percent: 64.70326086956523\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 165.62172650556096\n",
      "    mean_inference_ms: 13.983259031871226\n",
      "    mean_processing_ms: 25.40168141629506\n",
      "  time_since_restore: 7361.304147005081\n",
      "  time_this_iter_s: 64.09884738922119\n",
      "  time_total_s: 7361.304147005081\n",
      "  timestamp: 1575772602\n",
      "  timesteps_since_restore: 182000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 182000\n",
      "  training_iteration: 91\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7361 s, 91 iter, 182000 ts, 43.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-37-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.1888860706286\n",
      "  episode_reward_mean: 44.024881271123434\n",
      "  episode_reward_min: -3.7007706773576166\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7696.13\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.601172924041748\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03059498779475689\n",
      "        policy_loss: -0.04153210669755936\n",
      "        total_loss: 218.4055938720703\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 218.43162536621094\n",
      "    load_time_ms: 3.261\n",
      "    num_steps_sampled: 184000\n",
      "    num_steps_trained: 184000\n",
      "    sample_time_ms: 56103.692\n",
      "    update_time_ms: 53.051\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.88888888888889\n",
      "    ram_util_percent: 64.74222222222221\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 164.82386034299557\n",
      "    mean_inference_ms: 13.918076422897766\n",
      "    mean_processing_ms: 25.313473053814242\n",
      "  time_since_restore: 7424.568448543549\n",
      "  time_this_iter_s: 63.26430153846741\n",
      "  time_total_s: 7424.568448543549\n",
      "  timestamp: 1575772665\n",
      "  timesteps_since_restore: 184000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 184000\n",
      "  training_iteration: 92\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7424 s, 92 iter, 184000 ts, 44 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-38-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.1888860706286\n",
      "  episode_reward_mean: 44.581952096127\n",
      "  episode_reward_min: -20.92946134034405\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7542.958\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0832772254943848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030826842412352562\n",
      "        policy_loss: -0.048510294407606125\n",
      "        total_loss: 212.21047973632812\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 212.243408203125\n",
      "    load_time_ms: 3.277\n",
      "    num_steps_sampled: 186000\n",
      "    num_steps_trained: 186000\n",
      "    sample_time_ms: 56286.376\n",
      "    update_time_ms: 53.244\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.79333333333335\n",
      "    ram_util_percent: 64.75999999999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 164.16009671260937\n",
      "    mean_inference_ms: 13.867696849517761\n",
      "    mean_processing_ms: 25.23635139559937\n",
      "  time_since_restore: 7487.683735609055\n",
      "  time_this_iter_s: 63.11528706550598\n",
      "  time_total_s: 7487.683735609055\n",
      "  timestamp: 1575772728\n",
      "  timesteps_since_restore: 186000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 186000\n",
      "  training_iteration: 93\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7487 s, 93 iter, 186000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-39-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.1888860706286\n",
      "  episode_reward_mean: 43.8530207864614\n",
      "  episode_reward_min: -20.92946134034405\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7398.123\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.450941801071167\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01607416570186615\n",
      "        policy_loss: -0.02804638259112835\n",
      "        total_loss: 204.21109008789062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 204.23098754882812\n",
      "    load_time_ms: 3.292\n",
      "    num_steps_sampled: 188000\n",
      "    num_steps_trained: 188000\n",
      "    sample_time_ms: 56615.9\n",
      "    update_time_ms: 54.33\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.43913043478263\n",
      "    ram_util_percent: 64.66304347826087\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 163.15079967354893\n",
      "    mean_inference_ms: 13.795868712134638\n",
      "    mean_processing_ms: 25.10098159234766\n",
      "  time_since_restore: 7552.185786008835\n",
      "  time_this_iter_s: 64.50205039978027\n",
      "  time_total_s: 7552.185786008835\n",
      "  timestamp: 1575772793\n",
      "  timesteps_since_restore: 188000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 188000\n",
      "  training_iteration: 94\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7552 s, 94 iter, 188000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-40-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.1888860706286\n",
      "  episode_reward_mean: 44.34691159642213\n",
      "  episode_reward_min: -20.92946134034405\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7409.467\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.822784662246704\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01060194056481123\n",
      "        policy_loss: -0.02017040364444256\n",
      "        total_loss: 218.98184204101562\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 218.9966583251953\n",
      "    load_time_ms: 3.283\n",
      "    num_steps_sampled: 190000\n",
      "    num_steps_trained: 190000\n",
      "    sample_time_ms: 56700.797\n",
      "    update_time_ms: 54.901\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.44673913043479\n",
      "    ram_util_percent: 64.65978260869565\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 163.44288494502732\n",
      "    mean_inference_ms: 13.807698043831657\n",
      "    mean_processing_ms: 25.114944480581457\n",
      "  time_since_restore: 7616.448677062988\n",
      "  time_this_iter_s: 64.26289105415344\n",
      "  time_total_s: 7616.448677062988\n",
      "  timestamp: 1575772857\n",
      "  timesteps_since_restore: 190000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 190000\n",
      "  training_iteration: 95\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7616 s, 95 iter, 190000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-42-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.94865614714661\n",
      "  episode_reward_mean: 44.92577228075418\n",
      "  episode_reward_min: -20.92946134034405\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7411.803\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9628826379776\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01862151175737381\n",
      "        policy_loss: -0.03536504879593849\n",
      "        total_loss: 215.2901611328125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 215.3161163330078\n",
      "    load_time_ms: 3.282\n",
      "    num_steps_sampled: 192000\n",
      "    num_steps_trained: 192000\n",
      "    sample_time_ms: 56673.445\n",
      "    update_time_ms: 55.41\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.15714285714284\n",
      "    ram_util_percent: 64.7186813186813\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 163.31851191880824\n",
      "    mean_inference_ms: 13.789029283408844\n",
      "    mean_processing_ms: 25.103308081086265\n",
      "  time_since_restore: 7679.831531524658\n",
      "  time_this_iter_s: 63.38285446166992\n",
      "  time_total_s: 7679.831531524658\n",
      "  timestamp: 1575772920\n",
      "  timesteps_since_restore: 192000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 192000\n",
      "  training_iteration: 96\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7679 s, 96 iter, 192000 ts, 44.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-43-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.15949362411094\n",
      "  episode_reward_mean: 46.57177231853332\n",
      "  episode_reward_min: -20.92946134034405\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7573.243\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2979207038879395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02696734108030796\n",
      "        policy_loss: -0.03938573971390724\n",
      "        total_loss: 222.124755859375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 222.1504669189453\n",
      "    load_time_ms: 3.407\n",
      "    num_steps_sampled: 194000\n",
      "    num_steps_trained: 194000\n",
      "    sample_time_ms: 56558.211\n",
      "    update_time_ms: 56.601\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.94255319148937\n",
      "    ram_util_percent: 64.76595744680851\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 163.58344478572207\n",
      "    mean_inference_ms: 13.798903709234041\n",
      "    mean_processing_ms: 25.121896594404305\n",
      "  time_since_restore: 7745.842356443405\n",
      "  time_this_iter_s: 66.01082491874695\n",
      "  time_total_s: 7745.842356443405\n",
      "  timestamp: 1575772987\n",
      "  timesteps_since_restore: 194000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 194000\n",
      "  training_iteration: 97\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7745 s, 97 iter, 194000 ts, 46.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-44-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.15949362411094\n",
      "  episode_reward_mean: 47.32682312727059\n",
      "  episode_reward_min: -15.807312613969906\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7725.368\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.837697148323059\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01720617339015007\n",
      "        policy_loss: -0.02895326539874077\n",
      "        total_loss: 216.22280883789062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.24307250976562\n",
      "    load_time_ms: 3.43\n",
      "    num_steps_sampled: 196000\n",
      "    num_steps_trained: 196000\n",
      "    sample_time_ms: 56377.981\n",
      "    update_time_ms: 56.58\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.06813186813187\n",
      "    ram_util_percent: 64.82747252747252\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 163.47170699391384\n",
      "    mean_inference_ms: 13.782012949763079\n",
      "    mean_processing_ms: 25.109845696566655\n",
      "  time_since_restore: 7810.046886920929\n",
      "  time_this_iter_s: 64.2045304775238\n",
      "  time_total_s: 7810.046886920929\n",
      "  timestamp: 1575773051\n",
      "  timesteps_since_restore: 196000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 196000\n",
      "  training_iteration: 98\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7810 s, 98 iter, 196000 ts, 47.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-45-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.76756515145158\n",
      "  episode_reward_mean: 47.52296187640784\n",
      "  episode_reward_min: -15.807312613969906\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 1980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7722.391\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.750037431716919\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018447693437337875\n",
      "        policy_loss: -0.027000002562999725\n",
      "        total_loss: 223.203125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 223.22084045410156\n",
      "    load_time_ms: 3.376\n",
      "    num_steps_sampled: 198000\n",
      "    num_steps_trained: 198000\n",
      "    sample_time_ms: 56361.2\n",
      "    update_time_ms: 54.869\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1054347826087\n",
      "    ram_util_percent: 64.86521739130436\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 163.43211030298954\n",
      "    mean_inference_ms: 13.769097659009486\n",
      "    mean_processing_ms: 25.089732447082273\n",
      "  time_since_restore: 7874.191174983978\n",
      "  time_this_iter_s: 64.14428806304932\n",
      "  time_total_s: 7874.191174983978\n",
      "  timestamp: 1575773115\n",
      "  timesteps_since_restore: 198000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 198000\n",
      "  training_iteration: 99\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7874 s, 99 iter, 198000 ts, 47.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.76756515145158\n",
      "  episode_reward_mean: 46.48477702068753\n",
      "  episode_reward_min: -6.243496904498278\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7567.381\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.257463216781616\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011941466480493546\n",
      "        policy_loss: -0.021657569333910942\n",
      "        total_loss: 217.95880126953125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 217.97445678710938\n",
      "    load_time_ms: 3.345\n",
      "    num_steps_sampled: 200000\n",
      "    num_steps_trained: 200000\n",
      "    sample_time_ms: 56373.543\n",
      "    update_time_ms: 54.867\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.7388888888889\n",
      "    ram_util_percent: 64.9488888888889\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 162.69993255724273\n",
      "    mean_inference_ms: 13.71398922618439\n",
      "    mean_processing_ms: 24.977857295851486\n",
      "  time_since_restore: 7937.651599884033\n",
      "  time_this_iter_s: 63.46042490005493\n",
      "  time_total_s: 7937.651599884033\n",
      "  timestamp: 1575773178\n",
      "  timesteps_since_restore: 200000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 200000\n",
      "  training_iteration: 100\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 7937 s, 100 iter, 200000 ts, 46.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.76756515145158\n",
      "  episode_reward_mean: 45.15737581851346\n",
      "  episode_reward_min: -6.243496904498278\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7495.411\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1082661151885986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018792029470205307\n",
      "        policy_loss: -0.02758786454796791\n",
      "        total_loss: 204.22064208984375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 204.2387237548828\n",
      "    load_time_ms: 2.834\n",
      "    num_steps_sampled: 202000\n",
      "    num_steps_trained: 202000\n",
      "    sample_time_ms: 56486.696\n",
      "    update_time_ms: 54.072\n",
      "  iterations_since_restore: 101\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.21739130434783\n",
      "    ram_util_percent: 64.81413043478258\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.9237079546945\n",
      "    mean_inference_ms: 13.660187651508565\n",
      "    mean_processing_ms: 24.891274110064042\n",
      "  time_since_restore: 8002.1376831531525\n",
      "  time_this_iter_s: 64.48608326911926\n",
      "  time_total_s: 8002.1376831531525\n",
      "  timestamp: 1575773243\n",
      "  timesteps_since_restore: 202000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 202000\n",
      "  training_iteration: 101\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8002 s, 101 iter, 202000 ts, 45.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-48-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.76756515145158\n",
      "  episode_reward_mean: 43.080310051825855\n",
      "  episode_reward_min: -6.243496904498278\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7498.033\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0499343872070312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01808668114244938\n",
      "        policy_loss: -0.03189467638731003\n",
      "        total_loss: 228.52151489257812\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 228.54434204101562\n",
      "    load_time_ms: 3.048\n",
      "    num_steps_sampled: 204000\n",
      "    num_steps_trained: 204000\n",
      "    sample_time_ms: 56587.452\n",
      "    update_time_ms: 53.638\n",
      "  iterations_since_restore: 102\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.79239130434782\n",
      "    ram_util_percent: 64.81847826086954\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.40258659123276\n",
      "    mean_inference_ms: 13.622411715396147\n",
      "    mean_processing_ms: 24.807293127448002\n",
      "  time_since_restore: 8066.416869401932\n",
      "  time_this_iter_s: 64.2791862487793\n",
      "  time_total_s: 8066.416869401932\n",
      "  timestamp: 1575773307\n",
      "  timesteps_since_restore: 204000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 204000\n",
      "  training_iteration: 102\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8066 s, 102 iter, 204000 ts, 43.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-49-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.76756515145158\n",
      "  episode_reward_mean: 42.01200771724162\n",
      "  episode_reward_min: -5.439948364661679\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7508.642\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.43095064163208\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02363274246454239\n",
      "        policy_loss: -0.041496045887470245\n",
      "        total_loss: 210.7012939453125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.73085021972656\n",
      "    load_time_ms: 3.025\n",
      "    num_steps_sampled: 206000\n",
      "    num_steps_trained: 206000\n",
      "    sample_time_ms: 56648.108\n",
      "    update_time_ms: 54.803\n",
      "  iterations_since_restore: 103\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.11868131868131\n",
      "    ram_util_percent: 64.89780219780219\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.12500392057683\n",
      "    mean_inference_ms: 13.599468930705411\n",
      "    mean_processing_ms: 24.761590835152248\n",
      "  time_since_restore: 8130.243132591248\n",
      "  time_this_iter_s: 63.826263189315796\n",
      "  time_total_s: 8130.243132591248\n",
      "  timestamp: 1575773371\n",
      "  timesteps_since_restore: 206000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 206000\n",
      "  training_iteration: 103\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8130 s, 103 iter, 206000 ts, 42 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-50-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.37665973816786\n",
      "  episode_reward_mean: 41.72179207351587\n",
      "  episode_reward_min: -5.439948364661679\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7680.754\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.29343318939209\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03295554965734482\n",
      "        policy_loss: -0.043039895594120026\n",
      "        total_loss: 208.1837921142578\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 208.21014404296875\n",
      "    load_time_ms: 3.036\n",
      "    num_steps_sampled: 208000\n",
      "    num_steps_trained: 208000\n",
      "    sample_time_ms: 56518.741\n",
      "    update_time_ms: 56.967\n",
      "  iterations_since_restore: 104\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.81075268817204\n",
      "    ram_util_percent: 64.97096774193548\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 160.8320604357866\n",
      "    mean_inference_ms: 13.5748846831229\n",
      "    mean_processing_ms: 24.71303650883253\n",
      "  time_since_restore: 8195.201708078384\n",
      "  time_this_iter_s: 64.95857548713684\n",
      "  time_total_s: 8195.201708078384\n",
      "  timestamp: 1575773436\n",
      "  timesteps_since_restore: 208000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 208000\n",
      "  training_iteration: 104\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8195 s, 104 iter, 208000 ts, 41.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-51-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.37665973816786\n",
      "  episode_reward_mean: 42.534684259113156\n",
      "  episode_reward_min: -19.824723314316728\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7825.836\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7223780155181885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.034412652254104614\n",
      "        policy_loss: -0.04350981116294861\n",
      "        total_loss: 237.44618225097656\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 237.47230529785156\n",
      "    load_time_ms: 3.123\n",
      "    num_steps_sampled: 210000\n",
      "    num_steps_trained: 210000\n",
      "    sample_time_ms: 56372.187\n",
      "    update_time_ms: 55.431\n",
      "  iterations_since_restore: 105\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.9054347826087\n",
      "    ram_util_percent: 65.03913043478259\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.03271966725487\n",
      "    mean_inference_ms: 13.579565781002316\n",
      "    mean_processing_ms: 24.70651814152217\n",
      "  time_since_restore: 8259.439326286316\n",
      "  time_this_iter_s: 64.23761820793152\n",
      "  time_total_s: 8259.439326286316\n",
      "  timestamp: 1575773500\n",
      "  timesteps_since_restore: 210000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 210000\n",
      "  training_iteration: 105\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8259 s, 105 iter, 210000 ts, 42.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-52-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.02093267739691\n",
      "  episode_reward_mean: 42.79406679996516\n",
      "  episode_reward_min: -19.824723314316728\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7834.607\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7537155151367188\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028603006154298782\n",
      "        policy_loss: -0.04211920127272606\n",
      "        total_loss: 220.5272216796875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 220.55490112304688\n",
      "    load_time_ms: 3.102\n",
      "    num_steps_sampled: 212000\n",
      "    num_steps_trained: 212000\n",
      "    sample_time_ms: 56407.86\n",
      "    update_time_ms: 55.677\n",
      "  iterations_since_restore: 106\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.310989010989\n",
      "    ram_util_percent: 65.11758241758241\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.33057923858422\n",
      "    mean_inference_ms: 13.592942586063895\n",
      "    mean_processing_ms: 24.717431711039776\n",
      "  time_since_restore: 8323.267956972122\n",
      "  time_this_iter_s: 63.828630685806274\n",
      "  time_total_s: 8323.267956972122\n",
      "  timestamp: 1575773564\n",
      "  timesteps_since_restore: 212000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 212000\n",
      "  training_iteration: 106\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8323 s, 106 iter, 212000 ts, 42.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-53-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.02093267739691\n",
      "  episode_reward_mean: 44.14801512951405\n",
      "  episode_reward_min: -19.824723314316728\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7689.144\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8876961469650269\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01657027006149292\n",
      "        policy_loss: -0.025633912533521652\n",
      "        total_loss: 212.14772033691406\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 212.16493225097656\n",
      "    load_time_ms: 3.003\n",
      "    num_steps_sampled: 214000\n",
      "    num_steps_trained: 214000\n",
      "    sample_time_ms: 56383.198\n",
      "    update_time_ms: 55.306\n",
      "  iterations_since_restore: 107\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1021739130435\n",
      "    ram_util_percent: 65.15978260869565\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.12839492746136\n",
      "    mean_inference_ms: 13.567930021299738\n",
      "    mean_processing_ms: 24.663990085197675\n",
      "  time_since_restore: 8387.60113954544\n",
      "  time_this_iter_s: 64.33318257331848\n",
      "  time_total_s: 8387.60113954544\n",
      "  timestamp: 1575773629\n",
      "  timesteps_since_restore: 214000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 214000\n",
      "  training_iteration: 107\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8387 s, 107 iter, 214000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-54-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.02093267739691\n",
      "  episode_reward_mean: 44.26763196379096\n",
      "  episode_reward_min: -19.824723314316728\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7531.556\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.69450306892395\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019450118765234947\n",
      "        policy_loss: -0.029012180864810944\n",
      "        total_loss: 219.09780883789062\n",
      "        vf_explained_var: 2.384185791015625e-07\n",
      "        vf_loss: 219.11697387695312\n",
      "    load_time_ms: 3.103\n",
      "    num_steps_sampled: 216000\n",
      "    num_steps_trained: 216000\n",
      "    sample_time_ms: 56609.882\n",
      "    update_time_ms: 55.922\n",
      "  iterations_since_restore: 108\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.66630434782608\n",
      "    ram_util_percent: 65.06521739130437\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 160.95225907975458\n",
      "    mean_inference_ms: 13.551079533738006\n",
      "    mean_processing_ms: 24.61045634555524\n",
      "  time_since_restore: 8452.500408411026\n",
      "  time_this_iter_s: 64.89926886558533\n",
      "  time_total_s: 8452.500408411026\n",
      "  timestamp: 1575773694\n",
      "  timesteps_since_restore: 216000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 216000\n",
      "  training_iteration: 108\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8452 s, 108 iter, 216000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-55-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.02093267739691\n",
      "  episode_reward_mean: 40.44168623732594\n",
      "  episode_reward_min: -19.824723314316728\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7534.418\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1455867290496826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013764956966042519\n",
      "        policy_loss: -0.020574139431118965\n",
      "        total_loss: 225.0642852783203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 225.077880859375\n",
      "    load_time_ms: 3.139\n",
      "    num_steps_sampled: 218000\n",
      "    num_steps_trained: 218000\n",
      "    sample_time_ms: 56595.388\n",
      "    update_time_ms: 56.641\n",
      "  iterations_since_restore: 109\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.77065217391304\n",
      "    ram_util_percent: 65.0271739130435\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 161.0150660291258\n",
      "    mean_inference_ms: 13.552872244690159\n",
      "    mean_processing_ms: 24.610384499721967\n",
      "  time_since_restore: 8516.536689519882\n",
      "  time_this_iter_s: 64.0362811088562\n",
      "  time_total_s: 8516.536689519882\n",
      "  timestamp: 1575773758\n",
      "  timesteps_since_restore: 218000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 218000\n",
      "  training_iteration: 109\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8516 s, 109 iter, 218000 ts, 40.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-57-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.02093267739691\n",
      "  episode_reward_mean: 39.69776061546866\n",
      "  episode_reward_min: -10.959326969840495\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7561.894\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.961826801300049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02669369988143444\n",
      "        policy_loss: -0.03254291042685509\n",
      "        total_loss: 207.4326934814453\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 207.45176696777344\n",
      "    load_time_ms: 3.13\n",
      "    num_steps_sampled: 220000\n",
      "    num_steps_trained: 220000\n",
      "    sample_time_ms: 56610.969\n",
      "    update_time_ms: 57.147\n",
      "  iterations_since_restore: 110\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.51318681318682\n",
      "    ram_util_percent: 65.09890109890111\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 159.84397461943396\n",
      "    mean_inference_ms: 13.47451960735802\n",
      "    mean_processing_ms: 24.530547783551206\n",
      "  time_since_restore: 8580.436664581299\n",
      "  time_this_iter_s: 63.899975061416626\n",
      "  time_total_s: 8580.436664581299\n",
      "  timestamp: 1575773822\n",
      "  timesteps_since_restore: 220000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 220000\n",
      "  training_iteration: 110\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8580 s, 110 iter, 220000 ts, 39.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-58-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.09108698932548\n",
      "  episode_reward_mean: 40.8233393284556\n",
      "  episode_reward_min: -10.959326969840495\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7735.402\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0639894008636475\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024838237091898918\n",
      "        policy_loss: -0.038866158574819565\n",
      "        total_loss: 232.2608184814453\n",
      "        vf_explained_var: 2.384185791015625e-07\n",
      "        vf_loss: 232.287109375\n",
      "    load_time_ms: 3.226\n",
      "    num_steps_sampled: 222000\n",
      "    num_steps_trained: 222000\n",
      "    sample_time_ms: 56496.4\n",
      "    update_time_ms: 56.361\n",
      "  iterations_since_restore: 111\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.51182795698924\n",
      "    ram_util_percent: 65.16559139784948\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 159.49771124013333\n",
      "    mean_inference_ms: 13.44500765775011\n",
      "    mean_processing_ms: 24.463330107109396\n",
      "  time_since_restore: 8645.509871721268\n",
      "  time_this_iter_s: 65.07320713996887\n",
      "  time_total_s: 8645.509871721268\n",
      "  timestamp: 1575773887\n",
      "  timesteps_since_restore: 222000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 222000\n",
      "  training_iteration: 111\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8645 s, 111 iter, 222000 ts, 40.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_21-59-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.64453263440231\n",
      "  episode_reward_mean: 40.49365919145893\n",
      "  episode_reward_min: -10.959326969840495\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7901.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2414193153381348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03160283342003822\n",
      "        policy_loss: -0.0440458208322525\n",
      "        total_loss: 216.56419372558594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.59222412109375\n",
      "    load_time_ms: 2.968\n",
      "    num_steps_sampled: 224000\n",
      "    num_steps_trained: 224000\n",
      "    sample_time_ms: 56328.846\n",
      "    update_time_ms: 56.663\n",
      "  iterations_since_restore: 112\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.87142857142857\n",
      "    ram_util_percent: 65.23736263736266\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 158.82626624802825\n",
      "    mean_inference_ms: 13.398706551045612\n",
      "    mean_processing_ms: 24.41095488172352\n",
      "  time_since_restore: 8709.791128396988\n",
      "  time_this_iter_s: 64.28125667572021\n",
      "  time_total_s: 8709.791128396988\n",
      "  timestamp: 1575773951\n",
      "  timesteps_since_restore: 224000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 224000\n",
      "  training_iteration: 112\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8709 s, 112 iter, 224000 ts, 40.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-00-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.64453263440231\n",
      "  episode_reward_mean: 39.63477141990591\n",
      "  episode_reward_min: -12.539439251923719\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7904.682\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.62758731842041\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03293181583285332\n",
      "        policy_loss: -0.044067464768886566\n",
      "        total_loss: 227.66212463378906\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 227.68948364257812\n",
      "    load_time_ms: 3.087\n",
      "    num_steps_sampled: 226000\n",
      "    num_steps_trained: 226000\n",
      "    sample_time_ms: 56375.479\n",
      "    update_time_ms: 56.346\n",
      "  iterations_since_restore: 113\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.81847826086955\n",
      "    ram_util_percent: 65.275\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 159.32077774648175\n",
      "    mean_inference_ms: 13.424446676346934\n",
      "    mean_processing_ms: 24.444642771170138\n",
      "  time_since_restore: 8774.121356248856\n",
      "  time_this_iter_s: 64.33022785186768\n",
      "  time_total_s: 8774.121356248856\n",
      "  timestamp: 1575774015\n",
      "  timesteps_since_restore: 226000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 226000\n",
      "  training_iteration: 113\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8774 s, 113 iter, 226000 ts, 39.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-01-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.64453263440231\n",
      "  episode_reward_mean: 44.12991571402632\n",
      "  episode_reward_min: -12.539439251923719\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7736.639\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2013795375823975\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012345787137746811\n",
      "        policy_loss: -0.020566100254654884\n",
      "        total_loss: 225.2460479736328\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 225.26046752929688\n",
      "    load_time_ms: 3.065\n",
      "    num_steps_sampled: 228000\n",
      "    num_steps_trained: 228000\n",
      "    sample_time_ms: 56483.832\n",
      "    update_time_ms: 53.007\n",
      "  iterations_since_restore: 114\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.4054347826087\n",
      "    ram_util_percent: 65.3586956521739\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 158.55398025175418\n",
      "    mean_inference_ms: 13.367396005028361\n",
      "    mean_processing_ms: 24.36509740190838\n",
      "  time_since_restore: 8838.453483819962\n",
      "  time_this_iter_s: 64.33212757110596\n",
      "  time_total_s: 8838.453483819962\n",
      "  timestamp: 1575774080\n",
      "  timesteps_since_restore: 228000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 228000\n",
      "  training_iteration: 114\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8838 s, 114 iter, 228000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-02-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.64453263440231\n",
      "  episode_reward_mean: 46.33559143305567\n",
      "  episode_reward_min: -16.43150989763376\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7573.045\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0580315589904785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017127173021435738\n",
      "        policy_loss: -0.02985471673309803\n",
      "        total_loss: 211.05406188964844\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 211.07528686523438\n",
      "    load_time_ms: 2.995\n",
      "    num_steps_sampled: 230000\n",
      "    num_steps_trained: 230000\n",
      "    sample_time_ms: 56761.987\n",
      "    update_time_ms: 53.3\n",
      "  iterations_since_restore: 115\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.43978494623657\n",
      "    ram_util_percent: 65.2483870967742\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 158.5860775524183\n",
      "    mean_inference_ms: 13.364642197372305\n",
      "    mean_processing_ms: 24.333683531880204\n",
      "  time_since_restore: 8903.838232040405\n",
      "  time_this_iter_s: 65.38474822044373\n",
      "  time_total_s: 8903.838232040405\n",
      "  timestamp: 1575774145\n",
      "  timesteps_since_restore: 230000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 230000\n",
      "  training_iteration: 115\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8903 s, 115 iter, 230000 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-03-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.64453263440231\n",
      "  episode_reward_mean: 47.593803952446216\n",
      "  episode_reward_min: -16.43150989763376\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7565.059\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0713748931884766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016415370628237724\n",
      "        policy_loss: -0.02414904348552227\n",
      "        total_loss: 218.18075561523438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.19659423828125\n",
      "    load_time_ms: 2.995\n",
      "    num_steps_sampled: 232000\n",
      "    num_steps_trained: 232000\n",
      "    sample_time_ms: 56882.044\n",
      "    update_time_ms: 51.974\n",
      "  iterations_since_restore: 116\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.71505376344086\n",
      "    ram_util_percent: 65.2279569892473\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 157.90092562578272\n",
      "    mean_inference_ms: 13.316603048483694\n",
      "    mean_processing_ms: 24.279957184709332\n",
      "  time_since_restore: 8968.759225130081\n",
      "  time_this_iter_s: 64.9209930896759\n",
      "  time_total_s: 8968.759225130081\n",
      "  timestamp: 1575774210\n",
      "  timesteps_since_restore: 232000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 232000\n",
      "  training_iteration: 116\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 8968 s, 116 iter, 232000 ts, 47.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-04-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.79350728957924\n",
      "  episode_reward_mean: 48.25009918565743\n",
      "  episode_reward_min: -16.43150989763376\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7695.663\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7247447967529297\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021689100190997124\n",
      "        policy_loss: -0.03073509782552719\n",
      "        total_loss: 216.83230590820312\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 216.85211181640625\n",
      "    load_time_ms: 2.924\n",
      "    num_steps_sampled: 234000\n",
      "    num_steps_trained: 234000\n",
      "    sample_time_ms: 56822.462\n",
      "    update_time_ms: 52.867\n",
      "  iterations_since_restore: 117\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.78817204301075\n",
      "    ram_util_percent: 65.30537634408601\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 157.4886707937\n",
      "    mean_inference_ms: 13.286021695657391\n",
      "    mean_processing_ms: 24.2349037154088\n",
      "  time_since_restore: 9033.873564243317\n",
      "  time_this_iter_s: 65.11433911323547\n",
      "  time_total_s: 9033.873564243317\n",
      "  timestamp: 1575774275\n",
      "  timesteps_since_restore: 234000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 234000\n",
      "  training_iteration: 117\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9033 s, 117 iter, 234000 ts, 48.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-05-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.75397109749383\n",
      "  episode_reward_mean: 48.48373715782313\n",
      "  episode_reward_min: -16.43150989763376\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7869.957\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3363921642303467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03424353897571564\n",
      "        policy_loss: -0.04108518362045288\n",
      "        total_loss: 212.4045867919922\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 212.42828369140625\n",
      "    load_time_ms: 2.819\n",
      "    num_steps_sampled: 236000\n",
      "    num_steps_trained: 236000\n",
      "    sample_time_ms: 56561.651\n",
      "    update_time_ms: 54.368\n",
      "  iterations_since_restore: 118\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1021978021978\n",
      "    ram_util_percent: 65.37692307692306\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.52288111815034\n",
      "    mean_inference_ms: 13.21922220064885\n",
      "    mean_processing_ms: 24.13605112347291\n",
      "  time_since_restore: 9097.919752597809\n",
      "  time_this_iter_s: 64.04618835449219\n",
      "  time_total_s: 9097.919752597809\n",
      "  timestamp: 1575774339\n",
      "  timesteps_since_restore: 236000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 236000\n",
      "  training_iteration: 118\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9097 s, 118 iter, 236000 ts, 48.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-06-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.75397109749383\n",
      "  episode_reward_mean: 46.19796322640528\n",
      "  episode_reward_min: -16.43150989763376\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7883.32\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0192558765411377\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0225809495896101\n",
      "        policy_loss: -0.030759481713175774\n",
      "        total_loss: 224.71839904785156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 224.7377471923828\n",
      "    load_time_ms: 2.798\n",
      "    num_steps_sampled: 238000\n",
      "    num_steps_trained: 238000\n",
      "    sample_time_ms: 56476.897\n",
      "    update_time_ms: 54.874\n",
      "  iterations_since_restore: 119\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.0153846153846\n",
      "    ram_util_percent: 65.44285714285712\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.7397979229294\n",
      "    mean_inference_ms: 13.227597100347431\n",
      "    mean_processing_ms: 24.132316599913075\n",
      "  time_since_restore: 9161.234248876572\n",
      "  time_this_iter_s: 63.31449627876282\n",
      "  time_total_s: 9161.234248876572\n",
      "  timestamp: 1575774403\n",
      "  timesteps_since_restore: 238000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 238000\n",
      "  training_iteration: 119\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9161 s, 119 iter, 238000 ts, 46.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-07-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.75397109749383\n",
      "  episode_reward_mean: 46.07645747936807\n",
      "  episode_reward_min: -15.764363280714035\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7860.985\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5062500238418579\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6718857288360596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007186293601989746\n",
      "        policy_loss: -0.011610765010118484\n",
      "        total_loss: 210.1413116455078\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 210.1492462158203\n",
      "    load_time_ms: 2.875\n",
      "    num_steps_sampled: 240000\n",
      "    num_steps_trained: 240000\n",
      "    sample_time_ms: 56494.829\n",
      "    update_time_ms: 53.881\n",
      "  iterations_since_restore: 120\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16923076923077\n",
      "    ram_util_percent: 65.50329670329671\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.83848295598673\n",
      "    mean_inference_ms: 13.23251089509227\n",
      "    mean_processing_ms: 24.14025156287476\n",
      "  time_since_restore: 9225.07988166809\n",
      "  time_this_iter_s: 63.845632791519165\n",
      "  time_total_s: 9225.07988166809\n",
      "  timestamp: 1575774467\n",
      "  timesteps_since_restore: 240000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 240000\n",
      "  training_iteration: 120\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9225 s, 120 iter, 240000 ts, 46.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-08-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.75397109749383\n",
      "  episode_reward_mean: 45.88793329058416\n",
      "  episode_reward_min: -7.868732569284615\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7689.569\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.570204019546509\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.032883454114198685\n",
      "        policy_loss: -0.033205874264240265\n",
      "        total_loss: 209.2016143798828\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.2265625\n",
      "    load_time_ms: 2.886\n",
      "    num_steps_sampled: 242000\n",
      "    num_steps_trained: 242000\n",
      "    sample_time_ms: 56523.353\n",
      "    update_time_ms: 55.361\n",
      "  iterations_since_restore: 121\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.09230769230768\n",
      "    ram_util_percent: 65.55054945054944\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.6079140999081\n",
      "    mean_inference_ms: 13.214680065700195\n",
      "    mean_processing_ms: 24.098986718012466\n",
      "  time_since_restore: 9288.748354196548\n",
      "  time_this_iter_s: 63.66847252845764\n",
      "  time_total_s: 9288.748354196548\n",
      "  timestamp: 1575774530\n",
      "  timesteps_since_restore: 242000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 242000\n",
      "  training_iteration: 121\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9288 s, 121 iter, 242000 ts, 45.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-09-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.75397109749383\n",
      "  episode_reward_mean: 41.881025812756796\n",
      "  episode_reward_min: -7.868732569284615\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7522.585\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.412881374359131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029627854004502296\n",
      "        policy_loss: -0.03350993990898132\n",
      "        total_loss: 231.95059204101562\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 231.97666931152344\n",
      "    load_time_ms: 2.876\n",
      "    num_steps_sampled: 244000\n",
      "    num_steps_trained: 244000\n",
      "    sample_time_ms: 56698.416\n",
      "    update_time_ms: 54.852\n",
      "  iterations_since_restore: 122\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.60217391304347\n",
      "    ram_util_percent: 65.4608695652174\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.59494287014107\n",
      "    mean_inference_ms: 13.208627214807063\n",
      "    mean_processing_ms: 24.088485677245362\n",
      "  time_since_restore: 9353.092342376709\n",
      "  time_this_iter_s: 64.34398818016052\n",
      "  time_total_s: 9353.092342376709\n",
      "  timestamp: 1575774595\n",
      "  timesteps_since_restore: 244000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 244000\n",
      "  training_iteration: 122\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9353 s, 122 iter, 244000 ts, 41.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-10-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.73665572698899\n",
      "  episode_reward_mean: 43.310331408949786\n",
      "  episode_reward_min: -7.868732569284615\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7507.686\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5006773471832275\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023210082203149796\n",
      "        policy_loss: -0.02889885939657688\n",
      "        total_loss: 228.5495147705078\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 228.5724639892578\n",
      "    load_time_ms: 2.909\n",
      "    num_steps_sampled: 246000\n",
      "    num_steps_trained: 246000\n",
      "    sample_time_ms: 56611.444\n",
      "    update_time_ms: 53.944\n",
      "  iterations_since_restore: 123\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.47111111111111\n",
      "    ram_util_percent: 65.45333333333335\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 157.01588699714645\n",
      "    mean_inference_ms: 13.22580638348997\n",
      "    mean_processing_ms: 24.099199032431198\n",
      "  time_since_restore: 9416.38477230072\n",
      "  time_this_iter_s: 63.29242992401123\n",
      "  time_total_s: 9416.38477230072\n",
      "  timestamp: 1575774658\n",
      "  timesteps_since_restore: 246000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 246000\n",
      "  training_iteration: 123\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9416 s, 123 iter, 246000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-12-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.60410928372112\n",
      "  episode_reward_mean: 44.17157688070163\n",
      "  episode_reward_min: -7.1700777863521274\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7512.789\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.767190933227539\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020716672763228416\n",
      "        policy_loss: -0.026013007387518883\n",
      "        total_loss: 218.1785888671875\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 218.1993408203125\n",
      "    load_time_ms: 2.922\n",
      "    num_steps_sampled: 248000\n",
      "    num_steps_trained: 248000\n",
      "    sample_time_ms: 56484.306\n",
      "    update_time_ms: 56.191\n",
      "  iterations_since_restore: 124\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.62777777777778\n",
      "    ram_util_percent: 65.51666666666668\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.59751806907744\n",
      "    mean_inference_ms: 13.195796013118015\n",
      "    mean_processing_ms: 24.050843950840527\n",
      "  time_since_restore: 9479.518330335617\n",
      "  time_this_iter_s: 63.13355803489685\n",
      "  time_total_s: 9479.518330335617\n",
      "  timestamp: 1575774721\n",
      "  timesteps_since_restore: 248000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 248000\n",
      "  training_iteration: 124\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9479 s, 124 iter, 248000 ts, 44.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-13-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.60410928372112\n",
      "  episode_reward_mean: 43.785523701834336\n",
      "  episode_reward_min: -14.602583613070067\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7683.256\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6274800300598145\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019329018890857697\n",
      "        policy_loss: -0.029094401746988297\n",
      "        total_loss: 221.47340393066406\n",
      "        vf_explained_var: 2.980232238769531e-07\n",
      "        vf_loss: 221.49761962890625\n",
      "    load_time_ms: 3.885\n",
      "    num_steps_sampled: 250000\n",
      "    num_steps_trained: 250000\n",
      "    sample_time_ms: 56492.352\n",
      "    update_time_ms: 56.237\n",
      "  iterations_since_restore: 125\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.325\n",
      "    ram_util_percent: 65.571875\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.26709630141357\n",
      "    mean_inference_ms: 13.163956650958061\n",
      "    mean_processing_ms: 24.023618120828015\n",
      "  time_since_restore: 9546.697842359543\n",
      "  time_this_iter_s: 67.17951202392578\n",
      "  time_total_s: 9546.697842359543\n",
      "  timestamp: 1575774789\n",
      "  timesteps_since_restore: 250000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 250000\n",
      "  training_iteration: 125\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9546 s, 125 iter, 250000 ts, 43.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.8461155946651\n",
      "  episode_reward_mean: 43.179083475526575\n",
      "  episode_reward_min: -14.602583613070067\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7726.75\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5784194469451904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018234046176075935\n",
      "        policy_loss: -0.018124736845493317\n",
      "        total_loss: 225.99163818359375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 226.00526428222656\n",
      "    load_time_ms: 4.754\n",
      "    num_steps_sampled: 252000\n",
      "    num_steps_trained: 252000\n",
      "    sample_time_ms: 56295.527\n",
      "    update_time_ms: 56.75\n",
      "  iterations_since_restore: 126\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.33777777777777\n",
      "    ram_util_percent: 65.61777777777777\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 157.06853237662924\n",
      "    mean_inference_ms: 13.208271740050861\n",
      "    mean_processing_ms: 24.075048114138912\n",
      "  time_since_restore: 9610.100052118301\n",
      "  time_this_iter_s: 63.402209758758545\n",
      "  time_total_s: 9610.100052118301\n",
      "  timestamp: 1575774852\n",
      "  timesteps_since_restore: 252000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 252000\n",
      "  training_iteration: 126\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9610 s, 126 iter, 252000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-15-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.8461155946651\n",
      "  episode_reward_mean: 44.055237923414\n",
      "  episode_reward_min: -15.821216667921757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7610.737\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5992913246154785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022801494225859642\n",
      "        policy_loss: -0.0227036289870739\n",
      "        total_loss: 228.4619903564453\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 228.47900390625\n",
      "    load_time_ms: 4.93\n",
      "    num_steps_sampled: 254000\n",
      "    num_steps_trained: 254000\n",
      "    sample_time_ms: 56359.264\n",
      "    update_time_ms: 54.862\n",
      "  iterations_since_restore: 127\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.13695652173914\n",
      "    ram_util_percent: 65.70217391304347\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 157.0933793296001\n",
      "    mean_inference_ms: 13.204991530416612\n",
      "    mean_processing_ms: 24.047807900116624\n",
      "  time_since_restore: 9674.586966753006\n",
      "  time_this_iter_s: 64.48691463470459\n",
      "  time_total_s: 9674.586966753006\n",
      "  timestamp: 1575774917\n",
      "  timesteps_since_restore: 254000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 254000\n",
      "  training_iteration: 127\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9674 s, 127 iter, 254000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-16-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.8461155946651\n",
      "  episode_reward_mean: 43.494931249981846\n",
      "  episode_reward_min: -15.821216667921757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7439.53\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.371785879135132\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015507477335631847\n",
      "        policy_loss: -0.024060705676674843\n",
      "        total_loss: 231.87774658203125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 231.8978729248047\n",
      "    load_time_ms: 5.041\n",
      "    num_steps_sampled: 256000\n",
      "    num_steps_trained: 256000\n",
      "    sample_time_ms: 56543.685\n",
      "    update_time_ms: 52.955\n",
      "  iterations_since_restore: 128\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.21521739130434\n",
      "    ram_util_percent: 65.75978260869563\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.62937480130057\n",
      "    mean_inference_ms: 13.17221933268549\n",
      "    mean_processing_ms: 24.00121868340715\n",
      "  time_since_restore: 9738.750400543213\n",
      "  time_this_iter_s: 64.16343379020691\n",
      "  time_total_s: 9738.750400543213\n",
      "  timestamp: 1575774981\n",
      "  timesteps_since_restore: 256000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 256000\n",
      "  training_iteration: 128\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9738 s, 128 iter, 256000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-17-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.8407135707795\n",
      "  episode_reward_mean: 43.06482942130714\n",
      "  episode_reward_min: -15.821216667921757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7404.123\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.184701919555664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017906049266457558\n",
      "        policy_loss: -0.018416207283735275\n",
      "        total_loss: 218.02162170410156\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 218.03553771972656\n",
      "    load_time_ms: 5.03\n",
      "    num_steps_sampled: 258000\n",
      "    num_steps_trained: 258000\n",
      "    sample_time_ms: 56621.092\n",
      "    update_time_ms: 53.07\n",
      "  iterations_since_restore: 129\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.23516483516485\n",
      "    ram_util_percent: 65.66153846153846\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 156.239969946713\n",
      "    mean_inference_ms: 13.141189348031395\n",
      "    mean_processing_ms: 23.97423567239527\n",
      "  time_since_restore: 9802.484976291656\n",
      "  time_this_iter_s: 63.7345757484436\n",
      "  time_total_s: 9802.484976291656\n",
      "  timestamp: 1575775044\n",
      "  timesteps_since_restore: 258000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 258000\n",
      "  training_iteration: 129\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9802 s, 129 iter, 258000 ts, 43.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.8407135707795\n",
      "  episode_reward_mean: 44.55099704687299\n",
      "  episode_reward_min: -15.821216667921757\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7405.692\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0496885776519775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02250150963664055\n",
      "        policy_loss: -0.027053123340010643\n",
      "        total_loss: 215.27157592773438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 215.29295349121094\n",
      "    load_time_ms: 5.034\n",
      "    num_steps_sampled: 260000\n",
      "    num_steps_trained: 260000\n",
      "    sample_time_ms: 56606.539\n",
      "    update_time_ms: 54.622\n",
      "  iterations_since_restore: 130\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.84065934065934\n",
      "    ram_util_percent: 65.67032967032968\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 155.62915195317885\n",
      "    mean_inference_ms: 13.099381423778846\n",
      "    mean_processing_ms: 23.913707813038727\n",
      "  time_since_restore: 9866.216607093811\n",
      "  time_this_iter_s: 63.73163080215454\n",
      "  time_total_s: 9866.216607093811\n",
      "  timestamp: 1575775108\n",
      "  timesteps_since_restore: 260000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 260000\n",
      "  training_iteration: 130\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9866 s, 130 iter, 260000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-19-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.8407135707795\n",
      "  episode_reward_mean: 41.1105441917837\n",
      "  episode_reward_min: -18.43281599044627\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7409.73\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5378715991973877\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022589081898331642\n",
      "        policy_loss: -0.031290698796510696\n",
      "        total_loss: 254.95736694335938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 254.98300170898438\n",
      "    load_time_ms: 4.99\n",
      "    num_steps_sampled: 262000\n",
      "    num_steps_trained: 262000\n",
      "    sample_time_ms: 56606.535\n",
      "    update_time_ms: 53.222\n",
      "  iterations_since_restore: 131\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.74285714285713\n",
      "    ram_util_percent: 65.72307692307692\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 154.47563659472098\n",
      "    mean_inference_ms: 13.021378624914412\n",
      "    mean_processing_ms: 23.8207984181336\n",
      "  time_since_restore: 9929.899218082428\n",
      "  time_this_iter_s: 63.68261098861694\n",
      "  time_total_s: 9929.899218082428\n",
      "  timestamp: 1575775172\n",
      "  timesteps_since_restore: 262000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 262000\n",
      "  training_iteration: 131\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9929 s, 131 iter, 262000 ts, 41.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-20-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.8407135707795\n",
      "  episode_reward_mean: 41.988076445351524\n",
      "  episode_reward_min: -18.43281599044627\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7587.244\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5637378692626953\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013919677585363388\n",
      "        policy_loss: -0.01774359494447708\n",
      "        total_loss: 205.40512084960938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 205.4192352294922\n",
      "    load_time_ms: 4.989\n",
      "    num_steps_sampled: 264000\n",
      "    num_steps_trained: 264000\n",
      "    sample_time_ms: 56508.914\n",
      "    update_time_ms: 53.585\n",
      "  iterations_since_restore: 132\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82795698924731\n",
      "    ram_util_percent: 65.77311827956989\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 154.3187546395643\n",
      "    mean_inference_ms: 13.005746600975892\n",
      "    mean_processing_ms: 23.781518038683686\n",
      "  time_since_restore: 9995.06192445755\n",
      "  time_this_iter_s: 65.16270637512207\n",
      "  time_total_s: 9995.06192445755\n",
      "  timestamp: 1575775237\n",
      "  timesteps_since_restore: 264000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 264000\n",
      "  training_iteration: 132\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 9995 s, 132 iter, 264000 ts, 42 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-21-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.8407135707795\n",
      "  episode_reward_mean: 40.725441271403525\n",
      "  episode_reward_min: -18.43281599044627\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7767.888\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3952441215515137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01749568060040474\n",
      "        policy_loss: -0.02072403021156788\n",
      "        total_loss: 204.43170166015625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 204.44802856445312\n",
      "    load_time_ms: 4.849\n",
      "    num_steps_sampled: 266000\n",
      "    num_steps_trained: 266000\n",
      "    sample_time_ms: 56451.963\n",
      "    update_time_ms: 52.631\n",
      "  iterations_since_restore: 133\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16195652173914\n",
      "    ram_util_percent: 65.86847826086955\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.96720827639817\n",
      "    mean_inference_ms: 12.980377029443355\n",
      "    mean_processing_ms: 23.761740899217433\n",
      "  time_since_restore: 10059.582554340363\n",
      "  time_this_iter_s: 64.5206298828125\n",
      "  time_total_s: 10059.582554340363\n",
      "  timestamp: 1575775302\n",
      "  timesteps_since_restore: 266000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 266000\n",
      "  training_iteration: 133\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10059 s, 133 iter, 266000 ts, 40.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-22-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.17604925956024\n",
      "  episode_reward_mean: 41.759859748794725\n",
      "  episode_reward_min: -18.43281599044627\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7765.712\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1737453937530518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03999145328998566\n",
      "        policy_loss: -0.04625360667705536\n",
      "        total_loss: 211.86557006835938\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 211.9016876220703\n",
      "    load_time_ms: 4.79\n",
      "    num_steps_sampled: 268000\n",
      "    num_steps_trained: 268000\n",
      "    sample_time_ms: 56555.082\n",
      "    update_time_ms: 51.346\n",
      "  iterations_since_restore: 134\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16703296703298\n",
      "    ram_util_percent: 65.92747252747255\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 154.204501011294\n",
      "    mean_inference_ms: 12.990085125333671\n",
      "    mean_processing_ms: 23.76942424733176\n",
      "  time_since_restore: 10123.717068433762\n",
      "  time_this_iter_s: 64.13451409339905\n",
      "  time_total_s: 10123.717068433762\n",
      "  timestamp: 1575775366\n",
      "  timesteps_since_restore: 268000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 268000\n",
      "  training_iteration: 134\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10123 s, 134 iter, 268000 ts, 41.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-23-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.17604925956024\n",
      "  episode_reward_mean: 40.30100111407259\n",
      "  episode_reward_min: -18.43281599044627\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7595.288\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.866825580596924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01816132478415966\n",
      "        policy_loss: -0.022708848118782043\n",
      "        total_loss: 206.1649932861328\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 206.18309020996094\n",
      "    load_time_ms: 3.861\n",
      "    num_steps_sampled: 270000\n",
      "    num_steps_trained: 270000\n",
      "    sample_time_ms: 56488.876\n",
      "    update_time_ms: 51.219\n",
      "  iterations_since_restore: 135\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.32688172043012\n",
      "    ram_util_percent: 65.98387096774192\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 154.03995021445226\n",
      "    mean_inference_ms: 12.978667768108089\n",
      "    mean_processing_ms: 23.73917799461034\n",
      "  time_since_restore: 10188.5121884346\n",
      "  time_this_iter_s: 64.79512000083923\n",
      "  time_total_s: 10188.5121884346\n",
      "  timestamp: 1575775431\n",
      "  timesteps_since_restore: 270000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 270000\n",
      "  training_iteration: 135\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10188 s, 135 iter, 270000 ts, 40.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-24-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.17604925956024\n",
      "  episode_reward_mean: 42.31036831895478\n",
      "  episode_reward_min: -21.905187256744696\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7556.469\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8778486251831055\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0359511598944664\n",
      "        policy_loss: -0.03539111465215683\n",
      "        total_loss: 221.1005096435547\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 221.12686157226562\n",
      "    load_time_ms: 3.027\n",
      "    num_steps_sampled: 272000\n",
      "    num_steps_trained: 272000\n",
      "    sample_time_ms: 56782.693\n",
      "    update_time_ms: 51.595\n",
      "  iterations_since_restore: 136\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.05000000000001\n",
      "    ram_util_percent: 65.87021276595742\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 155.30212931680387\n",
      "    mean_inference_ms: 13.054213474347005\n",
      "    mean_processing_ms: 23.803294361477732\n",
      "  time_since_restore: 10254.460483074188\n",
      "  time_this_iter_s: 65.9482946395874\n",
      "  time_total_s: 10254.460483074188\n",
      "  timestamp: 1575775497\n",
      "  timesteps_since_restore: 272000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 272000\n",
      "  training_iteration: 136\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10254 s, 136 iter, 272000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-26-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.17604925956024\n",
      "  episode_reward_mean: 41.603530838550405\n",
      "  episode_reward_min: -21.905187256744696\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7538.105\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.749802350997925\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027241548523306847\n",
      "        policy_loss: -0.024016939103603363\n",
      "        total_loss: 228.63743591308594\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 228.6544952392578\n",
      "    load_time_ms: 2.899\n",
      "    num_steps_sampled: 274000\n",
      "    num_steps_trained: 274000\n",
      "    sample_time_ms: 56711.6\n",
      "    update_time_ms: 53.416\n",
      "  iterations_since_restore: 137\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.84395604395604\n",
      "    ram_util_percent: 65.88571428571429\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 155.70430094038073\n",
      "    mean_inference_ms: 13.078965347371332\n",
      "    mean_processing_ms: 23.852579149026173\n",
      "  time_since_restore: 10318.068404436111\n",
      "  time_this_iter_s: 63.60792136192322\n",
      "  time_total_s: 10318.068404436111\n",
      "  timestamp: 1575775560\n",
      "  timesteps_since_restore: 274000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 274000\n",
      "  training_iteration: 137\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10318 s, 137 iter, 274000 ts, 41.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-27-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.17604925956024\n",
      "  episode_reward_mean: 43.04497343712503\n",
      "  episode_reward_min: -21.905187256744696\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7718.748\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7343266010284424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029851512983441353\n",
      "        policy_loss: -0.040959909558296204\n",
      "        total_loss: 215.69631958007812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 215.72975158691406\n",
      "    load_time_ms: 2.879\n",
      "    num_steps_sampled: 276000\n",
      "    num_steps_trained: 276000\n",
      "    sample_time_ms: 56694.296\n",
      "    update_time_ms: 53.379\n",
      "  iterations_since_restore: 138\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.47234042553191\n",
      "    ram_util_percent: 66.00212765957447\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 155.1453993126921\n",
      "    mean_inference_ms: 13.042093944370624\n",
      "    mean_processing_ms: 23.802444253859388\n",
      "  time_since_restore: 10383.866161108017\n",
      "  time_this_iter_s: 65.79775667190552\n",
      "  time_total_s: 10383.866161108017\n",
      "  timestamp: 1575775626\n",
      "  timesteps_since_restore: 276000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 276000\n",
      "  training_iteration: 138\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10383 s, 138 iter, 276000 ts, 43 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-28-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.63861189410828\n",
      "  episode_reward_mean: 41.99258824400029\n",
      "  episode_reward_min: -21.905187256744696\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7899.92\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.805452346801758\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025862624868750572\n",
      "        policy_loss: -0.034475572407245636\n",
      "        total_loss: 202.607666015625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 202.63558959960938\n",
      "    load_time_ms: 2.947\n",
      "    num_steps_sampled: 278000\n",
      "    num_steps_trained: 278000\n",
      "    sample_time_ms: 56570.886\n",
      "    update_time_ms: 53.183\n",
      "  iterations_since_restore: 139\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56703296703297\n",
      "    ram_util_percent: 66.05384615384617\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 154.58718839483893\n",
      "    mean_inference_ms: 13.006251196016537\n",
      "    mean_processing_ms: 23.738686279109107\n",
      "  time_since_restore: 10448.178572177887\n",
      "  time_this_iter_s: 64.31241106987\n",
      "  time_total_s: 10448.178572177887\n",
      "  timestamp: 1575775691\n",
      "  timesteps_since_restore: 278000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 278000\n",
      "  training_iteration: 139\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10448 s, 139 iter, 278000 ts, 42 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-29-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.46307181368206\n",
      "  episode_reward_mean: 40.24364653147487\n",
      "  episode_reward_min: -21.905187256744696\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7911.148\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.929374933242798\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03895920515060425\n",
      "        policy_loss: -0.005606187041848898\n",
      "        total_loss: 200.84420776367188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 200.83981323242188\n",
      "    load_time_ms: 2.866\n",
      "    num_steps_sampled: 280000\n",
      "    num_steps_trained: 280000\n",
      "    sample_time_ms: 56600.352\n",
      "    update_time_ms: 52.681\n",
      "  iterations_since_restore: 140\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.75760869565218\n",
      "    ram_util_percent: 66.12934782608694\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 155.02653433360035\n",
      "    mean_inference_ms: 13.025558472020213\n",
      "    mean_processing_ms: 23.738752151558806\n",
      "  time_since_restore: 10512.311355113983\n",
      "  time_this_iter_s: 64.13278293609619\n",
      "  time_total_s: 10512.311355113983\n",
      "  timestamp: 1575775755\n",
      "  timesteps_since_restore: 280000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 280000\n",
      "  training_iteration: 140\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10512 s, 140 iter, 280000 ts, 40.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-30-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.73090969174602\n",
      "  episode_reward_mean: 42.340944172551\n",
      "  episode_reward_min: -10.963094989628798\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7906.269\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.434630870819092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02053327113389969\n",
      "        policy_loss: -0.022961752489209175\n",
      "        total_loss: 208.90386962890625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 208.9217529296875\n",
      "    load_time_ms: 2.883\n",
      "    num_steps_sampled: 282000\n",
      "    num_steps_trained: 282000\n",
      "    sample_time_ms: 56627.673\n",
      "    update_time_ms: 52.902\n",
      "  iterations_since_restore: 141\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.85274725274724\n",
      "    ram_util_percent: 66.17912087912086\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.85126646981\n",
      "    mean_inference_ms: 12.947100677740911\n",
      "    mean_processing_ms: 23.646507128406196\n",
      "  time_since_restore: 10576.218383550644\n",
      "  time_this_iter_s: 63.90702843666077\n",
      "  time_total_s: 10576.218383550644\n",
      "  timestamp: 1575775819\n",
      "  timesteps_since_restore: 282000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 282000\n",
      "  training_iteration: 141\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10576 s, 141 iter, 282000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-31-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.73090969174602\n",
      "  episode_reward_mean: 41.86443715102789\n",
      "  episode_reward_min: -19.885471524145842\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7746.082\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.208268165588379\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03640935942530632\n",
      "        policy_loss: -0.026783820241689682\n",
      "        total_loss: 231.7226104736328\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 231.7401580810547\n",
      "    load_time_ms: 2.895\n",
      "    num_steps_sampled: 284000\n",
      "    num_steps_trained: 284000\n",
      "    sample_time_ms: 56672.139\n",
      "    update_time_ms: 53.763\n",
      "  iterations_since_restore: 142\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.67065217391306\n",
      "    ram_util_percent: 66.24347826086954\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.1199004966487\n",
      "    mean_inference_ms: 12.894257932381933\n",
      "    mean_processing_ms: 23.573051898695947\n",
      "  time_since_restore: 10640.217574596405\n",
      "  time_this_iter_s: 63.99919104576111\n",
      "  time_total_s: 10640.217574596405\n",
      "  timestamp: 1575775883\n",
      "  timesteps_since_restore: 284000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 284000\n",
      "  training_iteration: 142\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10640 s, 142 iter, 284000 ts, 41.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-32-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.73090969174602\n",
      "  episode_reward_mean: 41.80004721334951\n",
      "  episode_reward_min: -19.885471524145842\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7570.066\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3798859119415283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026969585567712784\n",
      "        policy_loss: -0.029012765735387802\n",
      "        total_loss: 208.4591827392578\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 208.48129272460938\n",
      "    load_time_ms: 2.907\n",
      "    num_steps_sampled: 286000\n",
      "    num_steps_trained: 286000\n",
      "    sample_time_ms: 56760.59\n",
      "    update_time_ms: 54.675\n",
      "  iterations_since_restore: 143\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.80222222222223\n",
      "    ram_util_percent: 66.11000000000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.5470975504936\n",
      "    mean_inference_ms: 12.919115920254807\n",
      "    mean_processing_ms: 23.59532236282442\n",
      "  time_since_restore: 10703.869960308075\n",
      "  time_this_iter_s: 63.65238571166992\n",
      "  time_total_s: 10703.869960308075\n",
      "  timestamp: 1575775946\n",
      "  timesteps_since_restore: 286000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 286000\n",
      "  training_iteration: 143\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10703 s, 143 iter, 286000 ts, 41.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-33-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.88510371347888\n",
      "  episode_reward_mean: 40.82021703141416\n",
      "  episode_reward_min: -19.885471524145842\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7570.39\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.320040464401245\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028343087062239647\n",
      "        policy_loss: -0.01965205743908882\n",
      "        total_loss: 202.35269165039062\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 202.3651885986328\n",
      "    load_time_ms: 2.965\n",
      "    num_steps_sampled: 288000\n",
      "    num_steps_trained: 288000\n",
      "    sample_time_ms: 56712.081\n",
      "    update_time_ms: 55.135\n",
      "  iterations_since_restore: 144\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.99450549450549\n",
      "    ram_util_percent: 66.08571428571429\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 154.12996631004674\n",
      "    mean_inference_ms: 12.950662364897658\n",
      "    mean_processing_ms: 23.648406052400677\n",
      "  time_since_restore: 10767.516261339188\n",
      "  time_this_iter_s: 63.64630103111267\n",
      "  time_total_s: 10767.516261339188\n",
      "  timestamp: 1575776010\n",
      "  timesteps_since_restore: 288000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 288000\n",
      "  training_iteration: 144\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10767 s, 144 iter, 288000 ts, 40.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-34-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.53126426262428\n",
      "  episode_reward_mean: 43.98280904174333\n",
      "  episode_reward_min: -19.885471524145842\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7739.964\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.755252718925476\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026409713551402092\n",
      "        policy_loss: -0.030950505286455154\n",
      "        total_loss: 215.9276885986328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 215.95187377929688\n",
      "    load_time_ms: 2.922\n",
      "    num_steps_sampled: 290000\n",
      "    num_steps_trained: 290000\n",
      "    sample_time_ms: 56684.061\n",
      "    update_time_ms: 55.735\n",
      "  iterations_since_restore: 145\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.04842105263158\n",
      "    ram_util_percent: 66.20631578947369\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.41769012253545\n",
      "    mean_inference_ms: 12.903793322222988\n",
      "    mean_processing_ms: 23.606994958187926\n",
      "  time_since_restore: 10833.727180242538\n",
      "  time_this_iter_s: 66.21091890335083\n",
      "  time_total_s: 10833.727180242538\n",
      "  timestamp: 1575776076\n",
      "  timesteps_since_restore: 290000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 290000\n",
      "  training_iteration: 145\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10833 s, 145 iter, 290000 ts, 44 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-35-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.53126426262428\n",
      "  episode_reward_mean: 42.94042973369649\n",
      "  episode_reward_min: -19.885471524145842\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7901.979\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6049997806549072\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02673512138426304\n",
      "        policy_loss: -0.030489670112729073\n",
      "        total_loss: 212.37753295898438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 212.40118408203125\n",
      "    load_time_ms: 2.916\n",
      "    num_steps_sampled: 292000\n",
      "    num_steps_trained: 292000\n",
      "    sample_time_ms: 56294.725\n",
      "    update_time_ms: 54.802\n",
      "  iterations_since_restore: 146\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.67472527472529\n",
      "    ram_util_percent: 66.2978021978022\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.26811010026776\n",
      "    mean_inference_ms: 12.890578766531089\n",
      "    mean_processing_ms: 23.581636623484137\n",
      "  time_since_restore: 10897.393438339233\n",
      "  time_this_iter_s: 63.666258096694946\n",
      "  time_total_s: 10897.393438339233\n",
      "  timestamp: 1575776140\n",
      "  timesteps_since_restore: 292000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 292000\n",
      "  training_iteration: 146\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10897 s, 146 iter, 292000 ts, 42.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-36-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.3666494943128\n",
      "  episode_reward_mean: 45.22419994567241\n",
      "  episode_reward_min: -18.382800971364762\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7900.949\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7133493423461914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021099764853715897\n",
      "        policy_loss: -0.025309383869171143\n",
      "        total_loss: 227.04347229003906\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 227.06350708007812\n",
      "    load_time_ms: 2.943\n",
      "    num_steps_sampled: 294000\n",
      "    num_steps_trained: 294000\n",
      "    sample_time_ms: 56403.646\n",
      "    update_time_ms: 54.101\n",
      "  iterations_since_restore: 147\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.83913043478262\n",
      "    ram_util_percent: 66.35434782608696\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 153.6469567017647\n",
      "    mean_inference_ms: 12.910687428571578\n",
      "    mean_processing_ms: 23.592804830579396\n",
      "  time_since_restore: 10962.072147369385\n",
      "  time_this_iter_s: 64.67870903015137\n",
      "  time_total_s: 10962.072147369385\n",
      "  timestamp: 1575776205\n",
      "  timesteps_since_restore: 294000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 294000\n",
      "  training_iteration: 147\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 10962 s, 147 iter, 294000 ts, 45.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-37-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.66877269165073\n",
      "  episode_reward_mean: 43.82805451222168\n",
      "  episode_reward_min: -1.9154051349373016\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7727.291\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8547005653381348\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027940863743424416\n",
      "        policy_loss: -0.016877861693501472\n",
      "        total_loss: 210.96981811523438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.9795684814453\n",
      "    load_time_ms: 2.899\n",
      "    num_steps_sampled: 296000\n",
      "    num_steps_trained: 296000\n",
      "    sample_time_ms: 56431.273\n",
      "    update_time_ms: 54.837\n",
      "  iterations_since_restore: 148\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.68695652173913\n",
      "    ram_util_percent: 66.4163043478261\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 152.94427089392516\n",
      "    mean_inference_ms: 12.8598653149261\n",
      "    mean_processing_ms: 23.528281528991812\n",
      "  time_since_restore: 11026.413440704346\n",
      "  time_this_iter_s: 64.34129333496094\n",
      "  time_total_s: 11026.413440704346\n",
      "  timestamp: 1575776269\n",
      "  timesteps_since_restore: 296000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 296000\n",
      "  training_iteration: 148\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11026 s, 148 iter, 296000 ts, 43.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-38-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.66877269165073\n",
      "  episode_reward_mean: 45.87459570363102\n",
      "  episode_reward_min: -13.185860171631608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 2980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7558.525\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9655983448028564\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0361674427986145\n",
      "        policy_loss: -0.02569817565381527\n",
      "        total_loss: 219.46893310546875\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 219.48553466796875\n",
      "    load_time_ms: 2.82\n",
      "    num_steps_sampled: 298000\n",
      "    num_steps_trained: 298000\n",
      "    sample_time_ms: 56499.464\n",
      "    update_time_ms: 54.415\n",
      "  iterations_since_restore: 149\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.65111111111112\n",
      "    ram_util_percent: 66.48222222222222\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 152.31817393903896\n",
      "    mean_inference_ms: 12.817822387379788\n",
      "    mean_processing_ms: 23.45248409044812\n",
      "  time_since_restore: 11089.718962192535\n",
      "  time_this_iter_s: 63.3055214881897\n",
      "  time_total_s: 11089.718962192535\n",
      "  timestamp: 1575776333\n",
      "  timesteps_since_restore: 298000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 298000\n",
      "  training_iteration: 149\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11089 s, 149 iter, 298000 ts, 45.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-39-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.66877269165073\n",
      "  episode_reward_mean: 45.24115272017566\n",
      "  episode_reward_min: -13.185860171631608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7541.84\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.855816602706909\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028751594945788383\n",
      "        policy_loss: -0.02810443751513958\n",
      "        total_loss: 228.09359741210938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 228.11447143554688\n",
      "    load_time_ms: 2.936\n",
      "    num_steps_sampled: 300000\n",
      "    num_steps_trained: 300000\n",
      "    sample_time_ms: 56572.835\n",
      "    update_time_ms: 54.101\n",
      "  iterations_since_restore: 150\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16739130434782\n",
      "    ram_util_percent: 66.39673913043477\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 152.5516598745801\n",
      "    mean_inference_ms: 12.826285093016665\n",
      "    mean_processing_ms: 23.457317447953898\n",
      "  time_since_restore: 11154.41639137268\n",
      "  time_this_iter_s: 64.69742918014526\n",
      "  time_total_s: 11154.41639137268\n",
      "  timestamp: 1575776397\n",
      "  timesteps_since_restore: 300000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 300000\n",
      "  training_iteration: 150\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11154 s, 150 iter, 300000 ts, 45.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-41-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.66877269165073\n",
      "  episode_reward_mean: 44.89298389887102\n",
      "  episode_reward_min: -13.185860171631608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7546.13\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9517767429351807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021523768082261086\n",
      "        policy_loss: -0.012977685779333115\n",
      "        total_loss: 214.1195068359375\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 214.12698364257812\n",
      "    load_time_ms: 2.839\n",
      "    num_steps_sampled: 302000\n",
      "    num_steps_trained: 302000\n",
      "    sample_time_ms: 56592.874\n",
      "    update_time_ms: 54.371\n",
      "  iterations_since_restore: 151\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.0086956521739\n",
      "    ram_util_percent: 66.3804347826087\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 152.427439874641\n",
      "    mean_inference_ms: 12.814619487201439\n",
      "    mean_processing_ms: 23.449578372343634\n",
      "  time_since_restore: 11218.57192492485\n",
      "  time_this_iter_s: 64.1555335521698\n",
      "  time_total_s: 11218.57192492485\n",
      "  timestamp: 1575776461\n",
      "  timesteps_since_restore: 302000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 302000\n",
      "  training_iteration: 151\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11218 s, 151 iter, 302000 ts, 44.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-42-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.66877269165073\n",
      "  episode_reward_mean: 43.4251016250251\n",
      "  episode_reward_min: -13.185860171631608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7699.62\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5037732124328613\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03392967954277992\n",
      "        policy_loss: -0.023723522201180458\n",
      "        total_loss: 201.9871826171875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 202.0023193359375\n",
      "    load_time_ms: 2.814\n",
      "    num_steps_sampled: 304000\n",
      "    num_steps_trained: 304000\n",
      "    sample_time_ms: 56667.403\n",
      "    update_time_ms: 53.631\n",
      "  iterations_since_restore: 152\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.20425531914894\n",
      "    ram_util_percent: 66.44042553191488\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.99600134784075\n",
      "    mean_inference_ms: 12.782564315210347\n",
      "    mean_processing_ms: 23.406634149116076\n",
      "  time_since_restore: 11284.84310245514\n",
      "  time_this_iter_s: 66.2711775302887\n",
      "  time_total_s: 11284.84310245514\n",
      "  timestamp: 1575776528\n",
      "  timesteps_since_restore: 304000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 304000\n",
      "  training_iteration: 152\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11284 s, 152 iter, 304000 ts, 43.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-43-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.62305281220067\n",
      "  episode_reward_mean: 41.20950951422831\n",
      "  episode_reward_min: -16.968434099738793\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7889.462\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5221612453460693\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020985176786780357\n",
      "        policy_loss: -0.016245804727077484\n",
      "        total_loss: 243.021484375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 243.03237915039062\n",
      "    load_time_ms: 2.956\n",
      "    num_steps_sampled: 306000\n",
      "    num_steps_trained: 306000\n",
      "    sample_time_ms: 56532.264\n",
      "    update_time_ms: 53.793\n",
      "  iterations_since_restore: 153\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.55869565217391\n",
      "    ram_util_percent: 66.50326086956522\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 152.22832054709232\n",
      "    mean_inference_ms: 12.792018858727033\n",
      "    mean_processing_ms: 23.4092552572628\n",
      "  time_since_restore: 11349.046944856644\n",
      "  time_this_iter_s: 64.20384240150452\n",
      "  time_total_s: 11349.046944856644\n",
      "  timestamp: 1575776592\n",
      "  timesteps_since_restore: 306000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 306000\n",
      "  training_iteration: 153\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11349 s, 153 iter, 306000 ts, 41.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-44-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.62305281220067\n",
      "  episode_reward_mean: 40.93899307014632\n",
      "  episode_reward_min: -16.968434099738793\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7891.571\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9287919998168945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025096094235777855\n",
      "        policy_loss: -0.007491626311093569\n",
      "        total_loss: 224.0176239013672\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 224.01866149902344\n",
      "    load_time_ms: 3.031\n",
      "    num_steps_sampled: 308000\n",
      "    num_steps_trained: 308000\n",
      "    sample_time_ms: 56622.892\n",
      "    update_time_ms: 53.623\n",
      "  iterations_since_restore: 154\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.25217391304349\n",
      "    ram_util_percent: 66.6141304347826\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 152.4344162109401\n",
      "    mean_inference_ms: 12.801320883684394\n",
      "    mean_processing_ms: 23.427637187307877\n",
      "  time_since_restore: 11413.634981632233\n",
      "  time_this_iter_s: 64.58803677558899\n",
      "  time_total_s: 11413.634981632233\n",
      "  timestamp: 1575776657\n",
      "  timesteps_since_restore: 308000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 308000\n",
      "  training_iteration: 154\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11413 s, 154 iter, 308000 ts, 40.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.50797250965626\n",
      "  episode_reward_mean: 38.649360107081684\n",
      "  episode_reward_min: -16.968434099738793\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7721.104\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.419745445251465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01252730656415224\n",
      "        policy_loss: -0.01945333555340767\n",
      "        total_loss: 238.13134765625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 238.1476593017578\n",
      "    load_time_ms: 3.148\n",
      "    num_steps_sampled: 310000\n",
      "    num_steps_trained: 310000\n",
      "    sample_time_ms: 56569.016\n",
      "    update_time_ms: 53.292\n",
      "  iterations_since_restore: 155\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.6641304347826\n",
      "    ram_util_percent: 66.62717391304349\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.94897651379162\n",
      "    mean_inference_ms: 12.7691928867942\n",
      "    mean_processing_ms: 23.38764469963842\n",
      "  time_since_restore: 11477.61437177658\n",
      "  time_this_iter_s: 63.979390144348145\n",
      "  time_total_s: 11477.61437177658\n",
      "  timestamp: 1575776721\n",
      "  timesteps_since_restore: 310000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 310000\n",
      "  training_iteration: 155\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11477 s, 155 iter, 310000 ts, 38.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-46-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.50797250965626\n",
      "  episode_reward_mean: 37.44201825594796\n",
      "  episode_reward_min: -16.968434099738793\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7554.176\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.179271936416626\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012661295011639595\n",
      "        policy_loss: -0.016271132975816727\n",
      "        total_loss: 233.41903686523438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 233.43211364746094\n",
      "    load_time_ms: 3.463\n",
      "    num_steps_sampled: 312000\n",
      "    num_steps_trained: 312000\n",
      "    sample_time_ms: 56802.241\n",
      "    update_time_ms: 53.695\n",
      "  iterations_since_restore: 156\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16263736263735\n",
      "    ram_util_percent: 66.69890109890109\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.81923454391554\n",
      "    mean_inference_ms: 12.756881961145481\n",
      "    mean_processing_ms: 23.359998566209942\n",
      "  time_since_restore: 11541.94843673706\n",
      "  time_this_iter_s: 64.33406496047974\n",
      "  time_total_s: 11541.94843673706\n",
      "  timestamp: 1575776785\n",
      "  timesteps_since_restore: 312000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 312000\n",
      "  training_iteration: 156\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11541 s, 156 iter, 312000 ts, 37.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-47-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.50797250965626\n",
      "  episode_reward_mean: 38.853240596355256\n",
      "  episode_reward_min: -17.418675411455908\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7567.292\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.442939519882202\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01636192388832569\n",
      "        policy_loss: -0.02122204750776291\n",
      "        total_loss: 218.37571716308594\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 218.3927764892578\n",
      "    load_time_ms: 3.465\n",
      "    num_steps_sampled: 314000\n",
      "    num_steps_trained: 314000\n",
      "    sample_time_ms: 56910.799\n",
      "    update_time_ms: 53.092\n",
      "  iterations_since_restore: 157\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.33297872340425\n",
      "    ram_util_percent: 66.58404255319148\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.47178863992204\n",
      "    mean_inference_ms: 12.73041382049804\n",
      "    mean_processing_ms: 23.335683657128587\n",
      "  time_since_restore: 11607.8436357975\n",
      "  time_this_iter_s: 65.89519906044006\n",
      "  time_total_s: 11607.8436357975\n",
      "  timestamp: 1575776851\n",
      "  timesteps_since_restore: 314000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 314000\n",
      "  training_iteration: 157\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11607 s, 157 iter, 314000 ts, 38.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-48-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.50797250965626\n",
      "  episode_reward_mean: 40.28204383182333\n",
      "  episode_reward_min: -17.418675411455908\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7734.199\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.25312501192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0572147369384766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07287440448999405\n",
      "        policy_loss: 0.038197074085474014\n",
      "        total_loss: 210.38900756835938\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 210.33229064941406\n",
      "    load_time_ms: 3.434\n",
      "    num_steps_sampled: 316000\n",
      "    num_steps_trained: 316000\n",
      "    sample_time_ms: 56831.301\n",
      "    update_time_ms: 52.34\n",
      "  iterations_since_restore: 158\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.91827956989246\n",
      "    ram_util_percent: 66.61612903225807\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.54959598525042\n",
      "    mean_inference_ms: 12.734466305498366\n",
      "    mean_processing_ms: 23.351290077217005\n",
      "  time_since_restore: 11673.046149492264\n",
      "  time_this_iter_s: 65.20251369476318\n",
      "  time_total_s: 11673.046149492264\n",
      "  timestamp: 1575776916\n",
      "  timesteps_since_restore: 316000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 316000\n",
      "  training_iteration: 158\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11673 s, 158 iter, 316000 ts, 40.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-49-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.50797250965626\n",
      "  episode_reward_mean: 41.223754541902856\n",
      "  episode_reward_min: -17.418675411455908\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7897.832\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6910417079925537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012632785364985466\n",
      "        policy_loss: -0.0185499656945467\n",
      "        total_loss: 196.25315856933594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 196.26693725585938\n",
      "    load_time_ms: 3.433\n",
      "    num_steps_sampled: 318000\n",
      "    num_steps_trained: 318000\n",
      "    sample_time_ms: 56755.291\n",
      "    update_time_ms: 53.322\n",
      "  iterations_since_restore: 159\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.20434782608694\n",
      "    ram_util_percent: 66.64239130434783\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.22587302170027\n",
      "    mean_inference_ms: 12.706895624914978\n",
      "    mean_processing_ms: 23.314023986625834\n",
      "  time_since_restore: 11737.236807584763\n",
      "  time_this_iter_s: 64.19065809249878\n",
      "  time_total_s: 11737.236807584763\n",
      "  timestamp: 1575776980\n",
      "  timesteps_since_restore: 318000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 318000\n",
      "  training_iteration: 159\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11737 s, 159 iter, 318000 ts, 41.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-50-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.61460273351798\n",
      "  episode_reward_mean: 42.34219222565009\n",
      "  episode_reward_min: -17.418675411455908\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7903.433\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.775564432144165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015059598721563816\n",
      "        policy_loss: -0.017229674383997917\n",
      "        total_loss: 191.6200408935547\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 191.63153076171875\n",
      "    load_time_ms: 3.316\n",
      "    num_steps_sampled: 320000\n",
      "    num_steps_trained: 320000\n",
      "    sample_time_ms: 56523.686\n",
      "    update_time_ms: 54.472\n",
      "  iterations_since_restore: 160\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.6\n",
      "    ram_util_percent: 66.73483146067416\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.86477081220679\n",
      "    mean_inference_ms: 12.744435483233516\n",
      "    mean_processing_ms: 23.347398561781358\n",
      "  time_since_restore: 11799.687680959702\n",
      "  time_this_iter_s: 62.450873374938965\n",
      "  time_total_s: 11799.687680959702\n",
      "  timestamp: 1575777043\n",
      "  timesteps_since_restore: 320000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 320000\n",
      "  training_iteration: 160\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11799 s, 160 iter, 320000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-51-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.84646919700387\n",
      "  episode_reward_mean: 45.77676092058614\n",
      "  episode_reward_min: -17.418675411455908\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7901.348\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4451005458831787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01567014865577221\n",
      "        policy_loss: -0.009445393458008766\n",
      "        total_loss: 196.44229125976562\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 196.44570922851562\n",
      "    load_time_ms: 3.316\n",
      "    num_steps_sampled: 322000\n",
      "    num_steps_trained: 322000\n",
      "    sample_time_ms: 56572.305\n",
      "    update_time_ms: 55.539\n",
      "  iterations_since_restore: 161\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.7891304347826\n",
      "    ram_util_percent: 66.80652173913045\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.71751121474404\n",
      "    mean_inference_ms: 12.730526968350759\n",
      "    mean_processing_ms: 23.3472878765886\n",
      "  time_since_restore: 11864.320266246796\n",
      "  time_this_iter_s: 64.63258528709412\n",
      "  time_total_s: 11864.320266246796\n",
      "  timestamp: 1575777108\n",
      "  timesteps_since_restore: 322000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 322000\n",
      "  training_iteration: 161\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11864 s, 161 iter, 322000 ts, 45.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-52-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.13463518519362\n",
      "  episode_reward_mean: 47.847490862590455\n",
      "  episode_reward_min: -4.579376122495438\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7727.895\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1442461013793945\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.033837586641311646\n",
      "        policy_loss: -0.035965144634246826\n",
      "        total_loss: 217.7439422607422\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 217.76707458496094\n",
      "    load_time_ms: 3.31\n",
      "    num_steps_sampled: 324000\n",
      "    num_steps_trained: 324000\n",
      "    sample_time_ms: 56488.895\n",
      "    update_time_ms: 55.319\n",
      "  iterations_since_restore: 162\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.48901098901099\n",
      "    ram_util_percent: 66.86813186813188\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 151.3729129912926\n",
      "    mean_inference_ms: 12.706599562628634\n",
      "    mean_processing_ms: 23.321790365738917\n",
      "  time_since_restore: 11928.029486894608\n",
      "  time_this_iter_s: 63.70922064781189\n",
      "  time_total_s: 11928.029486894608\n",
      "  timestamp: 1575777171\n",
      "  timesteps_since_restore: 324000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 324000\n",
      "  training_iteration: 162\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11928 s, 162 iter, 324000 ts, 47.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-53-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.13463518519362\n",
      "  episode_reward_mean: 49.682193082714846\n",
      "  episode_reward_min: -4.842459054981326\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7542.12\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.371840715408325\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021147040650248528\n",
      "        policy_loss: -0.010009822435677052\n",
      "        total_loss: 208.43222045898438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 208.4342498779297\n",
      "    load_time_ms: 3.93\n",
      "    num_steps_sampled: 326000\n",
      "    num_steps_trained: 326000\n",
      "    sample_time_ms: 56716.665\n",
      "    update_time_ms: 54.982\n",
      "  iterations_since_restore: 163\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.31290322580644\n",
      "    ram_util_percent: 66.92795698924735\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.83796043360425\n",
      "    mean_inference_ms: 12.66605612240911\n",
      "    mean_processing_ms: 23.266341255464496\n",
      "  time_since_restore: 11992.671103477478\n",
      "  time_this_iter_s: 64.64161658287048\n",
      "  time_total_s: 11992.671103477478\n",
      "  timestamp: 1575777236\n",
      "  timesteps_since_restore: 326000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 326000\n",
      "  training_iteration: 163\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 11992 s, 163 iter, 326000 ts, 49.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-55-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.13463518519362\n",
      "  episode_reward_mean: 47.363362310631274\n",
      "  episode_reward_min: -4.842459054981326\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7549.218\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1293675899505615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02436843514442444\n",
      "        policy_loss: -0.011976150795817375\n",
      "        total_loss: 207.10702514648438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 207.1097412109375\n",
      "    load_time_ms: 3.838\n",
      "    num_steps_sampled: 328000\n",
      "    num_steps_trained: 328000\n",
      "    sample_time_ms: 56886.392\n",
      "    update_time_ms: 55.175\n",
      "  iterations_since_restore: 164\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.63936170212766\n",
      "    ram_util_percent: 66.82765957446806\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.62725264403127\n",
      "    mean_inference_ms: 12.652830797018385\n",
      "    mean_processing_ms: 23.231190509437784\n",
      "  time_since_restore: 12059.006303310394\n",
      "  time_this_iter_s: 66.33519983291626\n",
      "  time_total_s: 12059.006303310394\n",
      "  timestamp: 1575777302\n",
      "  timesteps_since_restore: 328000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 328000\n",
      "  training_iteration: 164\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12059 s, 164 iter, 328000 ts, 47.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-56-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.13463518519362\n",
      "  episode_reward_mean: 50.05890108938054\n",
      "  episode_reward_min: -4.842459054981326\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7714.349\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37968748807907104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8867506980895996\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04981003329157829\n",
      "        policy_loss: -0.029619887471199036\n",
      "        total_loss: 217.3514862060547\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 217.36212158203125\n",
      "    load_time_ms: 3.749\n",
      "    num_steps_sampled: 330000\n",
      "    num_steps_trained: 330000\n",
      "    sample_time_ms: 57014.767\n",
      "    update_time_ms: 55.105\n",
      "  iterations_since_restore: 165\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.55624999999999\n",
      "    ram_util_percent: 66.90104166666666\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.77197886006684\n",
      "    mean_inference_ms: 12.59564512039847\n",
      "    mean_processing_ms: 23.16654035537481\n",
      "  time_since_restore: 12125.905790805817\n",
      "  time_this_iter_s: 66.89948749542236\n",
      "  time_total_s: 12125.905790805817\n",
      "  timestamp: 1575777369\n",
      "  timesteps_since_restore: 330000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 330000\n",
      "  training_iteration: 165\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12125 s, 165 iter, 330000 ts, 50.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-57-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.13463518519362\n",
      "  episode_reward_mean: 50.30005416403768\n",
      "  episode_reward_min: -4.842459054981326\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7786.903\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.569531261920929\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.095569372177124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008108648471534252\n",
      "        policy_loss: -0.004637269768863916\n",
      "        total_loss: 219.73312377929688\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 219.733154296875\n",
      "    load_time_ms: 4.204\n",
      "    num_steps_sampled: 332000\n",
      "    num_steps_trained: 332000\n",
      "    sample_time_ms: 56792.34\n",
      "    update_time_ms: 54.798\n",
      "  iterations_since_restore: 166\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 66.91685393258426\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.67161417661157\n",
      "    mean_inference_ms: 12.591192711407714\n",
      "    mean_processing_ms: 23.151759423830256\n",
      "  time_since_restore: 12188.747067451477\n",
      "  time_this_iter_s: 62.8412766456604\n",
      "  time_total_s: 12188.747067451477\n",
      "  timestamp: 1575777432\n",
      "  timesteps_since_restore: 332000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 332000\n",
      "  training_iteration: 166\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12188 s, 166 iter, 332000 ts, 50.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-58-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.80369363126415\n",
      "  episode_reward_mean: 48.286059173465375\n",
      "  episode_reward_min: -4.842459054981326\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7769.599\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2847656309604645\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3433804512023926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007464401423931122\n",
      "        policy_loss: -0.014830663800239563\n",
      "        total_loss: 198.95957946777344\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 198.97232055664062\n",
      "    load_time_ms: 4.478\n",
      "    num_steps_sampled: 334000\n",
      "    num_steps_trained: 334000\n",
      "    sample_time_ms: 56638.248\n",
      "    update_time_ms: 54.063\n",
      "  iterations_since_restore: 167\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.62934782608696\n",
      "    ram_util_percent: 66.98478260869565\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.79380739112057\n",
      "    mean_inference_ms: 12.596002271751459\n",
      "    mean_processing_ms: 23.138731994531565\n",
      "  time_since_restore: 12252.927759170532\n",
      "  time_this_iter_s: 64.18069171905518\n",
      "  time_total_s: 12252.927759170532\n",
      "  timestamp: 1575777496\n",
      "  timesteps_since_restore: 334000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 334000\n",
      "  training_iteration: 167\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12252 s, 167 iter, 334000 ts, 48.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_22-59-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.80369363126415\n",
      "  episode_reward_mean: 48.45763214964333\n",
      "  episode_reward_min: -6.755394207451128\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7597.063\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.14238281548023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.648334503173828\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004243825096637011\n",
      "        policy_loss: -0.009211529046297073\n",
      "        total_loss: 213.4446563720703\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 213.4532012939453\n",
      "    load_time_ms: 4.56\n",
      "    num_steps_sampled: 336000\n",
      "    num_steps_trained: 336000\n",
      "    sample_time_ms: 56729.93\n",
      "    update_time_ms: 54.071\n",
      "  iterations_since_restore: 168\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.19673913043479\n",
      "    ram_util_percent: 67.075\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.90778087822463\n",
      "    mean_inference_ms: 12.60047103594793\n",
      "    mean_processing_ms: 23.130707087031855\n",
      "  time_since_restore: 12317.314700841904\n",
      "  time_this_iter_s: 64.38694167137146\n",
      "  time_total_s: 12317.314700841904\n",
      "  timestamp: 1575777561\n",
      "  timesteps_since_restore: 336000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 336000\n",
      "  training_iteration: 168\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12317 s, 168 iter, 336000 ts, 48.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-00-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.80369363126415\n",
      "  episode_reward_mean: 51.731411294414535\n",
      "  episode_reward_min: -6.755394207451128\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7435.951\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8136329650878906\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018439631909132004\n",
      "        policy_loss: -0.013984614983201027\n",
      "        total_loss: 223.97750854492188\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 223.9901885986328\n",
      "    load_time_ms: 4.724\n",
      "    num_steps_sampled: 338000\n",
      "    num_steps_trained: 338000\n",
      "    sample_time_ms: 56808.538\n",
      "    update_time_ms: 52.927\n",
      "  iterations_since_restore: 169\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.34333333333333\n",
      "    ram_util_percent: 67.15888888888888\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.68429757025547\n",
      "    mean_inference_ms: 12.58031578828363\n",
      "    mean_processing_ms: 23.1196842738964\n",
      "  time_since_restore: 12380.678907632828\n",
      "  time_this_iter_s: 63.36420679092407\n",
      "  time_total_s: 12380.678907632828\n",
      "  timestamp: 1575777624\n",
      "  timesteps_since_restore: 338000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 338000\n",
      "  training_iteration: 169\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12380 s, 169 iter, 338000 ts, 51.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-01-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.41766171185255\n",
      "  episode_reward_mean: 51.114460872030385\n",
      "  episode_reward_min: -10.585220251445634\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7424.585\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.564225673675537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023873722180724144\n",
      "        policy_loss: -0.01764843426644802\n",
      "        total_loss: 220.5199432373047\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 220.53598022460938\n",
      "    load_time_ms: 4.799\n",
      "    num_steps_sampled: 340000\n",
      "    num_steps_trained: 340000\n",
      "    sample_time_ms: 57045.038\n",
      "    update_time_ms: 51.731\n",
      "  iterations_since_restore: 170\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.0141304347826\n",
      "    ram_util_percent: 67.17282608695653\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.5728407380188\n",
      "    mean_inference_ms: 12.636255445291818\n",
      "    mean_processing_ms: 23.15562257407172\n",
      "  time_since_restore: 12445.357768297195\n",
      "  time_this_iter_s: 64.67886066436768\n",
      "  time_total_s: 12445.357768297195\n",
      "  timestamp: 1575777689\n",
      "  timesteps_since_restore: 340000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 340000\n",
      "  training_iteration: 170\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12445 s, 170 iter, 340000 ts, 51.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-02-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.41766171185255\n",
      "  episode_reward_mean: 48.477560885677256\n",
      "  episode_reward_min: -10.585220251445634\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7558.667\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.278710126876831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02496488206088543\n",
      "        policy_loss: -0.02334386669099331\n",
      "        total_loss: 216.2622833251953\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 216.28390502929688\n",
      "    load_time_ms: 4.79\n",
      "    num_steps_sampled: 342000\n",
      "    num_steps_trained: 342000\n",
      "    sample_time_ms: 57050.565\n",
      "    update_time_ms: 50.582\n",
      "  iterations_since_restore: 171\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.29894736842105\n",
      "    ram_util_percent: 67.05157894736844\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.46490716301471\n",
      "    mean_inference_ms: 12.623092072679801\n",
      "    mean_processing_ms: 23.127656647960183\n",
      "  time_since_restore: 12511.43100643158\n",
      "  time_this_iter_s: 66.07323813438416\n",
      "  time_total_s: 12511.43100643158\n",
      "  timestamp: 1575777755\n",
      "  timesteps_since_restore: 342000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 342000\n",
      "  training_iteration: 171\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12511 s, 171 iter, 342000 ts, 48.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-03-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.064389970959\n",
      "  episode_reward_mean: 48.27052206385724\n",
      "  episode_reward_min: -10.585220251445634\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7761.756\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07119140774011612\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2994346618652344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05536162108182907\n",
      "        policy_loss: 0.001900714822113514\n",
      "        total_loss: 234.4754638671875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 234.4696502685547\n",
      "    load_time_ms: 4.871\n",
      "    num_steps_sampled: 344000\n",
      "    num_steps_trained: 344000\n",
      "    sample_time_ms: 56917.206\n",
      "    update_time_ms: 57.364\n",
      "  iterations_since_restore: 172\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.26521739130436\n",
      "    ram_util_percent: 67.20543478260868\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.46232028948327\n",
      "    mean_inference_ms: 12.623253743103868\n",
      "    mean_processing_ms: 23.106009320997345\n",
      "  time_since_restore: 12575.898733139038\n",
      "  time_this_iter_s: 64.4677267074585\n",
      "  time_total_s: 12575.898733139038\n",
      "  timestamp: 1575777820\n",
      "  timesteps_since_restore: 344000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 344000\n",
      "  training_iteration: 172\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12575 s, 172 iter, 344000 ts, 48.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-04-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.064389970959\n",
      "  episode_reward_mean: 46.287424352632115\n",
      "  episode_reward_min: -13.84231728112079\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7766.779\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10678710788488388\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.740713596343994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06455817073583603\n",
      "        policy_loss: 0.016801433637738228\n",
      "        total_loss: 231.8526153564453\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 231.82891845703125\n",
      "    load_time_ms: 4.203\n",
      "    num_steps_sampled: 346000\n",
      "    num_steps_trained: 346000\n",
      "    sample_time_ms: 56793.693\n",
      "    update_time_ms: 58.626\n",
      "  iterations_since_restore: 173\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.27555555555557\n",
      "    ram_util_percent: 67.29555555555557\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.26231717348287\n",
      "    mean_inference_ms: 12.61241338932123\n",
      "    mean_processing_ms: 23.08402420826985\n",
      "  time_since_restore: 12639.355758666992\n",
      "  time_this_iter_s: 63.4570255279541\n",
      "  time_total_s: 12639.355758666992\n",
      "  timestamp: 1575777883\n",
      "  timesteps_since_restore: 346000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 346000\n",
      "  training_iteration: 173\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12639 s, 173 iter, 346000 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-05-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.064389970959\n",
      "  episode_reward_mean: 42.40730445311481\n",
      "  episode_reward_min: -13.84231728112079\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7768.732\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16018065810203552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.066329002380371\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020619235932826996\n",
      "        policy_loss: -0.022941723465919495\n",
      "        total_loss: 239.66641235351562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 239.68612670898438\n",
      "    load_time_ms: 4.204\n",
      "    num_steps_sampled: 348000\n",
      "    num_steps_trained: 348000\n",
      "    sample_time_ms: 56639.504\n",
      "    update_time_ms: 58.483\n",
      "  iterations_since_restore: 174\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.38064516129032\n",
      "    ram_util_percent: 67.33118279569895\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 150.37568813357117\n",
      "    mean_inference_ms: 12.621659064139845\n",
      "    mean_processing_ms: 23.088260105531845\n",
      "  time_since_restore: 12704.18207025528\n",
      "  time_this_iter_s: 64.82631158828735\n",
      "  time_total_s: 12704.18207025528\n",
      "  timestamp: 1575777948\n",
      "  timesteps_since_restore: 348000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 348000\n",
      "  training_iteration: 174\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12704 s, 174 iter, 348000 ts, 42.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-06-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.064389970959\n",
      "  episode_reward_mean: 38.79404115679817\n",
      "  episode_reward_min: -33.4494911178312\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7601.195\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16018065810203552\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2580180168151855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.15632766485214233\n",
      "        policy_loss: 0.013096066191792488\n",
      "        total_loss: 252.1628875732422\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 252.12469482421875\n",
      "    load_time_ms: 4.222\n",
      "    num_steps_sampled: 350000\n",
      "    num_steps_trained: 350000\n",
      "    sample_time_ms: 56496.739\n",
      "    update_time_ms: 58.034\n",
      "  iterations_since_restore: 175\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.31758241758243\n",
      "    ram_util_percent: 67.43956043956042\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.267671955144\n",
      "    mean_inference_ms: 12.547000111493503\n",
      "    mean_processing_ms: 23.019615402011727\n",
      "  time_since_restore: 12767.983400344849\n",
      "  time_this_iter_s: 63.80133008956909\n",
      "  time_total_s: 12767.983400344849\n",
      "  timestamp: 1575778012\n",
      "  timesteps_since_restore: 350000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 350000\n",
      "  training_iteration: 175\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12767 s, 175 iter, 350000 ts, 38.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-07-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.064389970959\n",
      "  episode_reward_mean: 38.092497997378814\n",
      "  episode_reward_min: -33.4494911178312\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7534.197\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24027100205421448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8719451427459717\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03674259036779404\n",
      "        policy_loss: 0.008072992786765099\n",
      "        total_loss: 216.30519104003906\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 216.2882537841797\n",
      "    load_time_ms: 3.436\n",
      "    num_steps_sampled: 352000\n",
      "    num_steps_trained: 352000\n",
      "    sample_time_ms: 56679.28\n",
      "    update_time_ms: 58.374\n",
      "  iterations_since_restore: 176\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.35164835164835\n",
      "    ram_util_percent: 67.52197802197803\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 149.1807718590858\n",
      "    mean_inference_ms: 12.53961822867867\n",
      "    mean_processing_ms: 23.010087077831077\n",
      "  time_since_restore: 12831.986765623093\n",
      "  time_this_iter_s: 64.00336527824402\n",
      "  time_total_s: 12831.986765623093\n",
      "  timestamp: 1575778076\n",
      "  timesteps_since_restore: 352000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 352000\n",
      "  training_iteration: 176\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12831 s, 176 iter, 352000 ts, 38.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-09-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.87301760795572\n",
      "  episode_reward_mean: 40.04471486533035\n",
      "  episode_reward_min: -33.4494911178312\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7543.32\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24027100205421448\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.163116931915283\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008657919242978096\n",
      "        policy_loss: -0.01193595677614212\n",
      "        total_loss: 220.42408752441406\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 220.43397521972656\n",
      "    load_time_ms: 3.121\n",
      "    num_steps_sampled: 354000\n",
      "    num_steps_trained: 354000\n",
      "    sample_time_ms: 56794.017\n",
      "    update_time_ms: 59.508\n",
      "  iterations_since_restore: 177\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.66702127659575\n",
      "    ram_util_percent: 67.57340425531915\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.67745950284245\n",
      "    mean_inference_ms: 12.506357267348099\n",
      "    mean_processing_ms: 22.98161197416419\n",
      "  time_since_restore: 12897.419461011887\n",
      "  time_this_iter_s: 65.43269538879395\n",
      "  time_total_s: 12897.419461011887\n",
      "  timestamp: 1575778141\n",
      "  timesteps_since_restore: 354000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 354000\n",
      "  training_iteration: 177\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12897 s, 177 iter, 354000 ts, 40 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.02263746147374\n",
      "  episode_reward_mean: 41.57358168580444\n",
      "  episode_reward_min: -33.4494911178312\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7737.171\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12013550102710724\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.565739154815674\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02244812808930874\n",
      "        policy_loss: -0.023318080231547356\n",
      "        total_loss: 224.8430633544922\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 224.8637237548828\n",
      "    load_time_ms: 3.047\n",
      "    num_steps_sampled: 356000\n",
      "    num_steps_trained: 356000\n",
      "    sample_time_ms: 56916.452\n",
      "    update_time_ms: 59.738\n",
      "  iterations_since_restore: 178\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.50416666666668\n",
      "    ram_util_percent: 67.42916666666666\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.49032269789248\n",
      "    mean_inference_ms: 12.488778558430983\n",
      "    mean_processing_ms: 22.961746733971367\n",
      "  time_since_restore: 12964.971650600433\n",
      "  time_this_iter_s: 67.55218958854675\n",
      "  time_total_s: 12964.971650600433\n",
      "  timestamp: 1575778209\n",
      "  timesteps_since_restore: 356000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 356000\n",
      "  training_iteration: 178\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 12964 s, 178 iter, 356000 ts, 41.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-11-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.78778309239719\n",
      "  episode_reward_mean: 43.47313787908649\n",
      "  episode_reward_min: -33.4494911178312\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7843.277\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12013550102710724\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.004718065261841\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018372200429439545\n",
      "        policy_loss: -0.015595094300806522\n",
      "        total_loss: 242.055908203125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 242.0692596435547\n",
      "    load_time_ms: 4.539\n",
      "    num_steps_sampled: 358000\n",
      "    num_steps_trained: 358000\n",
      "    sample_time_ms: 56830.248\n",
      "    update_time_ms: 58.948\n",
      "  iterations_since_restore: 179\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.94175824175825\n",
      "    ram_util_percent: 67.47912087912087\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.40449396570344\n",
      "    mean_inference_ms: 12.47746878435034\n",
      "    mean_processing_ms: 22.94005523203142\n",
      "  time_since_restore: 13028.538672924042\n",
      "  time_this_iter_s: 63.5670223236084\n",
      "  time_total_s: 13028.538672924042\n",
      "  timestamp: 1575778272\n",
      "  timesteps_since_restore: 358000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 358000\n",
      "  training_iteration: 179\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13028 s, 179 iter, 358000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-12-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.78778309239719\n",
      "  episode_reward_mean: 47.44223550236942\n",
      "  episode_reward_min: -17.40600413768558\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7873.349\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12013550102710724\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.463239312171936\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011072725988924503\n",
      "        policy_loss: -0.017057906836271286\n",
      "        total_loss: 222.15618896484375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.17193603515625\n",
      "    load_time_ms: 4.649\n",
      "    num_steps_sampled: 360000\n",
      "    num_steps_trained: 360000\n",
      "    sample_time_ms: 56809.809\n",
      "    update_time_ms: 58.252\n",
      "  iterations_since_restore: 180\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.2891304347826\n",
      "    ram_util_percent: 67.55869565217391\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.46173638655583\n",
      "    mean_inference_ms: 12.477757759068165\n",
      "    mean_processing_ms: 22.934446574502672\n",
      "  time_since_restore: 13093.305559158325\n",
      "  time_this_iter_s: 64.76688623428345\n",
      "  time_total_s: 13093.305559158325\n",
      "  timestamp: 1575778337\n",
      "  timesteps_since_restore: 360000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 360000\n",
      "  training_iteration: 180\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13093 s, 180 iter, 360000 ts, 47.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-13-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.78778309239719\n",
      "  episode_reward_mean: 48.63678575230425\n",
      "  episode_reward_min: -8.085974246032979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7752.508\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12013550102710724\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8461418151855469\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.39217108488082886\n",
      "        policy_loss: 0.05651700869202614\n",
      "        total_loss: 221.5269012451172\n",
      "        vf_explained_var: -2.384185791015625e-07\n",
      "        vf_loss: 221.4232635498047\n",
      "    load_time_ms: 4.85\n",
      "    num_steps_sampled: 362000\n",
      "    num_steps_trained: 362000\n",
      "    sample_time_ms: 56662.894\n",
      "    update_time_ms: 58.113\n",
      "  iterations_since_restore: 181\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.94065934065932\n",
      "    ram_util_percent: 67.59450549450548\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.165156440132\n",
      "    mean_inference_ms: 12.457267001422242\n",
      "    mean_processing_ms: 22.90272725143447\n",
      "  time_since_restore: 13156.648374080658\n",
      "  time_this_iter_s: 63.342814922332764\n",
      "  time_total_s: 13156.648374080658\n",
      "  timestamp: 1575778401\n",
      "  timesteps_since_restore: 362000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 362000\n",
      "  training_iteration: 181\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13156 s, 181 iter, 362000 ts, 48.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-14-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.78778309239719\n",
      "  episode_reward_mean: 47.339852953059705\n",
      "  episode_reward_min: -8.085974246032979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7552.549\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.18020324409008026\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.523441791534424\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00958026759326458\n",
      "        policy_loss: -0.014625877141952515\n",
      "        total_loss: 216.8623504638672\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.875244140625\n",
      "    load_time_ms: 4.829\n",
      "    num_steps_sampled: 364000\n",
      "    num_steps_trained: 364000\n",
      "    sample_time_ms: 56832.706\n",
      "    update_time_ms: 51.707\n",
      "  iterations_since_restore: 182\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.3\n",
      "    ram_util_percent: 67.65714285714284\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.2900085017741\n",
      "    mean_inference_ms: 12.463609385991003\n",
      "    mean_processing_ms: 22.90414341842734\n",
      "  time_since_restore: 13220.750725507736\n",
      "  time_this_iter_s: 64.10235142707825\n",
      "  time_total_s: 13220.750725507736\n",
      "  timestamp: 1575778465\n",
      "  timesteps_since_restore: 364000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 364000\n",
      "  training_iteration: 182\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13220 s, 182 iter, 364000 ts, 47.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-15-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.78778309239719\n",
      "  episode_reward_mean: 49.317647305040765\n",
      "  episode_reward_min: -8.085974246032979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7564.691\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09010162204504013\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.462308645248413\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029872402548789978\n",
      "        policy_loss: -0.0058569712564349174\n",
      "        total_loss: 227.90521240234375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 227.9084014892578\n",
      "    load_time_ms: 5.118\n",
      "    num_steps_sampled: 366000\n",
      "    num_steps_trained: 366000\n",
      "    sample_time_ms: 56924.176\n",
      "    update_time_ms: 51.562\n",
      "  iterations_since_restore: 183\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.5913043478261\n",
      "    ram_util_percent: 67.73586956521739\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.1060150584878\n",
      "    mean_inference_ms: 12.453615908046013\n",
      "    mean_processing_ms: 22.88168702688324\n",
      "  time_since_restore: 13285.234001159668\n",
      "  time_this_iter_s: 64.48327565193176\n",
      "  time_total_s: 13285.234001159668\n",
      "  timestamp: 1575778529\n",
      "  timesteps_since_restore: 366000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 366000\n",
      "  training_iteration: 183\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13285 s, 183 iter, 366000 ts, 49.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-16-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.78907164265237\n",
      "  episode_reward_mean: 48.98482174450792\n",
      "  episode_reward_min: -8.085974246032979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7766.604\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09010162204504013\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5525176525115967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017593827098608017\n",
      "        policy_loss: -0.0174537505954504\n",
      "        total_loss: 202.7788543701172\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 202.7947540283203\n",
      "    load_time_ms: 5.119\n",
      "    num_steps_sampled: 368000\n",
      "    num_steps_trained: 368000\n",
      "    sample_time_ms: 56916.312\n",
      "    update_time_ms: 50.579\n",
      "  iterations_since_restore: 184\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.65157894736844\n",
      "    ram_util_percent: 67.81263157894736\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.6797745357078\n",
      "    mean_inference_ms: 12.427009037420785\n",
      "    mean_processing_ms: 22.844131772276615\n",
      "  time_since_restore: 13352.010994672775\n",
      "  time_this_iter_s: 66.7769935131073\n",
      "  time_total_s: 13352.010994672775\n",
      "  timestamp: 1575778596\n",
      "  timesteps_since_restore: 368000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 368000\n",
      "  training_iteration: 184\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13352 s, 184 iter, 368000 ts, 49 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-17-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7637029059854\n",
      "  episode_reward_mean: 47.481891687237336\n",
      "  episode_reward_min: -8.085974246032979\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7970.851\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09010162204504013\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0154285430908203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.08079241961240768\n",
      "        policy_loss: 0.0167692918330431\n",
      "        total_loss: 210.8662567138672\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 210.84217834472656\n",
      "    load_time_ms: 5.104\n",
      "    num_steps_sampled: 370000\n",
      "    num_steps_trained: 370000\n",
      "    sample_time_ms: 56830.016\n",
      "    update_time_ms: 50.506\n",
      "  iterations_since_restore: 185\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.5784946236559\n",
      "    ram_util_percent: 67.64623655913978\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.8110027638781\n",
      "    mean_inference_ms: 12.433168175881196\n",
      "    mean_processing_ms: 22.84175175579654\n",
      "  time_since_restore: 13416.997886896133\n",
      "  time_this_iter_s: 64.98689222335815\n",
      "  time_total_s: 13416.997886896133\n",
      "  timestamp: 1575778661\n",
      "  timesteps_since_restore: 370000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 370000\n",
      "  training_iteration: 185\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13416 s, 185 iter, 370000 ts, 47.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-18-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7637029059854\n",
      "  episode_reward_mean: 46.800309312823366\n",
      "  episode_reward_min: -5.950188102766569\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7985.96\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1351524293422699\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7016396522521973\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.11368799954652786\n",
      "        policy_loss: 0.018842609599232674\n",
      "        total_loss: 229.08346557617188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 229.0491943359375\n",
      "    load_time_ms: 5.172\n",
      "    num_steps_sampled: 372000\n",
      "    num_steps_trained: 372000\n",
      "    sample_time_ms: 56817.797\n",
      "    update_time_ms: 50.622\n",
      "  iterations_since_restore: 186\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.78260869565217\n",
      "    ram_util_percent: 67.7108695652174\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.5928896015538\n",
      "    mean_inference_ms: 12.418839255327306\n",
      "    mean_processing_ms: 22.82002730254229\n",
      "  time_since_restore: 13481.06310391426\n",
      "  time_this_iter_s: 64.06521701812744\n",
      "  time_total_s: 13481.06310391426\n",
      "  timestamp: 1575778725\n",
      "  timesteps_since_restore: 372000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 372000\n",
      "  training_iteration: 186\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13481 s, 186 iter, 372000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-19-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7637029059854\n",
      "  episode_reward_mean: 47.91978619738168\n",
      "  episode_reward_min: -7.979805646078547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7985.655\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20272865891456604\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7412967681884766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.4461917579174042\n",
      "        policy_loss: 0.07727133482694626\n",
      "        total_loss: 249.0126495361328\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 248.8449249267578\n",
      "    load_time_ms: 5.169\n",
      "    num_steps_sampled: 374000\n",
      "    num_steps_trained: 374000\n",
      "    sample_time_ms: 56755.853\n",
      "    update_time_ms: 49.634\n",
      "  iterations_since_restore: 187\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.24782608695651\n",
      "    ram_util_percent: 67.7858695652174\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.5180736064713\n",
      "    mean_inference_ms: 12.411763531526365\n",
      "    mean_processing_ms: 22.802508623223492\n",
      "  time_since_restore: 13545.860617399216\n",
      "  time_this_iter_s: 64.79751348495483\n",
      "  time_total_s: 13545.860617399216\n",
      "  timestamp: 1575778790\n",
      "  timesteps_since_restore: 374000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 374000\n",
      "  training_iteration: 187\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13545 s, 187 iter, 374000 ts, 47.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-20-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7637029059854\n",
      "  episode_reward_mean: 44.33515125384724\n",
      "  episode_reward_min: -7.979805646078547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7811.572\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2737298011779785\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01600947603583336\n",
      "        policy_loss: -0.022179020568728447\n",
      "        total_loss: 238.55226135253906\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 238.569580078125\n",
      "    load_time_ms: 5.171\n",
      "    num_steps_sampled: 376000\n",
      "    num_steps_trained: 376000\n",
      "    sample_time_ms: 56717.03\n",
      "    update_time_ms: 50.04\n",
      "  iterations_since_restore: 188\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.37741935483871\n",
      "    ram_util_percent: 67.85483870967742\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.6477979242656\n",
      "    mean_inference_ms: 12.417500521906465\n",
      "    mean_processing_ms: 22.80400255757326\n",
      "  time_since_restore: 13611.289979934692\n",
      "  time_this_iter_s: 65.42936253547668\n",
      "  time_total_s: 13611.289979934692\n",
      "  timestamp: 1575778856\n",
      "  timesteps_since_restore: 376000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 376000\n",
      "  training_iteration: 188\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13611 s, 188 iter, 376000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-22-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.7637029059854\n",
      "  episode_reward_mean: 43.48623375042709\n",
      "  episode_reward_min: -7.979805646078547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7711.981\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.256340742111206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024868013337254524\n",
      "        policy_loss: -0.011533820070326328\n",
      "        total_loss: 216.92843627929688\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 216.9323272705078\n",
      "    load_time_ms: 3.802\n",
      "    num_steps_sampled: 378000\n",
      "    num_steps_trained: 378000\n",
      "    sample_time_ms: 56932.403\n",
      "    update_time_ms: 50.577\n",
      "  iterations_since_restore: 189\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.18602150537633\n",
      "    ram_util_percent: 67.90860215053762\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.55752958574524\n",
      "    mean_inference_ms: 12.408940859013764\n",
      "    mean_processing_ms: 22.793437484646983\n",
      "  time_since_restore: 13675.996799945831\n",
      "  time_this_iter_s: 64.70682001113892\n",
      "  time_total_s: 13675.996799945831\n",
      "  timestamp: 1575778920\n",
      "  timesteps_since_restore: 378000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 378000\n",
      "  training_iteration: 189\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13675 s, 189 iter, 378000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-23-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.91556283507018\n",
      "  episode_reward_mean: 43.209635440993225\n",
      "  episode_reward_min: -7.979805646078547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7914.188\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.67659068107605\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015429213643074036\n",
      "        policy_loss: -0.01084219478070736\n",
      "        total_loss: 219.1752471923828\n",
      "        vf_explained_var: 2.980232238769531e-07\n",
      "        vf_loss: 219.18138122558594\n",
      "    load_time_ms: 3.625\n",
      "    num_steps_sampled: 380000\n",
      "    num_steps_trained: 380000\n",
      "    sample_time_ms: 56846.62\n",
      "    update_time_ms: 51.774\n",
      "  iterations_since_restore: 190\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.98404255319149\n",
      "    ram_util_percent: 67.9872340425532\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.54059064920315\n",
      "    mean_inference_ms: 12.407190701972112\n",
      "    mean_processing_ms: 22.78540265525973\n",
      "  time_since_restore: 13741.93858408928\n",
      "  time_this_iter_s: 65.94178414344788\n",
      "  time_total_s: 13741.93858408928\n",
      "  timestamp: 1575778986\n",
      "  timesteps_since_restore: 380000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 380000\n",
      "  training_iteration: 190\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13741 s, 190 iter, 380000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-24-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.91556283507018\n",
      "  episode_reward_mean: 44.464631863154096\n",
      "  episode_reward_min: -7.979805646078547\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8310.073\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30409297347068787\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5716030597686768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004327753093093634\n",
      "        policy_loss: -0.010600101202726364\n",
      "        total_loss: 227.58192443847656\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 227.5911865234375\n",
      "    load_time_ms: 3.956\n",
      "    num_steps_sampled: 382000\n",
      "    num_steps_trained: 382000\n",
      "    sample_time_ms: 56708.415\n",
      "    update_time_ms: 53.134\n",
      "  iterations_since_restore: 191\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.31914893617021\n",
      "    ram_util_percent: 68.11276595744681\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.01364794899752\n",
      "    mean_inference_ms: 12.436496083209956\n",
      "    mean_processing_ms: 22.82192657436552\n",
      "  time_since_restore: 13807.871928691864\n",
      "  time_this_iter_s: 65.93334460258484\n",
      "  time_total_s: 13807.871928691864\n",
      "  timestamp: 1575779052\n",
      "  timesteps_since_restore: 382000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 382000\n",
      "  training_iteration: 191\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13807 s, 191 iter, 382000 ts, 44.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-25-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.48194364864044\n",
      "  episode_reward_mean: 43.29776950828773\n",
      "  episode_reward_min: -2.457252508827485\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8337.811\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15204648673534393\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7270400524139404\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008010199293494225\n",
      "        policy_loss: -0.011570531874895096\n",
      "        total_loss: 211.1763153076172\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 211.18673706054688\n",
      "    load_time_ms: 3.909\n",
      "    num_steps_sampled: 384000\n",
      "    num_steps_trained: 384000\n",
      "    sample_time_ms: 56663.577\n",
      "    update_time_ms: 52.712\n",
      "  iterations_since_restore: 192\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.79120879120879\n",
      "    ram_util_percent: 67.91208791208791\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.00697839015646\n",
      "    mean_inference_ms: 12.430434572233263\n",
      "    mean_processing_ms: 22.820460797470442\n",
      "  time_since_restore: 13871.80001115799\n",
      "  time_this_iter_s: 63.92808246612549\n",
      "  time_total_s: 13871.80001115799\n",
      "  timestamp: 1575779116\n",
      "  timesteps_since_restore: 384000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 384000\n",
      "  training_iteration: 192\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13871 s, 192 iter, 384000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-26-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.8989538241418\n",
      "  episode_reward_mean: 46.37786957767663\n",
      "  episode_reward_min: -3.5754264207974895\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8317.702\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.217522382736206\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01480928249657154\n",
      "        policy_loss: -0.01886199228465557\n",
      "        total_loss: 232.8157501220703\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 232.83349609375\n",
      "    load_time_ms: 3.607\n",
      "    num_steps_sampled: 386000\n",
      "    num_steps_trained: 386000\n",
      "    sample_time_ms: 56668.283\n",
      "    update_time_ms: 52.083\n",
      "  iterations_since_restore: 193\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.51847826086957\n",
      "    ram_util_percent: 67.98804347826088\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.99243542923384\n",
      "    mean_inference_ms: 12.423754989866348\n",
      "    mean_processing_ms: 22.816410060860324\n",
      "  time_since_restore: 13936.126330375671\n",
      "  time_this_iter_s: 64.32631921768188\n",
      "  time_total_s: 13936.126330375671\n",
      "  timestamp: 1575779181\n",
      "  timesteps_since_restore: 386000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 386000\n",
      "  training_iteration: 193\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 13936 s, 193 iter, 386000 ts, 46.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-27-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.89181988283765\n",
      "  episode_reward_mean: 48.61453461106258\n",
      "  episode_reward_min: -6.128501601728365\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8103.739\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.563606023788452\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013005062006413937\n",
      "        policy_loss: -0.017237015068531036\n",
      "        total_loss: 251.15078735351562\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 251.1670684814453\n",
      "    load_time_ms: 3.745\n",
      "    num_steps_sampled: 388000\n",
      "    num_steps_trained: 388000\n",
      "    sample_time_ms: 56650.929\n",
      "    update_time_ms: 53.677\n",
      "  iterations_since_restore: 194\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.2163043478261\n",
      "    ram_util_percent: 68.06086956521737\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 148.05524508129315\n",
      "    mean_inference_ms: 12.424257259493142\n",
      "    mean_processing_ms: 22.818275976400834\n",
      "  time_since_restore: 14000.586294174194\n",
      "  time_this_iter_s: 64.45996379852295\n",
      "  time_total_s: 14000.586294174194\n",
      "  timestamp: 1575779245\n",
      "  timesteps_since_restore: 388000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 388000\n",
      "  training_iteration: 194\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14000 s, 194 iter, 388000 ts, 48.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-28-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.89181988283765\n",
      "  episode_reward_mean: 49.91019207591811\n",
      "  episode_reward_min: -6.128501601728365\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7904.92\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2110326290130615\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02695198729634285\n",
      "        policy_loss: -0.014244851656258106\n",
      "        total_loss: 225.69821166992188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 225.71034240722656\n",
      "    load_time_ms: 3.73\n",
      "    num_steps_sampled: 390000\n",
      "    num_steps_trained: 390000\n",
      "    sample_time_ms: 56901.984\n",
      "    update_time_ms: 54.243\n",
      "  iterations_since_restore: 195\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.77634408602151\n",
      "    ram_util_percent: 68.1279569892473\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.54802669049164\n",
      "    mean_inference_ms: 12.390281709705349\n",
      "    mean_processing_ms: 22.789203753683235\n",
      "  time_since_restore: 14066.084751844406\n",
      "  time_this_iter_s: 65.49845767021179\n",
      "  time_total_s: 14066.084751844406\n",
      "  timestamp: 1575779311\n",
      "  timesteps_since_restore: 390000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 390000\n",
      "  training_iteration: 195\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14066 s, 195 iter, 390000 ts, 49.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-29-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.89181988283765\n",
      "  episode_reward_mean: 47.39077182472653\n",
      "  episode_reward_min: -13.441874134752362\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8109.566\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1148884296417236\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02010957896709442\n",
      "        policy_loss: -0.020120682194828987\n",
      "        total_loss: 245.7838134765625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 245.80233764648438\n",
      "    load_time_ms: 3.651\n",
      "    num_steps_sampled: 392000\n",
      "    num_steps_trained: 392000\n",
      "    sample_time_ms: 56962.473\n",
      "    update_time_ms: 53.926\n",
      "  iterations_since_restore: 196\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.22916666666667\n",
      "    ram_util_percent: 68.18958333333332\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.18386689388836\n",
      "    mean_inference_ms: 12.364974292901811\n",
      "    mean_processing_ms: 22.744715584556058\n",
      "  time_since_restore: 14132.754074573517\n",
      "  time_this_iter_s: 66.66932272911072\n",
      "  time_total_s: 14132.754074573517\n",
      "  timestamp: 1575779377\n",
      "  timesteps_since_restore: 392000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 392000\n",
      "  training_iteration: 196\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14132 s, 196 iter, 392000 ts, 47.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-30-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.89181988283765\n",
      "  episode_reward_mean: 44.96874088293806\n",
      "  episode_reward_min: -16.280795548349836\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8343.726\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.920869827270508\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03836550563573837\n",
      "        policy_loss: -0.010463060811161995\n",
      "        total_loss: 251.0907745361328\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 251.09828186035156\n",
      "    load_time_ms: 3.669\n",
      "    num_steps_sampled: 394000\n",
      "    num_steps_trained: 394000\n",
      "    sample_time_ms: 56751.873\n",
      "    update_time_ms: 55.465\n",
      "  iterations_since_restore: 197\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.31195652173912\n",
      "    ram_util_percent: 68.31195652173913\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.10803776698728\n",
      "    mean_inference_ms: 12.358382243591805\n",
      "    mean_processing_ms: 22.74461192723751\n",
      "  time_since_restore: 14197.79099225998\n",
      "  time_this_iter_s: 65.0369176864624\n",
      "  time_total_s: 14197.79099225998\n",
      "  timestamp: 1575779442\n",
      "  timesteps_since_restore: 394000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 394000\n",
      "  training_iteration: 197\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14197 s, 197 iter, 394000 ts, 45 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-31-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.89181988283765\n",
      "  episode_reward_mean: 44.554134207741\n",
      "  episode_reward_min: -16.280795548349836\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8353.622\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.855559825897217\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015013334341347218\n",
      "        policy_loss: -0.013891772367060184\n",
      "        total_loss: 237.53488159179688\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 237.54759216308594\n",
      "    load_time_ms: 3.649\n",
      "    num_steps_sampled: 396000\n",
      "    num_steps_trained: 396000\n",
      "    sample_time_ms: 56707.401\n",
      "    update_time_ms: 54.103\n",
      "  iterations_since_restore: 198\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.83440860215052\n",
      "    ram_util_percent: 68.19462365591399\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.06056464159659\n",
      "    mean_inference_ms: 12.358730785769339\n",
      "    mean_processing_ms: 22.748188692460907\n",
      "  time_since_restore: 14262.875818252563\n",
      "  time_this_iter_s: 65.08482599258423\n",
      "  time_total_s: 14262.875818252563\n",
      "  timestamp: 1575779508\n",
      "  timesteps_since_restore: 396000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 396000\n",
      "  training_iteration: 198\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14262 s, 198 iter, 396000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-32-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.28929067125735\n",
      "  episode_reward_mean: 41.50839235826669\n",
      "  episode_reward_min: -19.053618438156576\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 3980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8352.523\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07602324336767197\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5009992122650146\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07338777929544449\n",
      "        policy_loss: 0.02130422368645668\n",
      "        total_loss: 239.533935546875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 239.50709533691406\n",
      "    load_time_ms: 3.379\n",
      "    num_steps_sampled: 398000\n",
      "    num_steps_trained: 398000\n",
      "    sample_time_ms: 56631.406\n",
      "    update_time_ms: 53.879\n",
      "  iterations_since_restore: 199\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.77608695652174\n",
      "    ram_util_percent: 68.24891304347828\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.12013173107582\n",
      "    mean_inference_ms: 12.363828471841435\n",
      "    mean_processing_ms: 22.743166205148803\n",
      "  time_since_restore: 14326.808581829071\n",
      "  time_this_iter_s: 63.93276357650757\n",
      "  time_total_s: 14326.808581829071\n",
      "  timestamp: 1575779572\n",
      "  timesteps_since_restore: 398000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 398000\n",
      "  training_iteration: 199\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14326 s, 199 iter, 398000 ts, 41.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-33-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.06944610016905\n",
      "  episode_reward_mean: 38.473811785745426\n",
      "  episode_reward_min: -19.053618438156576\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8135.201\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11403486877679825\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0611209869384766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.12594565749168396\n",
      "        policy_loss: 0.039527252316474915\n",
      "        total_loss: 226.2284698486328\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 226.17462158203125\n",
      "    load_time_ms: 3.613\n",
      "    num_steps_sampled: 400000\n",
      "    num_steps_trained: 400000\n",
      "    sample_time_ms: 56713.622\n",
      "    update_time_ms: 53.526\n",
      "  iterations_since_restore: 200\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.27934782608696\n",
      "    ram_util_percent: 68.29130434782611\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.31832810573243\n",
      "    mean_inference_ms: 12.37611126238818\n",
      "    mean_processing_ms: 22.73943185693522\n",
      "  time_since_restore: 14391.402964353561\n",
      "  time_this_iter_s: 64.59438252449036\n",
      "  time_total_s: 14391.402964353561\n",
      "  timestamp: 1575779636\n",
      "  timesteps_since_restore: 400000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 400000\n",
      "  training_iteration: 200\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14391 s, 200 iter, 400000 ts, 38.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-35-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.06944610016905\n",
      "  episode_reward_mean: 37.826389436633775\n",
      "  episode_reward_min: -21.053593165463614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7742.522\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17105230689048767\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.665942907333374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01573330909013748\n",
      "        policy_loss: -0.021442417055368423\n",
      "        total_loss: 229.3992462158203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 229.41799926757812\n",
      "    load_time_ms: 3.121\n",
      "    num_steps_sampled: 402000\n",
      "    num_steps_trained: 402000\n",
      "    sample_time_ms: 56919.889\n",
      "    update_time_ms: 52.795\n",
      "  iterations_since_restore: 201\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.85934065934067\n",
      "    ram_util_percent: 68.3934065934066\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.9738949394722\n",
      "    mean_inference_ms: 12.351410889670598\n",
      "    mean_processing_ms: 22.718457444602087\n",
      "  time_since_restore: 14455.462746620178\n",
      "  time_this_iter_s: 64.05978226661682\n",
      "  time_total_s: 14455.462746620178\n",
      "  timestamp: 1575779700\n",
      "  timesteps_since_restore: 402000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 402000\n",
      "  training_iteration: 201\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14455 s, 201 iter, 402000 ts, 37.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-36-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.06944610016905\n",
      "  episode_reward_mean: 38.50357315904453\n",
      "  episode_reward_min: -21.053593165463614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7946.149\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17105230689048767\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.075460433959961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.3947637379169464\n",
      "        policy_loss: 0.06883042305707932\n",
      "        total_loss: 207.85797119140625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 207.7216033935547\n",
      "    load_time_ms: 3.204\n",
      "    num_steps_sampled: 404000\n",
      "    num_steps_trained: 404000\n",
      "    sample_time_ms: 56982.563\n",
      "    update_time_ms: 52.93\n",
      "  iterations_since_restore: 202\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.23473684210525\n",
      "    ram_util_percent: 68.46\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.55142007441498\n",
      "    mean_inference_ms: 12.32530581287695\n",
      "    mean_processing_ms: 22.67953582363555\n",
      "  time_since_restore: 14522.053450345993\n",
      "  time_this_iter_s: 66.59070372581482\n",
      "  time_total_s: 14522.053450345993\n",
      "  timestamp: 1575779767\n",
      "  timesteps_since_restore: 404000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 404000\n",
      "  training_iteration: 202\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14522 s, 202 iter, 404000 ts, 38.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-37-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.17057752431741\n",
      "  episode_reward_mean: 37.63008027806678\n",
      "  episode_reward_min: -21.053593165463614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8170.025\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2565784454345703\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4400157928466797\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020350197330117226\n",
      "        policy_loss: -0.015341105870902538\n",
      "        total_loss: 237.52581787109375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 237.53594970703125\n",
      "    load_time_ms: 3.167\n",
      "    num_steps_sampled: 406000\n",
      "    num_steps_trained: 406000\n",
      "    sample_time_ms: 56805.69\n",
      "    update_time_ms: 53.44\n",
      "  iterations_since_restore: 203\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.40322580645162\n",
      "    ram_util_percent: 68.53655913978496\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.3521343890857\n",
      "    mean_inference_ms: 12.373083674745534\n",
      "    mean_processing_ms: 22.700011131736886\n",
      "  time_since_restore: 14586.849880218506\n",
      "  time_this_iter_s: 64.79642987251282\n",
      "  time_total_s: 14586.849880218506\n",
      "  timestamp: 1575779832\n",
      "  timesteps_since_restore: 406000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 406000\n",
      "  training_iteration: 203\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14586 s, 203 iter, 406000 ts, 37.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-38-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.17057752431741\n",
      "  episode_reward_mean: 39.09716025303535\n",
      "  episode_reward_min: -21.053593165463614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8184.901\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2565784454345703\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.472660779953003\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02175244688987732\n",
      "        policy_loss: -0.028064770624041557\n",
      "        total_loss: 223.66506958007812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 223.6875457763672\n",
      "    load_time_ms: 3.063\n",
      "    num_steps_sampled: 408000\n",
      "    num_steps_trained: 408000\n",
      "    sample_time_ms: 56905.228\n",
      "    update_time_ms: 53.173\n",
      "  iterations_since_restore: 204\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.28279569892473\n",
      "    ram_util_percent: 68.58709677419357\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.35682716242206\n",
      "    mean_inference_ms: 12.368553611558054\n",
      "    mean_processing_ms: 22.695225403108367\n",
      "  time_since_restore: 14652.451776504517\n",
      "  time_this_iter_s: 65.60189628601074\n",
      "  time_total_s: 14652.451776504517\n",
      "  timestamp: 1575779897\n",
      "  timesteps_since_restore: 408000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 408000\n",
      "  training_iteration: 204\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14652 s, 204 iter, 408000 ts, 39.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-39-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.17057752431741\n",
      "  episode_reward_mean: 36.232962571298046\n",
      "  episode_reward_min: -21.053593165463614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8187.032\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2565784454345703\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4413111209869385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013633928261697292\n",
      "        policy_loss: -0.015193285420536995\n",
      "        total_loss: 239.10031127929688\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 239.11190795898438\n",
      "    load_time_ms: 3.13\n",
      "    num_steps_sampled: 410000\n",
      "    num_steps_trained: 410000\n",
      "    sample_time_ms: 56830.23\n",
      "    update_time_ms: 52.69\n",
      "  iterations_since_restore: 205\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.17956989247313\n",
      "    ram_util_percent: 68.43225806451613\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.01635675119968\n",
      "    mean_inference_ms: 12.344224185609459\n",
      "    mean_processing_ms: 22.672965692076076\n",
      "  time_since_restore: 14717.21986413002\n",
      "  time_this_iter_s: 64.76808762550354\n",
      "  time_total_s: 14717.21986413002\n",
      "  timestamp: 1575779962\n",
      "  timesteps_since_restore: 410000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 410000\n",
      "  training_iteration: 205\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14717 s, 205 iter, 410000 ts, 36.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-40-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.17057752431741\n",
      "  episode_reward_mean: 37.76629090588327\n",
      "  episode_reward_min: -18.191197753246293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7968.004\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2565784454345703\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.287808656692505\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019235679879784584\n",
      "        policy_loss: -0.011293977499008179\n",
      "        total_loss: 214.07850646972656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 214.08482360839844\n",
      "    load_time_ms: 3.227\n",
      "    num_steps_sampled: 412000\n",
      "    num_steps_trained: 412000\n",
      "    sample_time_ms: 56863.511\n",
      "    update_time_ms: 51.804\n",
      "  iterations_since_restore: 206\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.32608695652173\n",
      "    ram_util_percent: 68.50326086956521\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.11673977639552\n",
      "    mean_inference_ms: 12.348404300548596\n",
      "    mean_processing_ms: 22.67162523955734\n",
      "  time_since_restore: 14782.031861543655\n",
      "  time_this_iter_s: 64.81199741363525\n",
      "  time_total_s: 14782.031861543655\n",
      "  timestamp: 1575780027\n",
      "  timesteps_since_restore: 412000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 412000\n",
      "  training_iteration: 206\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14782 s, 206 iter, 412000 ts, 37.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-41-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.17057752431741\n",
      "  episode_reward_mean: 37.18813870315371\n",
      "  episode_reward_min: -18.191197753246293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7729.205\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2565784454345703\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5201127529144287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04455028474330902\n",
      "        policy_loss: -0.011506226845085621\n",
      "        total_loss: 208.16818237304688\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 208.168212890625\n",
      "    load_time_ms: 3.265\n",
      "    num_steps_sampled: 414000\n",
      "    num_steps_trained: 414000\n",
      "    sample_time_ms: 57090.559\n",
      "    update_time_ms: 51.969\n",
      "  iterations_since_restore: 207\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.60215053763442\n",
      "    ram_util_percent: 68.54408602150538\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.15676954256045\n",
      "    mean_inference_ms: 12.35187443716525\n",
      "    mean_processing_ms: 22.663019496307854\n",
      "  time_since_restore: 14846.954662322998\n",
      "  time_this_iter_s: 64.92280077934265\n",
      "  time_total_s: 14846.954662322998\n",
      "  timestamp: 1575780092\n",
      "  timesteps_since_restore: 414000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 414000\n",
      "  training_iteration: 207\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14846 s, 207 iter, 414000 ts, 37.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-42-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.7239343140801\n",
      "  episode_reward_mean: 37.44334271434735\n",
      "  episode_reward_min: -18.191197753246293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7928.669\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.38486766815185547\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5985090732574463\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010233307257294655\n",
      "        policy_loss: -0.012931308709084988\n",
      "        total_loss: 212.24710083007812\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 212.25611877441406\n",
      "    load_time_ms: 3.312\n",
      "    num_steps_sampled: 416000\n",
      "    num_steps_trained: 416000\n",
      "    sample_time_ms: 57087.1\n",
      "    update_time_ms: 53.029\n",
      "  iterations_since_restore: 208\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.453125\n",
      "    ram_util_percent: 68.63541666666666\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.15226409218502\n",
      "    mean_inference_ms: 12.286786143340134\n",
      "    mean_processing_ms: 22.60030470276205\n",
      "  time_since_restore: 14913.997921228409\n",
      "  time_this_iter_s: 67.04325890541077\n",
      "  time_total_s: 14913.997921228409\n",
      "  timestamp: 1575780159\n",
      "  timesteps_since_restore: 416000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 416000\n",
      "  training_iteration: 208\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14913 s, 208 iter, 416000 ts, 37.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.15985268303344\n",
      "  episode_reward_mean: 37.87849125569745\n",
      "  episode_reward_min: -18.191197753246293\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7917.941\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.38486766815185547\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.698009729385376\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02872544154524803\n",
      "        policy_loss: 0.004036605823785067\n",
      "        total_loss: 206.20046997070312\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 206.1854705810547\n",
      "    load_time_ms: 3.316\n",
      "    num_steps_sampled: 418000\n",
      "    num_steps_trained: 418000\n",
      "    sample_time_ms: 57321.851\n",
      "    update_time_ms: 54.34\n",
      "  iterations_since_restore: 209\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.93936170212766\n",
      "    ram_util_percent: 68.72553191489361\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.74525173694653\n",
      "    mean_inference_ms: 12.261637415562031\n",
      "    mean_processing_ms: 22.56201653262985\n",
      "  time_since_restore: 14980.18041586876\n",
      "  time_this_iter_s: 66.18249464035034\n",
      "  time_total_s: 14980.18041586876\n",
      "  timestamp: 1575780225\n",
      "  timesteps_since_restore: 418000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 418000\n",
      "  training_iteration: 209\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 14980 s, 209 iter, 418000 ts, 37.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-44-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.15985268303344\n",
      "  episode_reward_mean: 44.13342190457817\n",
      "  episode_reward_min: -1.0024641080800727\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7918.0\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.38486766815185547\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.42752206325531\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.039514489471912384\n",
      "        policy_loss: -0.004159348551183939\n",
      "        total_loss: 213.8542938232422\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 213.8431854248047\n",
      "    load_time_ms: 3.152\n",
      "    num_steps_sampled: 420000\n",
      "    num_steps_trained: 420000\n",
      "    sample_time_ms: 57402.048\n",
      "    update_time_ms: 55.455\n",
      "  iterations_since_restore: 210\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.13225806451612\n",
      "    ram_util_percent: 68.8021505376344\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.04606043930377\n",
      "    mean_inference_ms: 12.27748474909569\n",
      "    mean_processing_ms: 22.576206615085063\n",
      "  time_since_restore: 15045.596310138702\n",
      "  time_this_iter_s: 65.41589426994324\n",
      "  time_total_s: 15045.596310138702\n",
      "  timestamp: 1575780291\n",
      "  timesteps_since_restore: 420000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 420000\n",
      "  training_iteration: 210\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15045 s, 210 iter, 420000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-45-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.43325478767382\n",
      "  episode_reward_mean: 46.196632922742126\n",
      "  episode_reward_min: -8.27227864586375\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7926.208\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.38486766815185547\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3444252014160156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01074903178960085\n",
      "        policy_loss: -0.013806428760290146\n",
      "        total_loss: 218.1217041015625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.13140869140625\n",
      "    load_time_ms: 3.12\n",
      "    num_steps_sampled: 422000\n",
      "    num_steps_trained: 422000\n",
      "    sample_time_ms: 57451.385\n",
      "    update_time_ms: 54.736\n",
      "  iterations_since_restore: 211\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82688172043012\n",
      "    ram_util_percent: 68.8193548387097\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.19593308517958\n",
      "    mean_inference_ms: 12.288922371235863\n",
      "    mean_processing_ms: 22.58612223030777\n",
      "  time_since_restore: 15110.218047618866\n",
      "  time_this_iter_s: 64.62173748016357\n",
      "  time_total_s: 15110.218047618866\n",
      "  timestamp: 1575780355\n",
      "  timesteps_since_restore: 422000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 422000\n",
      "  training_iteration: 211\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15110 s, 211 iter, 422000 ts, 46.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-47-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.43325478767382\n",
      "  episode_reward_mean: 47.55530021378426\n",
      "  episode_reward_min: -18.080685818252583\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7700.475\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.38486766815185547\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8284008502960205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007813802920281887\n",
      "        policy_loss: -0.011521827429533005\n",
      "        total_loss: 254.4340362548828\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 254.44259643554688\n",
      "    load_time_ms: 3.15\n",
      "    num_steps_sampled: 424000\n",
      "    num_steps_trained: 424000\n",
      "    sample_time_ms: 57472.842\n",
      "    update_time_ms: 55.152\n",
      "  iterations_since_restore: 212\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.58260869565218\n",
      "    ram_util_percent: 68.73152173913046\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.40167721281344\n",
      "    mean_inference_ms: 12.300634870220666\n",
      "    mean_processing_ms: 22.605800363018233\n",
      "  time_since_restore: 15174.772104501724\n",
      "  time_this_iter_s: 64.55405688285828\n",
      "  time_total_s: 15174.772104501724\n",
      "  timestamp: 1575780420\n",
      "  timesteps_since_restore: 424000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 424000\n",
      "  training_iteration: 212\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15174 s, 212 iter, 424000 ts, 47.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-48-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.43325478767382\n",
      "  episode_reward_mean: 50.36431231984472\n",
      "  episode_reward_min: -18.080685818252583\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7692.618\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19243383407592773\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8965847492218018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013019832782447338\n",
      "        policy_loss: -0.013585617765784264\n",
      "        total_loss: 219.5837860107422\n",
      "        vf_explained_var: 2.384185791015625e-07\n",
      "        vf_loss: 219.59494018554688\n",
      "    load_time_ms: 3.139\n",
      "    num_steps_sampled: 426000\n",
      "    num_steps_trained: 426000\n",
      "    sample_time_ms: 57756.302\n",
      "    update_time_ms: 54.173\n",
      "  iterations_since_restore: 213\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.73020833333334\n",
      "    ram_util_percent: 68.809375\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.15649147511218\n",
      "    mean_inference_ms: 12.2844726069429\n",
      "    mean_processing_ms: 22.58492626378528\n",
      "  time_since_restore: 15242.314811706543\n",
      "  time_this_iter_s: 67.54270720481873\n",
      "  time_total_s: 15242.314811706543\n",
      "  timestamp: 1575780488\n",
      "  timesteps_since_restore: 426000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 426000\n",
      "  training_iteration: 213\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15242 s, 213 iter, 426000 ts, 50.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.43325478767382\n",
      "  episode_reward_mean: 49.08588378347772\n",
      "  episode_reward_min: -18.080685818252583\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7912.007\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19243383407592773\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1110644340515137\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007566991727799177\n",
      "        policy_loss: -0.013628872111439705\n",
      "        total_loss: 248.85874938964844\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 248.8708953857422\n",
      "    load_time_ms: 3.123\n",
      "    num_steps_sampled: 428000\n",
      "    num_steps_trained: 428000\n",
      "    sample_time_ms: 57439.712\n",
      "    update_time_ms: 53.334\n",
      "  iterations_since_restore: 214\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90434782608696\n",
      "    ram_util_percent: 68.88695652173914\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.6219227709461\n",
      "    mean_inference_ms: 12.31282446805805\n",
      "    mean_processing_ms: 22.592541551852833\n",
      "  time_since_restore: 15306.924887657166\n",
      "  time_this_iter_s: 64.61007595062256\n",
      "  time_total_s: 15306.924887657166\n",
      "  timestamp: 1575780552\n",
      "  timesteps_since_restore: 428000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 428000\n",
      "  training_iteration: 214\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15306 s, 214 iter, 428000 ts, 49.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-50-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.43325478767382\n",
      "  episode_reward_mean: 45.01502155951189\n",
      "  episode_reward_min: -32.01128872380667\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7918.326\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09621691703796387\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0844194889068604\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006269602105021477\n",
      "        policy_loss: -0.0092981718480587\n",
      "        total_loss: 263.1348571777344\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 263.1435546875\n",
      "    load_time_ms: 3.17\n",
      "    num_steps_sampled: 430000\n",
      "    num_steps_trained: 430000\n",
      "    sample_time_ms: 57403.701\n",
      "    update_time_ms: 57.949\n",
      "  iterations_since_restore: 215\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.80217391304348\n",
      "    ram_util_percent: 68.96739130434784\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.89688938006418\n",
      "    mean_inference_ms: 12.330275096362993\n",
      "    mean_processing_ms: 22.585478217716364\n",
      "  time_since_restore: 15371.439894676208\n",
      "  time_this_iter_s: 64.51500701904297\n",
      "  time_total_s: 15371.439894676208\n",
      "  timestamp: 1575780617\n",
      "  timesteps_since_restore: 430000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 430000\n",
      "  training_iteration: 215\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15371 s, 215 iter, 430000 ts, 45 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-51-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.61187689725168\n",
      "  episode_reward_mean: 43.82094262877606\n",
      "  episode_reward_min: -32.01128872380667\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7918.757\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5730607509613037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01634208671748638\n",
      "        policy_loss: -0.01874377764761448\n",
      "        total_loss: 255.0211944580078\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 255.0391387939453\n",
      "    load_time_ms: 3.07\n",
      "    num_steps_sampled: 432000\n",
      "    num_steps_trained: 432000\n",
      "    sample_time_ms: 57361.835\n",
      "    update_time_ms: 59.399\n",
      "  iterations_since_restore: 216\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82826086956523\n",
      "    ram_util_percent: 69.05217391304348\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.62826887794904\n",
      "    mean_inference_ms: 12.308675646865419\n",
      "    mean_processing_ms: 22.55804355379968\n",
      "  time_since_restore: 15435.84872841835\n",
      "  time_this_iter_s: 64.40883374214172\n",
      "  time_total_s: 15435.84872841835\n",
      "  timestamp: 1575780681\n",
      "  timesteps_since_restore: 432000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 432000\n",
      "  training_iteration: 216\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15435 s, 216 iter, 432000 ts, 43.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-52-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.12305472019467\n",
      "  episode_reward_mean: 44.52470280016893\n",
      "  episode_reward_min: -32.01128872380667\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7912.003\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0273702144622803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015657782554626465\n",
      "        policy_loss: -0.016163092106580734\n",
      "        total_loss: 227.61773681640625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 227.63308715820312\n",
      "    load_time_ms: 3.06\n",
      "    num_steps_sampled: 434000\n",
      "    num_steps_trained: 434000\n",
      "    sample_time_ms: 57227.836\n",
      "    update_time_ms: 58.868\n",
      "  iterations_since_restore: 217\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.84615384615384\n",
      "    ram_util_percent: 69.07142857142857\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.4891349253704\n",
      "    mean_inference_ms: 12.298045621007656\n",
      "    mean_processing_ms: 22.533590607307705\n",
      "  time_since_restore: 15499.364392280579\n",
      "  time_this_iter_s: 63.515663862228394\n",
      "  time_total_s: 15499.364392280579\n",
      "  timestamp: 1575780745\n",
      "  timesteps_since_restore: 434000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 434000\n",
      "  training_iteration: 217\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15499 s, 217 iter, 434000 ts, 44.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-53-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.91374831490515\n",
      "  episode_reward_mean: 38.808432307458496\n",
      "  episode_reward_min: -33.422340641056635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7694.165\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.708512783050537\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025668563321232796\n",
      "        policy_loss: -0.02586292289197445\n",
      "        total_loss: 270.4659423828125\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 270.490478515625\n",
      "    load_time_ms: 3.004\n",
      "    num_steps_sampled: 436000\n",
      "    num_steps_trained: 436000\n",
      "    sample_time_ms: 57337.964\n",
      "    update_time_ms: 58.219\n",
      "  iterations_since_restore: 218\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.4531914893617\n",
      "    ram_util_percent: 69.07446808510639\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.60566985350223\n",
      "    mean_inference_ms: 12.304294878677165\n",
      "    mean_processing_ms: 22.539867757032475\n",
      "  time_since_restore: 15565.320500135422\n",
      "  time_this_iter_s: 65.95610785484314\n",
      "  time_total_s: 15565.320500135422\n",
      "  timestamp: 1575780811\n",
      "  timesteps_since_restore: 436000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 436000\n",
      "  training_iteration: 218\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15565 s, 218 iter, 436000 ts, 38.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-54-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.91374831490515\n",
      "  episode_reward_mean: 38.72516351272713\n",
      "  episode_reward_min: -33.422340641056635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8071.173\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.691445827484131\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01923494040966034\n",
      "        policy_loss: -0.016694355756044388\n",
      "        total_loss: 219.09085083007812\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 219.1065673828125\n",
      "    load_time_ms: 3.042\n",
      "    num_steps_sampled: 438000\n",
      "    num_steps_trained: 438000\n",
      "    sample_time_ms: 57263.793\n",
      "    update_time_ms: 57.905\n",
      "  iterations_since_restore: 219\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.15656565656566\n",
      "    ram_util_percent: 69.05252525252526\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.26757533662195\n",
      "    mean_inference_ms: 12.347025529689956\n",
      "    mean_processing_ms: 22.584963640135456\n",
      "  time_since_restore: 15634.532151937485\n",
      "  time_this_iter_s: 69.21165180206299\n",
      "  time_total_s: 15634.532151937485\n",
      "  timestamp: 1575780880\n",
      "  timesteps_since_restore: 438000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 438000\n",
      "  training_iteration: 219\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15634 s, 219 iter, 438000 ts, 38.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-55-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.17886623485862\n",
      "  episode_reward_mean: 40.0535581014764\n",
      "  episode_reward_min: -33.422340641056635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8068.72\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6680941581726074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01582356169819832\n",
      "        policy_loss: -0.016071463003754616\n",
      "        total_loss: 226.33045959472656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 226.3457794189453\n",
      "    load_time_ms: 2.982\n",
      "    num_steps_sampled: 440000\n",
      "    num_steps_trained: 440000\n",
      "    sample_time_ms: 57255.509\n",
      "    update_time_ms: 57.504\n",
      "  iterations_since_restore: 220\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.19032258064517\n",
      "    ram_util_percent: 69.12043010752689\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.88788822923232\n",
      "    mean_inference_ms: 12.32152208418085\n",
      "    mean_processing_ms: 22.57333321252997\n",
      "  time_since_restore: 15699.822139263153\n",
      "  time_this_iter_s: 65.28998732566833\n",
      "  time_total_s: 15699.822139263153\n",
      "  timestamp: 1575780945\n",
      "  timesteps_since_restore: 440000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 440000\n",
      "  training_iteration: 220\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15699 s, 220 iter, 440000 ts, 40.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-56-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.86353592192421\n",
      "  episode_reward_mean: 38.99851118379077\n",
      "  episode_reward_min: -33.422340641056635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8045.942\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9477357864379883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010452992282807827\n",
      "        policy_loss: -0.008888431824743748\n",
      "        total_loss: 191.7918701171875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 191.80029296875\n",
      "    load_time_ms: 3.043\n",
      "    num_steps_sampled: 442000\n",
      "    num_steps_trained: 442000\n",
      "    sample_time_ms: 57345.112\n",
      "    update_time_ms: 59.004\n",
      "  iterations_since_restore: 221\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.34946236559139\n",
      "    ram_util_percent: 69.19032258064516\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.24625657557647\n",
      "    mean_inference_ms: 12.34164158776171\n",
      "    mean_processing_ms: 22.580182244271867\n",
      "  time_since_restore: 15765.128700494766\n",
      "  time_this_iter_s: 65.30656123161316\n",
      "  time_total_s: 15765.128700494766\n",
      "  timestamp: 1575781011\n",
      "  timesteps_since_restore: 442000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 442000\n",
      "  training_iteration: 221\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15765 s, 221 iter, 442000 ts, 39 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-57-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.86353592192421\n",
      "  episode_reward_mean: 38.46516873621415\n",
      "  episode_reward_min: -33.422340641056635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8037.471\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0221800804138184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015364084392786026\n",
      "        policy_loss: -0.01439935527741909\n",
      "        total_loss: 196.5403594970703\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 196.5540008544922\n",
      "    load_time_ms: 2.965\n",
      "    num_steps_sampled: 444000\n",
      "    num_steps_trained: 444000\n",
      "    sample_time_ms: 57389.439\n",
      "    update_time_ms: 57.879\n",
      "  iterations_since_restore: 222\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.6\n",
      "    ram_util_percent: 69.26451612903227\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.35355972626536\n",
      "    mean_inference_ms: 12.345990343207125\n",
      "    mean_processing_ms: 22.58411275043985\n",
      "  time_since_restore: 15830.040604829788\n",
      "  time_this_iter_s: 64.91190433502197\n",
      "  time_total_s: 15830.040604829788\n",
      "  timestamp: 1575781076\n",
      "  timesteps_since_restore: 444000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 444000\n",
      "  training_iteration: 222\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15830 s, 222 iter, 444000 ts, 38.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-07_23-59-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.86353592192421\n",
      "  episode_reward_mean: 38.77177774865157\n",
      "  episode_reward_min: -40.85431075669406\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7833.984\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3268301486968994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017153630033135414\n",
      "        policy_loss: -0.017220191657543182\n",
      "        total_loss: 230.15431213378906\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 230.17071533203125\n",
      "    load_time_ms: 2.935\n",
      "    num_steps_sampled: 446000\n",
      "    num_steps_trained: 446000\n",
      "    sample_time_ms: 57389.783\n",
      "    update_time_ms: 58.354\n",
      "  iterations_since_restore: 223\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.67849462365592\n",
      "    ram_util_percent: 69.34408602150539\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.459979889452\n",
      "    mean_inference_ms: 12.35006316492454\n",
      "    mean_processing_ms: 22.587577511725904\n",
      "  time_since_restore: 15895.555124759674\n",
      "  time_this_iter_s: 65.51451992988586\n",
      "  time_total_s: 15895.555124759674\n",
      "  timestamp: 1575781141\n",
      "  timesteps_since_restore: 446000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 446000\n",
      "  training_iteration: 223\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15895 s, 223 iter, 446000 ts, 38.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-00-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.86353592192421\n",
      "  episode_reward_mean: 40.513891453784574\n",
      "  episode_reward_min: -40.85431075669406\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7808.279\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.135763168334961\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016539838165044785\n",
      "        policy_loss: -0.016588017344474792\n",
      "        total_loss: 209.75537109375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.7712860107422\n",
      "    load_time_ms: 2.945\n",
      "    num_steps_sampled: 448000\n",
      "    num_steps_trained: 448000\n",
      "    sample_time_ms: 57667.043\n",
      "    update_time_ms: 58.796\n",
      "  iterations_since_restore: 224\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.69375000000001\n",
      "    ram_util_percent: 68.43020833333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.52499346773163\n",
      "    mean_inference_ms: 12.289999272709863\n",
      "    mean_processing_ms: 22.54965231300861\n",
      "  time_since_restore: 15962.692952156067\n",
      "  time_this_iter_s: 67.13782739639282\n",
      "  time_total_s: 15962.692952156067\n",
      "  timestamp: 1575781208\n",
      "  timesteps_since_restore: 448000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 448000\n",
      "  training_iteration: 224\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 15962 s, 224 iter, 448000 ts, 40.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-01-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.60534019842845\n",
      "  episode_reward_mean: 39.66082197069489\n",
      "  episode_reward_min: -40.85431075669406\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7814.009\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.048108458518981934\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.517103433609009\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0541636161506176\n",
      "        policy_loss: 0.001560576492920518\n",
      "        total_loss: 217.85006713867188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 217.8459014892578\n",
      "    load_time_ms: 2.922\n",
      "    num_steps_sampled: 450000\n",
      "    num_steps_trained: 450000\n",
      "    sample_time_ms: 57736.692\n",
      "    update_time_ms: 54.19\n",
      "  iterations_since_restore: 225\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.46559139784945\n",
      "    ram_util_percent: 68.01290322580645\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.1944385604489\n",
      "    mean_inference_ms: 12.270416428747298\n",
      "    mean_processing_ms: 22.530324950036082\n",
      "  time_since_restore: 16027.915136098862\n",
      "  time_this_iter_s: 65.2221839427948\n",
      "  time_total_s: 16027.915136098862\n",
      "  timestamp: 1575781274\n",
      "  timesteps_since_restore: 450000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 450000\n",
      "  training_iteration: 225\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16027 s, 225 iter, 450000 ts, 39.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-02-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.8500167605849\n",
      "  episode_reward_mean: 42.365882121712794\n",
      "  episode_reward_min: -40.85431075669406\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7812.76\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8393468856811523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027852756902575493\n",
      "        policy_loss: -0.027836930006742477\n",
      "        total_loss: 209.5894012451172\n",
      "        vf_explained_var: 2.384185791015625e-07\n",
      "        vf_loss: 209.615234375\n",
      "    load_time_ms: 3.041\n",
      "    num_steps_sampled: 452000\n",
      "    num_steps_trained: 452000\n",
      "    sample_time_ms: 57871.903\n",
      "    update_time_ms: 53.945\n",
      "  iterations_since_restore: 226\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.47978723404253\n",
      "    ram_util_percent: 68.0117021276596\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.2766082886289\n",
      "    mean_inference_ms: 12.277463191801798\n",
      "    mean_processing_ms: 22.549163019836193\n",
      "  time_since_restore: 16093.659162044525\n",
      "  time_this_iter_s: 65.74402594566345\n",
      "  time_total_s: 16093.659162044525\n",
      "  timestamp: 1575781339\n",
      "  timesteps_since_restore: 452000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 452000\n",
      "  training_iteration: 226\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16093 s, 226 iter, 452000 ts, 42.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-03-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.8500167605849\n",
      "  episode_reward_mean: 42.72850005284655\n",
      "  episode_reward_min: -40.85431075669406\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7805.657\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.356520891189575\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014158559031784534\n",
      "        policy_loss: -0.012008990161120892\n",
      "        total_loss: 241.0747528076172\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 241.08566284179688\n",
      "    load_time_ms: 3.068\n",
      "    num_steps_sampled: 454000\n",
      "    num_steps_trained: 454000\n",
      "    sample_time_ms: 57938.855\n",
      "    update_time_ms: 53.955\n",
      "  iterations_since_restore: 227\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.69120879120881\n",
      "    ram_util_percent: 68.13736263736263\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.86269573138668\n",
      "    mean_inference_ms: 12.251725802794704\n",
      "    mean_processing_ms: 22.521857208964764\n",
      "  time_since_restore: 16157.7624604702\n",
      "  time_this_iter_s: 64.10329842567444\n",
      "  time_total_s: 16157.7624604702\n",
      "  timestamp: 1575781404\n",
      "  timesteps_since_restore: 454000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 454000\n",
      "  training_iteration: 227\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16157 s, 227 iter, 454000 ts, 42.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-04-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.8500167605849\n",
      "  episode_reward_mean: 45.453082855917266\n",
      "  episode_reward_min: -20.426864345984914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7797.259\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.493863821029663\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016896378248929977\n",
      "        policy_loss: -0.019427405670285225\n",
      "        total_loss: 202.83042907714844\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 202.84866333007812\n",
      "    load_time_ms: 3.09\n",
      "    num_steps_sampled: 456000\n",
      "    num_steps_trained: 456000\n",
      "    sample_time_ms: 57854.303\n",
      "    update_time_ms: 54.574\n",
      "  iterations_since_restore: 228\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.52903225806452\n",
      "    ram_util_percent: 68.24838709677418\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.0269814648609\n",
      "    mean_inference_ms: 12.260606059758272\n",
      "    mean_processing_ms: 22.51481447854734\n",
      "  time_since_restore: 16222.794116258621\n",
      "  time_this_iter_s: 65.03165578842163\n",
      "  time_total_s: 16222.794116258621\n",
      "  timestamp: 1575781469\n",
      "  timesteps_since_restore: 456000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 456000\n",
      "  training_iteration: 228\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16222 s, 228 iter, 456000 ts, 45.5 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 00:05:34,469\tWARNING util.py:145 -- The `process_trial` operation took 0.20131731033325195 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-05-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.8500167605849\n",
      "  episode_reward_mean: 45.916642866181164\n",
      "  episode_reward_min: -20.426864345984914\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7449.75\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1943624019622803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01371838804334402\n",
      "        policy_loss: -0.0180059801787138\n",
      "        total_loss: 218.78656005859375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 218.8035888671875\n",
      "    load_time_ms: 3.252\n",
      "    num_steps_sampled: 458000\n",
      "    num_steps_trained: 458000\n",
      "    sample_time_ms: 57769.27\n",
      "    update_time_ms: 55.378\n",
      "  iterations_since_restore: 229\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.23548387096774\n",
      "    ram_util_percent: 68.35376344086018\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.67819967333207\n",
      "    mean_inference_ms: 12.300561502335912\n",
      "    mean_processing_ms: 22.533633036723014\n",
      "  time_since_restore: 16287.874325275421\n",
      "  time_this_iter_s: 65.08020901679993\n",
      "  time_total_s: 16287.874325275421\n",
      "  timestamp: 1575781534\n",
      "  timesteps_since_restore: 458000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 458000\n",
      "  training_iteration: 229\n",
      "  trial_id: 42ddae0c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 00:05:34,638\tWARNING util.py:145 -- The `on_step_begin` operation took 0.1022803783416748 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16287 s, 229 iter, 458000 ts, 45.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-06-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.8500167605849\n",
      "  episode_reward_mean: 49.151862089010095\n",
      "  episode_reward_min: -16.64669317227277\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7641.178\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1763038635253906\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01974409632384777\n",
      "        policy_loss: -0.0151801323518157\n",
      "        total_loss: 202.7451934814453\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 202.75897216796875\n",
      "    load_time_ms: 3.266\n",
      "    num_steps_sampled: 460000\n",
      "    num_steps_trained: 460000\n",
      "    sample_time_ms: 57597.604\n",
      "    update_time_ms: 76.034\n",
      "  iterations_since_restore: 230\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.91702127659575\n",
      "    ram_util_percent: 68.48404255319149\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.96916633324426\n",
      "    mean_inference_ms: 12.316818901839685\n",
      "    mean_processing_ms: 22.54123525511165\n",
      "  time_since_restore: 16353.56979727745\n",
      "  time_this_iter_s: 65.69547200202942\n",
      "  time_total_s: 16353.56979727745\n",
      "  timestamp: 1575781600\n",
      "  timesteps_since_restore: 460000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 460000\n",
      "  training_iteration: 230\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16353 s, 230 iter, 460000 ts, 49.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-07-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.6837410422477\n",
      "  episode_reward_mean: 46.540754711648745\n",
      "  episode_reward_min: -16.64669317227277\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7642.804\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0842783451080322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01183691993355751\n",
      "        policy_loss: -0.01970566250383854\n",
      "        total_loss: 208.4679412841797\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 208.48675537109375\n",
      "    load_time_ms: 3.206\n",
      "    num_steps_sampled: 462000\n",
      "    num_steps_trained: 462000\n",
      "    sample_time_ms: 57412.04\n",
      "    update_time_ms: 75.668\n",
      "  iterations_since_restore: 231\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.3688888888889\n",
      "    ram_util_percent: 68.58222222222221\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.9791694292467\n",
      "    mean_inference_ms: 12.316728498802599\n",
      "    mean_processing_ms: 22.525704730053786\n",
      "  time_since_restore: 16417.03213405609\n",
      "  time_this_iter_s: 63.46233677864075\n",
      "  time_total_s: 16417.03213405609\n",
      "  timestamp: 1575781663\n",
      "  timesteps_since_restore: 462000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 462000\n",
      "  training_iteration: 231\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16417 s, 231 iter, 462000 ts, 46.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-08-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.6837410422477\n",
      "  episode_reward_mean: 46.5764980895046\n",
      "  episode_reward_min: -3.755598708648526\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7643.476\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9616284370422363\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020461305975914\n",
      "        policy_loss: -0.01759914681315422\n",
      "        total_loss: 203.53094482421875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 203.54710388183594\n",
      "    load_time_ms: 3.248\n",
      "    num_steps_sampled: 464000\n",
      "    num_steps_trained: 464000\n",
      "    sample_time_ms: 57480.565\n",
      "    update_time_ms: 77.425\n",
      "  iterations_since_restore: 232\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.26702127659574\n",
      "    ram_util_percent: 68.52127659574468\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 147.54007109400732\n",
      "    mean_inference_ms: 12.350358781874666\n",
      "    mean_processing_ms: 22.562051738614354\n",
      "  time_since_restore: 16482.65492272377\n",
      "  time_this_iter_s: 65.62278866767883\n",
      "  time_total_s: 16482.65492272377\n",
      "  timestamp: 1575781729\n",
      "  timesteps_since_restore: 464000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 464000\n",
      "  training_iteration: 232\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16482 s, 232 iter, 464000 ts, 46.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84254448143697\n",
      "  episode_reward_mean: 47.064222429495274\n",
      "  episode_reward_min: 0.04583901078044479\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7623.186\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0816566944122314\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021598350256681442\n",
      "        policy_loss: -0.012929477728903294\n",
      "        total_loss: 222.2921600341797\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 222.3035430908203\n",
      "    load_time_ms: 3.321\n",
      "    num_steps_sampled: 466000\n",
      "    num_steps_trained: 466000\n",
      "    sample_time_ms: 57461.597\n",
      "    update_time_ms: 77.146\n",
      "  iterations_since_restore: 233\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.95913978494625\n",
      "    ram_util_percent: 68.48494623655917\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.91167720791404\n",
      "    mean_inference_ms: 12.311691817748754\n",
      "    mean_processing_ms: 22.533455424532352\n",
      "  time_since_restore: 16547.773166894913\n",
      "  time_this_iter_s: 65.11824417114258\n",
      "  time_total_s: 16547.773166894913\n",
      "  timestamp: 1575781794\n",
      "  timesteps_since_restore: 466000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 466000\n",
      "  training_iteration: 233\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16547 s, 233 iter, 466000 ts, 47.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-10-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84254448143697\n",
      "  episode_reward_mean: 45.499493908230086\n",
      "  episode_reward_min: -2.757341090335871\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7407.426\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.386413097381592\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016181346029043198\n",
      "        policy_loss: -0.015234414488077164\n",
      "        total_loss: 221.41761779785156\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 221.43170166015625\n",
      "    load_time_ms: 3.354\n",
      "    num_steps_sampled: 468000\n",
      "    num_steps_trained: 468000\n",
      "    sample_time_ms: 57479.899\n",
      "    update_time_ms: 78.344\n",
      "  iterations_since_restore: 234\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.0989247311828\n",
      "    ram_util_percent: 68.55698924731186\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.8678390313161\n",
      "    mean_inference_ms: 12.244341555863384\n",
      "    mean_processing_ms: 22.471358467497026\n",
      "  time_since_restore: 16612.94988656044\n",
      "  time_this_iter_s: 65.17671966552734\n",
      "  time_total_s: 16612.94988656044\n",
      "  timestamp: 1575781859\n",
      "  timesteps_since_restore: 468000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 468000\n",
      "  training_iteration: 234\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16612 s, 234 iter, 468000 ts, 45.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-12-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.99863965785755\n",
      "  episode_reward_mean: 46.05502706682118\n",
      "  episode_reward_min: -2.757341090335871\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7609.699\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7986853122711182\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03656190633773804\n",
      "        policy_loss: -0.0030618724413216114\n",
      "        total_loss: 224.20199584960938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 224.2024383544922\n",
      "    load_time_ms: 3.259\n",
      "    num_steps_sampled: 470000\n",
      "    num_steps_trained: 470000\n",
      "    sample_time_ms: 57498.035\n",
      "    update_time_ms: 79.48\n",
      "  iterations_since_restore: 235\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.91458333333334\n",
      "    ram_util_percent: 68.62083333333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 146.02170493896688\n",
      "    mean_inference_ms: 12.251636526160759\n",
      "    mean_processing_ms: 22.467030885853518\n",
      "  time_since_restore: 16680.388083457947\n",
      "  time_this_iter_s: 67.43819689750671\n",
      "  time_total_s: 16680.388083457947\n",
      "  timestamp: 1575781927\n",
      "  timesteps_since_restore: 470000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 470000\n",
      "  training_iteration: 235\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16680 s, 235 iter, 470000 ts, 46.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-13-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.99863965785755\n",
      "  episode_reward_mean: 46.47028741069543\n",
      "  episode_reward_min: -12.076277428819258\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7818.317\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.0721626877784729\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9613633155822754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.11087582260370255\n",
      "        policy_loss: 0.039323437958955765\n",
      "        total_loss: 225.587890625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 225.54054260253906\n",
      "    load_time_ms: 3.162\n",
      "    num_steps_sampled: 472000\n",
      "    num_steps_trained: 472000\n",
      "    sample_time_ms: 57193.024\n",
      "    update_time_ms: 78.182\n",
      "  iterations_since_restore: 236\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.75591397849462\n",
      "    ram_util_percent: 68.73118279569896\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.89392143794083\n",
      "    mean_inference_ms: 12.241853402850836\n",
      "    mean_processing_ms: 22.465262728157235\n",
      "  time_since_restore: 16745.166332244873\n",
      "  time_this_iter_s: 64.77824878692627\n",
      "  time_total_s: 16745.166332244873\n",
      "  timestamp: 1575781991\n",
      "  timesteps_since_restore: 472000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 472000\n",
      "  training_iteration: 236\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16745 s, 236 iter, 472000 ts, 46.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-14-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.99863965785755\n",
      "  episode_reward_mean: 46.62800480194237\n",
      "  episode_reward_min: -12.076277428819258\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7850.167\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10824403166770935\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2600977420806885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.17735616862773895\n",
      "        policy_loss: 0.04413197934627533\n",
      "        total_loss: 221.21502685546875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 221.1516571044922\n",
      "    load_time_ms: 3.105\n",
      "    num_steps_sampled: 474000\n",
      "    num_steps_trained: 474000\n",
      "    sample_time_ms: 57316.599\n",
      "    update_time_ms: 78.205\n",
      "  iterations_since_restore: 237\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.81935483870969\n",
      "    ram_util_percent: 68.77311827956989\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.1737192803866\n",
      "    mean_inference_ms: 12.193702418493308\n",
      "    mean_processing_ms: 22.412241453103338\n",
      "  time_since_restore: 16810.83166527748\n",
      "  time_this_iter_s: 65.66533303260803\n",
      "  time_total_s: 16810.83166527748\n",
      "  timestamp: 1575782057\n",
      "  timesteps_since_restore: 474000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 474000\n",
      "  training_iteration: 237\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16810 s, 237 iter, 474000 ts, 46.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-15-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.99863965785755\n",
      "  episode_reward_mean: 44.28053367283567\n",
      "  episode_reward_min: -12.076277428819258\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7858.78\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16236604750156403\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8519604206085205\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017494695261120796\n",
      "        policy_loss: -0.012345313094556332\n",
      "        total_loss: 232.4070587158203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 232.41661071777344\n",
      "    load_time_ms: 3.137\n",
      "    num_steps_sampled: 476000\n",
      "    num_steps_trained: 476000\n",
      "    sample_time_ms: 57296.192\n",
      "    update_time_ms: 78.769\n",
      "  iterations_since_restore: 238\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.58172043010754\n",
      "    ram_util_percent: 68.86881720430108\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.12327375598264\n",
      "    mean_inference_ms: 12.188513868187425\n",
      "    mean_processing_ms: 22.403201758560918\n",
      "  time_since_restore: 16875.76404261589\n",
      "  time_this_iter_s: 64.93237733840942\n",
      "  time_total_s: 16875.76404261589\n",
      "  timestamp: 1575782122\n",
      "  timesteps_since_restore: 476000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 476000\n",
      "  training_iteration: 238\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16875 s, 238 iter, 476000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-16-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.99863965785755\n",
      "  episode_reward_mean: 44.05746900404153\n",
      "  episode_reward_min: -25.071876768882827\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7835.433\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16236604750156403\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4549062252044678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04609919711947441\n",
      "        policy_loss: -0.008947286754846573\n",
      "        total_loss: 274.9125061035156\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 274.9139404296875\n",
      "    load_time_ms: 2.971\n",
      "    num_steps_sampled: 478000\n",
      "    num_steps_trained: 478000\n",
      "    sample_time_ms: 57389.315\n",
      "    update_time_ms: 77.085\n",
      "  iterations_since_restore: 239\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.92978723404256\n",
      "    ram_util_percent: 68.7372340425532\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.02469750966998\n",
      "    mean_inference_ms: 12.17893576360424\n",
      "    mean_processing_ms: 22.39298217766277\n",
      "  time_since_restore: 16941.339304208755\n",
      "  time_this_iter_s: 65.57526159286499\n",
      "  time_total_s: 16941.339304208755\n",
      "  timestamp: 1575782188\n",
      "  timesteps_since_restore: 478000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 478000\n",
      "  training_iteration: 239\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 16941 s, 239 iter, 478000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-17-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.15582763912654\n",
      "  episode_reward_mean: 40.39654854937157\n",
      "  episode_reward_min: -25.071876768882827\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7644.693\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24354907870292664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.731593608856201\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025521766394376755\n",
      "        policy_loss: -0.016132941469550133\n",
      "        total_loss: 218.1001434326172\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.110107421875\n",
      "    load_time_ms: 3.355\n",
      "    num_steps_sampled: 480000\n",
      "    num_steps_trained: 480000\n",
      "    sample_time_ms: 57497.714\n",
      "    update_time_ms: 55.653\n",
      "  iterations_since_restore: 240\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.00652173913043\n",
      "    ram_util_percent: 68.7728260869565\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.74453832065458\n",
      "    mean_inference_ms: 12.15930256520533\n",
      "    mean_processing_ms: 22.38844247849779\n",
      "  time_since_restore: 17006.00008583069\n",
      "  time_this_iter_s: 64.66078162193298\n",
      "  time_total_s: 17006.00008583069\n",
      "  timestamp: 1575782252\n",
      "  timesteps_since_restore: 480000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 480000\n",
      "  training_iteration: 240\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.7/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17006 s, 240 iter, 480000 ts, 40.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-18-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.15582763912654\n",
      "  episode_reward_mean: 38.19527485959367\n",
      "  episode_reward_min: -38.71916176551688\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7842.418\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24354907870292664\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5009589195251465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008292121812701225\n",
      "        policy_loss: -0.01371060125529766\n",
      "        total_loss: 235.26773071289062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 235.2794952392578\n",
      "    load_time_ms: 3.436\n",
      "    num_steps_sampled: 482000\n",
      "    num_steps_trained: 482000\n",
      "    sample_time_ms: 57601.307\n",
      "    update_time_ms: 54.319\n",
      "  iterations_since_restore: 241\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.9063157894737\n",
      "    ram_util_percent: 68.86105263157896\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.51068631542915\n",
      "    mean_inference_ms: 12.142321432783369\n",
      "    mean_processing_ms: 22.347423875263143\n",
      "  time_since_restore: 17072.46412563324\n",
      "  time_this_iter_s: 66.46403980255127\n",
      "  time_total_s: 17072.46412563324\n",
      "  timestamp: 1575782319\n",
      "  timesteps_since_restore: 482000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 482000\n",
      "  training_iteration: 241\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17072 s, 241 iter, 482000 ts, 38.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-19-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.15582763912654\n",
      "  episode_reward_mean: 35.91119330671327\n",
      "  episode_reward_min: -38.71916176551688\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7848.889\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12177453935146332\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0165517330169678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01980694942176342\n",
      "        policy_loss: -0.01451918389648199\n",
      "        total_loss: 183.5645751953125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 183.5767059326172\n",
      "    load_time_ms: 3.438\n",
      "    num_steps_sampled: 484000\n",
      "    num_steps_trained: 484000\n",
      "    sample_time_ms: 57429.975\n",
      "    update_time_ms: 52.776\n",
      "  iterations_since_restore: 242\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.33076923076923\n",
      "    ram_util_percent: 68.93406593406593\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.7905308792544\n",
      "    mean_inference_ms: 12.161093412004389\n",
      "    mean_processing_ms: 22.3673741904095\n",
      "  time_since_restore: 17136.423785448074\n",
      "  time_this_iter_s: 63.959659814834595\n",
      "  time_total_s: 17136.423785448074\n",
      "  timestamp: 1575782383\n",
      "  timesteps_since_restore: 484000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 484000\n",
      "  training_iteration: 242\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17136 s, 242 iter, 484000 ts, 35.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-20-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.15582763912654\n",
      "  episode_reward_mean: 37.975669653288165\n",
      "  episode_reward_min: -38.71916176551688\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7878.96\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12177453935146332\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1272590160369873\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01201250497251749\n",
      "        policy_loss: -0.014453765004873276\n",
      "        total_loss: 236.83023071289062\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 236.8431854248047\n",
      "    load_time_ms: 3.465\n",
      "    num_steps_sampled: 486000\n",
      "    num_steps_trained: 486000\n",
      "    sample_time_ms: 57354.077\n",
      "    update_time_ms: 52.375\n",
      "  iterations_since_restore: 243\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.8586956521739\n",
      "    ram_util_percent: 69.0163043478261\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.69209094959135\n",
      "    mean_inference_ms: 12.152742501115554\n",
      "    mean_processing_ms: 22.358225343311357\n",
      "  time_since_restore: 17201.08487534523\n",
      "  time_this_iter_s: 64.66108989715576\n",
      "  time_total_s: 17201.08487534523\n",
      "  timestamp: 1575782448\n",
      "  timesteps_since_restore: 486000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 486000\n",
      "  training_iteration: 243\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17201 s, 243 iter, 486000 ts, 38 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-21-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.58017753499186\n",
      "  episode_reward_mean: 40.68173794429882\n",
      "  episode_reward_min: -38.71916176551688\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7888.74\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12177453935146332\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7276469469070435\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013518151827156544\n",
      "        policy_loss: -0.010052735917270184\n",
      "        total_loss: 208.33396911621094\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 208.34231567382812\n",
      "    load_time_ms: 3.411\n",
      "    num_steps_sampled: 488000\n",
      "    num_steps_trained: 488000\n",
      "    sample_time_ms: 57326.109\n",
      "    update_time_ms: 51.2\n",
      "  iterations_since_restore: 244\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.62795698924731\n",
      "    ram_util_percent: 69.0956989247312\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.06254505848904\n",
      "    mean_inference_ms: 12.175655198383781\n",
      "    mean_processing_ms: 22.36839231923524\n",
      "  time_since_restore: 17266.069759607315\n",
      "  time_this_iter_s: 64.98488426208496\n",
      "  time_total_s: 17266.069759607315\n",
      "  timestamp: 1575782513\n",
      "  timesteps_since_restore: 488000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 488000\n",
      "  training_iteration: 244\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17266 s, 244 iter, 488000 ts, 40.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-22-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.73483366307103\n",
      "  episode_reward_mean: 40.89290278466604\n",
      "  episode_reward_min: -38.71916176551688\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7690.531\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12177453935146332\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2596724033355713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.034219976514577866\n",
      "        policy_loss: -0.02795194275677204\n",
      "        total_loss: 224.38265991210938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 224.4063720703125\n",
      "    load_time_ms: 3.572\n",
      "    num_steps_sampled: 490000\n",
      "    num_steps_trained: 490000\n",
      "    sample_time_ms: 57370.443\n",
      "    update_time_ms: 50.486\n",
      "  iterations_since_restore: 245\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.99787234042553\n",
      "    ram_util_percent: 69.17234042553193\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.07535747183735\n",
      "    mean_inference_ms: 12.177420694076968\n",
      "    mean_processing_ms: 22.364372659017455\n",
      "  time_since_restore: 17331.96242904663\n",
      "  time_this_iter_s: 65.8926694393158\n",
      "  time_total_s: 17331.96242904663\n",
      "  timestamp: 1575782579\n",
      "  timesteps_since_restore: 490000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 490000\n",
      "  training_iteration: 245\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17331 s, 245 iter, 490000 ts, 40.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-24-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.73483366307103\n",
      "  episode_reward_mean: 40.70331406298194\n",
      "  episode_reward_min: -13.81190467586972\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7474.294\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12177453935146332\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3345346450805664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014309226535260677\n",
      "        policy_loss: -0.016873937100172043\n",
      "        total_loss: 204.09751892089844\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 204.1126708984375\n",
      "    load_time_ms: 3.59\n",
      "    num_steps_sampled: 492000\n",
      "    num_steps_trained: 492000\n",
      "    sample_time_ms: 57558.541\n",
      "    update_time_ms: 50.917\n",
      "  iterations_since_restore: 246\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.43152173913045\n",
      "    ram_util_percent: 69.0217391304348\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.29700208712364\n",
      "    mean_inference_ms: 12.192207559334038\n",
      "    mean_processing_ms: 22.38626310399748\n",
      "  time_since_restore: 17396.447071552277\n",
      "  time_this_iter_s: 64.48464250564575\n",
      "  time_total_s: 17396.447071552277\n",
      "  timestamp: 1575782643\n",
      "  timesteps_since_restore: 492000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 492000\n",
      "  training_iteration: 246\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17396 s, 246 iter, 492000 ts, 40.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-25-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.73483366307103\n",
      "  episode_reward_mean: 41.25968739342288\n",
      "  episode_reward_min: -20.566177805776608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7618.582\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12177453935146332\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0887157917022705\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009838646277785301\n",
      "        policy_loss: -0.012961762957274914\n",
      "        total_loss: 205.21876525878906\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 205.23057556152344\n",
      "    load_time_ms: 4.269\n",
      "    num_steps_sampled: 494000\n",
      "    num_steps_trained: 494000\n",
      "    sample_time_ms: 57798.707\n",
      "    update_time_ms: 55.895\n",
      "  iterations_since_restore: 247\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.59797979797979\n",
      "    ram_util_percent: 69.14040404040405\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.9333119240337\n",
      "    mean_inference_ms: 12.167370256097259\n",
      "    mean_processing_ms: 22.353183637307797\n",
      "  time_since_restore: 17466.006445407867\n",
      "  time_this_iter_s: 69.55937385559082\n",
      "  time_total_s: 17466.006445407867\n",
      "  timestamp: 1575782713\n",
      "  timesteps_since_restore: 494000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 494000\n",
      "  training_iteration: 247\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17466 s, 247 iter, 494000 ts, 41.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-26-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.73483366307103\n",
      "  episode_reward_mean: 42.19686992658498\n",
      "  episode_reward_min: -20.566177805776608\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7645.096\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06088726967573166\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3861963748931885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04873571917414665\n",
      "        policy_loss: -0.01582043617963791\n",
      "        total_loss: 186.99996948242188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 187.01290893554688\n",
      "    load_time_ms: 4.22\n",
      "    num_steps_sampled: 496000\n",
      "    num_steps_trained: 496000\n",
      "    sample_time_ms: 57791.163\n",
      "    update_time_ms: 54.581\n",
      "  iterations_since_restore: 248\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.18064516129031\n",
      "    ram_util_percent: 69.16344086021508\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.89616702825847\n",
      "    mean_inference_ms: 12.164754302717576\n",
      "    mean_processing_ms: 22.348213716903807\n",
      "  time_since_restore: 17531.116373300552\n",
      "  time_this_iter_s: 65.10992789268494\n",
      "  time_total_s: 17531.116373300552\n",
      "  timestamp: 1575782778\n",
      "  timesteps_since_restore: 496000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 496000\n",
      "  training_iteration: 248\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17531 s, 248 iter, 496000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-27-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.73483366307103\n",
      "  episode_reward_mean: 39.23022664751216\n",
      "  episode_reward_min: -26.634411635765478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 4980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7658.477\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09133090078830719\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9983999729156494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022670850157737732\n",
      "        policy_loss: -0.014483079314231873\n",
      "        total_loss: 236.8728485107422\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 236.88523864746094\n",
      "    load_time_ms: 4.193\n",
      "    num_steps_sampled: 498000\n",
      "    num_steps_trained: 498000\n",
      "    sample_time_ms: 57796.592\n",
      "    update_time_ms: 54.909\n",
      "  iterations_since_restore: 249\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.59361702127659\n",
      "    ram_util_percent: 69.24574468085109\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.88830304097212\n",
      "    mean_inference_ms: 12.163472464364483\n",
      "    mean_processing_ms: 22.351535217429543\n",
      "  time_since_restore: 17596.893680095673\n",
      "  time_this_iter_s: 65.77730679512024\n",
      "  time_total_s: 17596.893680095673\n",
      "  timestamp: 1575782844\n",
      "  timesteps_since_restore: 498000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 498000\n",
      "  training_iteration: 249\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17596 s, 249 iter, 498000 ts, 39.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-28-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.06426507077151\n",
      "  episode_reward_mean: 41.68002973685125\n",
      "  episode_reward_min: -26.634411635765478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7666.384\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09133090078830719\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.248887777328491\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014171240851283073\n",
      "        policy_loss: -0.01597709394991398\n",
      "        total_loss: 219.70004272460938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 219.71469116210938\n",
      "    load_time_ms: 3.792\n",
      "    num_steps_sampled: 500000\n",
      "    num_steps_trained: 500000\n",
      "    sample_time_ms: 57845.356\n",
      "    update_time_ms: 56.352\n",
      "  iterations_since_restore: 250\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.80215053763442\n",
      "    ram_util_percent: 69.3225806451613\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.74315413453152\n",
      "    mean_inference_ms: 12.155828720934728\n",
      "    mean_processing_ms: 22.344206961525078\n",
      "  time_since_restore: 17662.133419036865\n",
      "  time_this_iter_s: 65.23973894119263\n",
      "  time_total_s: 17662.133419036865\n",
      "  timestamp: 1575782909\n",
      "  timesteps_since_restore: 500000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 500000\n",
      "  training_iteration: 250\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17662 s, 250 iter, 500000 ts, 41.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 00:29:34,878\tWARNING util.py:145 -- The `process_trial` operation took 0.16254234313964844 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-29-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.06426507077151\n",
      "  episode_reward_mean: 44.51797312463662\n",
      "  episode_reward_min: -26.634411635765478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7549.051\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09133090078830719\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.62837815284729\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.1308518648147583\n",
      "        policy_loss: 0.15088513493537903\n",
      "        total_loss: 194.2867431640625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 194.03253173828125\n",
      "    load_time_ms: 3.719\n",
      "    num_steps_sampled: 502000\n",
      "    num_steps_trained: 502000\n",
      "    sample_time_ms: 57809.635\n",
      "    update_time_ms: 57.224\n",
      "  iterations_since_restore: 251\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82688172043012\n",
      "    ram_util_percent: 69.39139784946236\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.8174661903439\n",
      "    mean_inference_ms: 12.157257428722568\n",
      "    mean_processing_ms: 22.33673741730665\n",
      "  time_since_restore: 17727.258607387543\n",
      "  time_this_iter_s: 65.12518835067749\n",
      "  time_total_s: 17727.258607387543\n",
      "  timestamp: 1575782974\n",
      "  timesteps_since_restore: 502000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 502000\n",
      "  training_iteration: 251\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17727 s, 251 iter, 502000 ts, 44.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-30-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.72830776352\n",
      "  episode_reward_mean: 46.290660230892\n",
      "  episode_reward_min: -26.634411635765478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7795.518\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13699635863304138\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.68857741355896\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00552657525986433\n",
      "        policy_loss: -0.01312343031167984\n",
      "        total_loss: 209.74099731445312\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 209.75331115722656\n",
      "    load_time_ms: 3.726\n",
      "    num_steps_sampled: 504000\n",
      "    num_steps_trained: 504000\n",
      "    sample_time_ms: 57757.037\n",
      "    update_time_ms: 74.604\n",
      "  iterations_since_restore: 252\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.6863157894737\n",
      "    ram_util_percent: 69.51263157894738\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.32849057216836\n",
      "    mean_inference_ms: 12.190296744409013\n",
      "    mean_processing_ms: 22.365454296981344\n",
      "  time_since_restore: 17793.33175444603\n",
      "  time_this_iter_s: 66.07314705848694\n",
      "  time_total_s: 17793.33175444603\n",
      "  timestamp: 1575783040\n",
      "  timesteps_since_restore: 504000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 504000\n",
      "  training_iteration: 252\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17793 s, 252 iter, 504000 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-31-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.72830776352\n",
      "  episode_reward_mean: 45.46292043734376\n",
      "  episode_reward_min: -26.634411635765478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7791.151\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06849817931652069\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3862645626068115\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023319069296121597\n",
      "        policy_loss: -0.014087424613535404\n",
      "        total_loss: 207.3389129638672\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 207.3514404296875\n",
      "    load_time_ms: 3.633\n",
      "    num_steps_sampled: 506000\n",
      "    num_steps_trained: 506000\n",
      "    sample_time_ms: 57767.543\n",
      "    update_time_ms: 75.163\n",
      "  iterations_since_restore: 253\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16521739130435\n",
      "    ram_util_percent: 69.325\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.5886918471119\n",
      "    mean_inference_ms: 12.206875457378798\n",
      "    mean_processing_ms: 22.375536431414066\n",
      "  time_since_restore: 17858.055166482925\n",
      "  time_this_iter_s: 64.72341203689575\n",
      "  time_total_s: 17858.055166482925\n",
      "  timestamp: 1575783105\n",
      "  timesteps_since_restore: 506000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 506000\n",
      "  training_iteration: 253\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17858 s, 253 iter, 506000 ts, 45.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-32-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.72830776352\n",
      "  episode_reward_mean: 46.00142265549724\n",
      "  episode_reward_min: -7.819558422821552\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7792.176\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06849817931652069\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.042600154876709\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.6122472882270813\n",
      "        policy_loss: 0.14835630357265472\n",
      "        total_loss: 239.3532257080078\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 239.16290283203125\n",
      "    load_time_ms: 3.711\n",
      "    num_steps_sampled: 508000\n",
      "    num_steps_trained: 508000\n",
      "    sample_time_ms: 57826.859\n",
      "    update_time_ms: 75.065\n",
      "  iterations_since_restore: 254\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.24574468085105\n",
      "    ram_util_percent: 69.3851063829787\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.2433193213904\n",
      "    mean_inference_ms: 12.183707493133838\n",
      "    mean_processing_ms: 22.351156439023235\n",
      "  time_since_restore: 17923.631016731262\n",
      "  time_this_iter_s: 65.57585024833679\n",
      "  time_total_s: 17923.631016731262\n",
      "  timestamp: 1575783171\n",
      "  timesteps_since_restore: 508000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 508000\n",
      "  training_iteration: 254\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17923 s, 254 iter, 508000 ts, 46 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-33-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.72830776352\n",
      "  episode_reward_mean: 44.29938721121948\n",
      "  episode_reward_min: -7.819558422821552\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7777.754\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10274726897478104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.341846227645874\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.11341935396194458\n",
      "        policy_loss: 0.031039424240589142\n",
      "        total_loss: 219.92103576660156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 219.8783721923828\n",
      "    load_time_ms: 3.55\n",
      "    num_steps_sampled: 510000\n",
      "    num_steps_trained: 510000\n",
      "    sample_time_ms: 57741.507\n",
      "    update_time_ms: 74.287\n",
      "  iterations_since_restore: 255\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.10108695652173\n",
      "    ram_util_percent: 69.46630434782608\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.17332597809255\n",
      "    mean_inference_ms: 12.177116655539944\n",
      "    mean_processing_ms: 22.336118035946708\n",
      "  time_since_restore: 17988.52931523323\n",
      "  time_this_iter_s: 64.89829850196838\n",
      "  time_total_s: 17988.52931523323\n",
      "  timestamp: 1575783236\n",
      "  timesteps_since_restore: 510000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 510000\n",
      "  training_iteration: 255\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 17988 s, 255 iter, 510000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-35-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.72830776352\n",
      "  episode_reward_mean: 44.632818578336746\n",
      "  episode_reward_min: -7.819558422821552\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7780.663\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15412090718746185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7965970039367676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007859279401600361\n",
      "        policy_loss: -0.012213406153023243\n",
      "        total_loss: 209.29412841796875\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.30517578125\n",
      "    load_time_ms: 3.53\n",
      "    num_steps_sampled: 512000\n",
      "    num_steps_trained: 512000\n",
      "    sample_time_ms: 57873.76\n",
      "    update_time_ms: 74.144\n",
      "  iterations_since_restore: 256\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.81063829787234\n",
      "    ram_util_percent: 69.56702127659575\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.41944615390236\n",
      "    mean_inference_ms: 12.129579894845328\n",
      "    mean_processing_ms: 22.299620885231942\n",
      "  time_since_restore: 18054.36627292633\n",
      "  time_this_iter_s: 65.83695769309998\n",
      "  time_total_s: 18054.36627292633\n",
      "  timestamp: 1575783302\n",
      "  timesteps_since_restore: 512000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 512000\n",
      "  training_iteration: 256\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18054 s, 256 iter, 512000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-36-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.6810780997832\n",
      "  episode_reward_mean: 42.9208041937513\n",
      "  episode_reward_min: -10.44408650888443\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7843.787\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07706045359373093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0915303230285645\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.4833206236362457\n",
      "        policy_loss: 0.08957882970571518\n",
      "        total_loss: 217.75172424316406\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 217.62484741210938\n",
      "    load_time_ms: 2.941\n",
      "    num_steps_sampled: 514000\n",
      "    num_steps_trained: 514000\n",
      "    sample_time_ms: 57603.43\n",
      "    update_time_ms: 69.669\n",
      "  iterations_since_restore: 257\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.42708333333333\n",
      "    ram_util_percent: 69.63958333333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.96917442502192\n",
      "    mean_inference_ms: 12.097438914624318\n",
      "    mean_processing_ms: 22.265729436333512\n",
      "  time_since_restore: 18121.802575826645\n",
      "  time_this_iter_s: 67.43630290031433\n",
      "  time_total_s: 18121.802575826645\n",
      "  timestamp: 1575783369\n",
      "  timesteps_since_restore: 514000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 514000\n",
      "  training_iteration: 257\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18121 s, 257 iter, 514000 ts, 42.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-37-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.6810780997832\n",
      "  episode_reward_mean: 42.62998702173735\n",
      "  episode_reward_min: -10.44408650888443\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7832.545\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11559067666530609\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6316771507263184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024279216304421425\n",
      "        policy_loss: -0.0026752206031233072\n",
      "        total_loss: 227.9639434814453\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 227.96383666992188\n",
      "    load_time_ms: 3.68\n",
      "    num_steps_sampled: 516000\n",
      "    num_steps_trained: 516000\n",
      "    sample_time_ms: 57548.82\n",
      "    update_time_ms: 69.758\n",
      "  iterations_since_restore: 258\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86413043478261\n",
      "    ram_util_percent: 69.73152173913043\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.00310192201107\n",
      "    mean_inference_ms: 12.096253691135594\n",
      "    mean_processing_ms: 22.267823703912423\n",
      "  time_since_restore: 18186.259114027023\n",
      "  time_this_iter_s: 64.45653820037842\n",
      "  time_total_s: 18186.259114027023\n",
      "  timestamp: 1575783434\n",
      "  timesteps_since_restore: 516000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 516000\n",
      "  training_iteration: 258\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18186 s, 258 iter, 516000 ts, 42.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-38-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.80656903805122\n",
      "  episode_reward_mean: 43.35924196923378\n",
      "  episode_reward_min: -10.44408650888443\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7828.94\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11559067666530609\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.838339328765869\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016399452462792397\n",
      "        policy_loss: -0.022983310744166374\n",
      "        total_loss: 210.87460327148438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.89569091796875\n",
      "    load_time_ms: 3.68\n",
      "    num_steps_sampled: 518000\n",
      "    num_steps_trained: 518000\n",
      "    sample_time_ms: 57561.416\n",
      "    update_time_ms: 69.749\n",
      "  iterations_since_restore: 259\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.24042553191488\n",
      "    ram_util_percent: 69.75425531914891\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.8707075728538\n",
      "    mean_inference_ms: 12.15217286254219\n",
      "    mean_processing_ms: 22.302734445832066\n",
      "  time_since_restore: 18252.1246407032\n",
      "  time_this_iter_s: 65.86552667617798\n",
      "  time_total_s: 18252.1246407032\n",
      "  timestamp: 1575783500\n",
      "  timesteps_since_restore: 518000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 518000\n",
      "  training_iteration: 259\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18252 s, 259 iter, 518000 ts, 43.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-39-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.80656903805122\n",
      "  episode_reward_mean: 44.96949844674172\n",
      "  episode_reward_min: -10.44408650888443\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7821.417\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11559067666530609\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.609226703643799\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05141045153141022\n",
      "        policy_loss: -0.0032146854791790247\n",
      "        total_loss: 208.2730712890625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 208.2704620361328\n",
      "    load_time_ms: 3.723\n",
      "    num_steps_sampled: 520000\n",
      "    num_steps_trained: 520000\n",
      "    sample_time_ms: 57496.592\n",
      "    update_time_ms: 69.299\n",
      "  iterations_since_restore: 260\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.96129032258064\n",
      "    ram_util_percent: 69.6268817204301\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.15872953800965\n",
      "    mean_inference_ms: 12.171320503145694\n",
      "    mean_processing_ms: 22.321245010863386\n",
      "  time_since_restore: 18316.63586783409\n",
      "  time_this_iter_s: 64.51122713088989\n",
      "  time_total_s: 18316.63586783409\n",
      "  timestamp: 1575783564\n",
      "  timesteps_since_restore: 520000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 520000\n",
      "  training_iteration: 260\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18316 s, 260 iter, 520000 ts, 45 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-40-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.80656903805122\n",
      "  episode_reward_mean: 43.94410094538861\n",
      "  episode_reward_min: -18.882880449239156\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7736.538\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17338600754737854\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6282825469970703\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04577142372727394\n",
      "        policy_loss: 0.0015814675716683269\n",
      "        total_loss: 211.42953491210938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 211.41993713378906\n",
      "    load_time_ms: 3.741\n",
      "    num_steps_sampled: 522000\n",
      "    num_steps_trained: 522000\n",
      "    sample_time_ms: 57575.475\n",
      "    update_time_ms: 69.265\n",
      "  iterations_since_restore: 261\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.49891304347825\n",
      "    ram_util_percent: 69.70000000000002\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.04415682591477\n",
      "    mean_inference_ms: 12.164755306580808\n",
      "    mean_processing_ms: 22.307689058890556\n",
      "  time_since_restore: 18381.526267528534\n",
      "  time_this_iter_s: 64.89039969444275\n",
      "  time_total_s: 18381.526267528534\n",
      "  timestamp: 1575783629\n",
      "  timesteps_since_restore: 522000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 522000\n",
      "  training_iteration: 261\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18381 s, 261 iter, 522000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-41-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.84311678124229\n",
      "  episode_reward_mean: 46.11632170649377\n",
      "  episode_reward_min: -18.882880449239156\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7715.404\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.260079026222229\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.836625337600708\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024956772103905678\n",
      "        policy_loss: -0.00485617620870471\n",
      "        total_loss: 212.80674743652344\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 212.80508422851562\n",
      "    load_time_ms: 3.789\n",
      "    num_steps_sampled: 524000\n",
      "    num_steps_trained: 524000\n",
      "    sample_time_ms: 57768.573\n",
      "    update_time_ms: 52.989\n",
      "  iterations_since_restore: 262\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.94845360824742\n",
      "    ram_util_percent: 69.78453608247423\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.4198882794075\n",
      "    mean_inference_ms: 12.189768683037741\n",
      "    mean_processing_ms: 22.335825740360157\n",
      "  time_since_restore: 18449.182357788086\n",
      "  time_this_iter_s: 67.656090259552\n",
      "  time_total_s: 18449.182357788086\n",
      "  timestamp: 1575783697\n",
      "  timesteps_since_restore: 524000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 524000\n",
      "  training_iteration: 262\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18449 s, 262 iter, 524000 ts, 46.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.84311678124229\n",
      "  episode_reward_mean: 46.69339312547766\n",
      "  episode_reward_min: -18.882880449239156\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7936.68\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.260079026222229\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.058410167694092\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05276763066649437\n",
      "        policy_loss: 0.009570562280714512\n",
      "        total_loss: 209.76458740234375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 209.74134826660156\n",
      "    load_time_ms: 3.753\n",
      "    num_steps_sampled: 526000\n",
      "    num_steps_trained: 526000\n",
      "    sample_time_ms: 57481.809\n",
      "    update_time_ms: 54.895\n",
      "  iterations_since_restore: 263\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56923076923076\n",
      "    ram_util_percent: 69.87692307692308\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.29735755869794\n",
      "    mean_inference_ms: 12.182909184032207\n",
      "    mean_processing_ms: 22.321137150393515\n",
      "  time_since_restore: 18513.269980430603\n",
      "  time_this_iter_s: 64.08762264251709\n",
      "  time_total_s: 18513.269980430603\n",
      "  timestamp: 1575783761\n",
      "  timesteps_since_restore: 526000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 526000\n",
      "  training_iteration: 263\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18513 s, 263 iter, 526000 ts, 46.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-43-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.84311678124229\n",
      "  episode_reward_mean: 46.3780246237797\n",
      "  episode_reward_min: -18.882880449239156\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7930.344\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3901185393333435\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.15701961517334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0052224681712687016\n",
      "        policy_loss: -0.010753065347671509\n",
      "        total_loss: 208.47726440429688\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 208.48599243164062\n",
      "    load_time_ms: 3.815\n",
      "    num_steps_sampled: 528000\n",
      "    num_steps_trained: 528000\n",
      "    sample_time_ms: 57372.107\n",
      "    update_time_ms: 55.114\n",
      "  iterations_since_restore: 264\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.51086956521739\n",
      "    ram_util_percent: 69.95652173913041\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.5702386542155\n",
      "    mean_inference_ms: 12.132859249459294\n",
      "    mean_processing_ms: 22.288413768511575\n",
      "  time_since_restore: 18577.69912672043\n",
      "  time_this_iter_s: 64.42914628982544\n",
      "  time_total_s: 18577.69912672043\n",
      "  timestamp: 1575783825\n",
      "  timesteps_since_restore: 528000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 528000\n",
      "  training_iteration: 264\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18577 s, 264 iter, 528000 ts, 46.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-44-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.84311678124229\n",
      "  episode_reward_mean: 44.27565380075653\n",
      "  episode_reward_min: -18.882880449239156\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7925.985\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19505926966667175\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3345770835876465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03787372633814812\n",
      "        policy_loss: -0.03332587331533432\n",
      "        total_loss: 202.433349609375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 202.45928955078125\n",
      "    load_time_ms: 3.817\n",
      "    num_steps_sampled: 530000\n",
      "    num_steps_trained: 530000\n",
      "    sample_time_ms: 57350.165\n",
      "    update_time_ms: 56.161\n",
      "  iterations_since_restore: 265\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82173913043479\n",
      "    ram_util_percent: 70.02717391304348\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.32451885725797\n",
      "    mean_inference_ms: 12.112921155334595\n",
      "    mean_processing_ms: 22.265166394335193\n",
      "  time_since_restore: 18642.332801103592\n",
      "  time_this_iter_s: 64.63367438316345\n",
      "  time_total_s: 18642.332801103592\n",
      "  timestamp: 1575783890\n",
      "  timesteps_since_restore: 530000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 530000\n",
      "  training_iteration: 265\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18642 s, 265 iter, 530000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-45-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.11462432012192\n",
      "  episode_reward_mean: 45.07745320466338\n",
      "  episode_reward_min: -1.5801121600802328\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7922.385\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19505926966667175\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7610137462615967\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016746625304222107\n",
      "        policy_loss: -0.019361186772584915\n",
      "        total_loss: 223.21365356445312\n",
      "        vf_explained_var: -2.384185791015625e-07\n",
      "        vf_loss: 223.2297821044922\n",
      "    load_time_ms: 3.928\n",
      "    num_steps_sampled: 532000\n",
      "    num_steps_trained: 532000\n",
      "    sample_time_ms: 57347.017\n",
      "    update_time_ms: 56.197\n",
      "  iterations_since_restore: 266\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.29893617021276\n",
      "    ram_util_percent: 70.05744680851063\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.84071506667922\n",
      "    mean_inference_ms: 12.146562742226424\n",
      "    mean_processing_ms: 22.295192114232208\n",
      "  time_since_restore: 18708.100804805756\n",
      "  time_this_iter_s: 65.7680037021637\n",
      "  time_total_s: 18708.100804805756\n",
      "  timestamp: 1575783956\n",
      "  timesteps_since_restore: 532000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 532000\n",
      "  training_iteration: 266\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18708 s, 266 iter, 532000 ts, 45.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-47-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.11462432012192\n",
      "  episode_reward_mean: 42.18101792607532\n",
      "  episode_reward_min: -11.572872224316079\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7687.608\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19505926966667175\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1283228397369385\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011630207300186157\n",
      "        policy_loss: -0.004723092075437307\n",
      "        total_loss: 233.11122131347656\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 233.11363220214844\n",
      "    load_time_ms: 3.878\n",
      "    num_steps_sampled: 534000\n",
      "    num_steps_trained: 534000\n",
      "    sample_time_ms: 57389.103\n",
      "    update_time_ms: 55.588\n",
      "  iterations_since_restore: 267\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.80957446808509\n",
      "    ram_util_percent: 69.91595744680852\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.2510673381869\n",
      "    mean_inference_ms: 12.106593248154194\n",
      "    mean_processing_ms: 22.24867618592551\n",
      "  time_since_restore: 18773.615072488785\n",
      "  time_this_iter_s: 65.51426768302917\n",
      "  time_total_s: 18773.615072488785\n",
      "  timestamp: 1575784021\n",
      "  timesteps_since_restore: 534000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 534000\n",
      "  training_iteration: 267\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18773 s, 267 iter, 534000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-48-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.11462432012192\n",
      "  episode_reward_mean: 40.07413356026097\n",
      "  episode_reward_min: -38.95259980645812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7911.796\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19505926966667175\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2384793758392334\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008967543952167034\n",
      "        policy_loss: -0.013619589619338512\n",
      "        total_loss: 245.76768493652344\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 245.77957153320312\n",
      "    load_time_ms: 3.155\n",
      "    num_steps_sampled: 536000\n",
      "    num_steps_trained: 536000\n",
      "    sample_time_ms: 57489.001\n",
      "    update_time_ms: 55.387\n",
      "  iterations_since_restore: 268\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.21354166666667\n",
      "    ram_util_percent: 69.99583333333332\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.93118750121718\n",
      "    mean_inference_ms: 12.084716867540905\n",
      "    mean_processing_ms: 22.22715897487498\n",
      "  time_since_restore: 18841.29455757141\n",
      "  time_this_iter_s: 67.67948508262634\n",
      "  time_total_s: 18841.29455757141\n",
      "  timestamp: 1575784089\n",
      "  timesteps_since_restore: 536000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 536000\n",
      "  training_iteration: 268\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18841 s, 268 iter, 536000 ts, 40.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-49-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.11462432012192\n",
      "  episode_reward_mean: 39.117587789147\n",
      "  episode_reward_min: -38.95259980645812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7917.356\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09752963483333588\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9665279388427734\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009939675219357014\n",
      "        policy_loss: -0.011175716295838356\n",
      "        total_loss: 220.07907104492188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 220.0892791748047\n",
      "    load_time_ms: 3.21\n",
      "    num_steps_sampled: 538000\n",
      "    num_steps_trained: 538000\n",
      "    sample_time_ms: 57573.67\n",
      "    update_time_ms: 55.813\n",
      "  iterations_since_restore: 269\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.903125\n",
      "    ram_util_percent: 70.08645833333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.82170004728883\n",
      "    mean_inference_ms: 12.078993112031085\n",
      "    mean_processing_ms: 22.215093822886637\n",
      "  time_since_restore: 18908.057121038437\n",
      "  time_this_iter_s: 66.76256346702576\n",
      "  time_total_s: 18908.057121038437\n",
      "  timestamp: 1575784156\n",
      "  timesteps_since_restore: 538000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 538000\n",
      "  training_iteration: 269\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18908 s, 269 iter, 538000 ts, 39.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-50-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.11462432012192\n",
      "  episode_reward_mean: 37.187464348441225\n",
      "  episode_reward_min: -38.95259980645812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7915.007\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04876481741666794\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3508169651031494\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013558757491409779\n",
      "        policy_loss: -0.008747905492782593\n",
      "        total_loss: 224.25328063964844\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 224.2612762451172\n",
      "    load_time_ms: 3.296\n",
      "    num_steps_sampled: 540000\n",
      "    num_steps_trained: 540000\n",
      "    sample_time_ms: 57704.583\n",
      "    update_time_ms: 55.249\n",
      "  iterations_since_restore: 270\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.77526881720429\n",
      "    ram_util_percent: 70.07096774193549\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.38768124627825\n",
      "    mean_inference_ms: 12.115439446092351\n",
      "    mean_processing_ms: 22.24062006023535\n",
      "  time_since_restore: 18973.84813094139\n",
      "  time_this_iter_s: 65.7910099029541\n",
      "  time_total_s: 18973.84813094139\n",
      "  timestamp: 1575784222\n",
      "  timesteps_since_restore: 540000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 540000\n",
      "  training_iteration: 270\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.8/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 18973 s, 270 iter, 540000 ts, 37.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-51-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.27919595216346\n",
      "  episode_reward_mean: 34.848707156222375\n",
      "  episode_reward_min: -38.95259980645812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7916.801\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.04876481741666794\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1626429557800293\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.22430254518985748\n",
      "        policy_loss: 0.05303002893924713\n",
      "        total_loss: 199.2456817626953\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 199.18173217773438\n",
      "    load_time_ms: 3.324\n",
      "    num_steps_sampled: 542000\n",
      "    num_steps_trained: 542000\n",
      "    sample_time_ms: 57728.7\n",
      "    update_time_ms: 55.458\n",
      "  iterations_since_restore: 271\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59677419354838\n",
      "    ram_util_percent: 70.12580645161293\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.79971658938993\n",
      "    mean_inference_ms: 12.075775706881124\n",
      "    mean_processing_ms: 22.20126422549137\n",
      "  time_since_restore: 19038.99504184723\n",
      "  time_this_iter_s: 65.14691090583801\n",
      "  time_total_s: 19038.99504184723\n",
      "  timestamp: 1575784287\n",
      "  timesteps_since_restore: 542000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 542000\n",
      "  training_iteration: 271\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19038 s, 271 iter, 542000 ts, 34.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.76614291840355\n",
      "  episode_reward_mean: 38.43925809405483\n",
      "  episode_reward_min: -38.95259980645812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7705.472\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.07314722239971161\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6214697360992432\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.29569533467292786\n",
      "        policy_loss: 0.06475149095058441\n",
      "        total_loss: 199.99453735351562\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 199.90809631347656\n",
      "    load_time_ms: 3.393\n",
      "    num_steps_sampled: 544000\n",
      "    num_steps_trained: 544000\n",
      "    sample_time_ms: 57692.009\n",
      "    update_time_ms: 54.664\n",
      "  iterations_since_restore: 272\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.65483870967742\n",
      "    ram_util_percent: 70.20860215053763\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.64618136176688\n",
      "    mean_inference_ms: 12.127977403162244\n",
      "    mean_processing_ms: 22.242640192024837\n",
      "  time_since_restore: 19104.121944904327\n",
      "  time_this_iter_s: 65.12690305709839\n",
      "  time_total_s: 19104.121944904327\n",
      "  timestamp: 1575784352\n",
      "  timesteps_since_restore: 544000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 544000\n",
      "  training_iteration: 272\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19104 s, 272 iter, 544000 ts, 38.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-53-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.47460853653625\n",
      "  episode_reward_mean: 38.68146333565835\n",
      "  episode_reward_min: -17.531639986939364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7714.682\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10972083359956741\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.737636089324951\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.38149353861808777\n",
      "        policy_loss: 0.07685766369104385\n",
      "        total_loss: 216.53143310546875\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 216.4127655029297\n",
      "    load_time_ms: 3.457\n",
      "    num_steps_sampled: 546000\n",
      "    num_steps_trained: 546000\n",
      "    sample_time_ms: 58043.378\n",
      "    update_time_ms: 52.544\n",
      "  iterations_since_restore: 273\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.36082474226804\n",
      "    ram_util_percent: 70.19072164948453\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.75848416078722\n",
      "    mean_inference_ms: 12.133268301051348\n",
      "    mean_processing_ms: 22.246955174133547\n",
      "  time_since_restore: 19171.8052546978\n",
      "  time_this_iter_s: 67.68330979347229\n",
      "  time_total_s: 19171.8052546978\n",
      "  timestamp: 1575784420\n",
      "  timesteps_since_restore: 546000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 546000\n",
      "  training_iteration: 273\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19171 s, 273 iter, 546000 ts, 38.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-54-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.47460853653625\n",
      "  episode_reward_mean: 38.93249615669196\n",
      "  episode_reward_min: -17.531639986939364\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7726.238\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16458125412464142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4442734718322754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019011927768588066\n",
      "        policy_loss: -0.009112573228776455\n",
      "        total_loss: 210.5203399658203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.52626037597656\n",
      "    load_time_ms: 3.371\n",
      "    num_steps_sampled: 548000\n",
      "    num_steps_trained: 548000\n",
      "    sample_time_ms: 58173.634\n",
      "    update_time_ms: 52.335\n",
      "  iterations_since_restore: 274\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.2840425531915\n",
      "    ram_util_percent: 70.11382978723402\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.10818629327758\n",
      "    mean_inference_ms: 12.153492329406578\n",
      "    mean_processing_ms: 22.26173957999046\n",
      "  time_since_restore: 19237.64390206337\n",
      "  time_this_iter_s: 65.83864736557007\n",
      "  time_total_s: 19237.64390206337\n",
      "  timestamp: 1575784486\n",
      "  timesteps_since_restore: 548000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 548000\n",
      "  training_iteration: 274\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19237 s, 274 iter, 548000 ts, 38.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-55-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.47460853653625\n",
      "  episode_reward_mean: 40.68503241578169\n",
      "  episode_reward_min: -11.25607768281472\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7724.031\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16458125412464142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.583263635635376\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017668284475803375\n",
      "        policy_loss: -0.003422863082960248\n",
      "        total_loss: 190.64540100097656\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 190.64596557617188\n",
      "    load_time_ms: 3.698\n",
      "    num_steps_sampled: 550000\n",
      "    num_steps_trained: 550000\n",
      "    sample_time_ms: 58330.255\n",
      "    update_time_ms: 51.694\n",
      "  iterations_since_restore: 275\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.61914893617022\n",
      "    ram_util_percent: 70.18297872340425\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.88578786653156\n",
      "    mean_inference_ms: 12.136095072237229\n",
      "    mean_processing_ms: 22.239994029425862\n",
      "  time_since_restore: 19303.81770157814\n",
      "  time_this_iter_s: 66.17379951477051\n",
      "  time_total_s: 19303.81770157814\n",
      "  timestamp: 1575784552\n",
      "  timesteps_since_restore: 550000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 550000\n",
      "  training_iteration: 275\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19303 s, 275 iter, 550000 ts, 40.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-56-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.33152750196119\n",
      "  episode_reward_mean: 43.54013297878577\n",
      "  episode_reward_min: -11.25607768281472\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7735.938\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16458125412464142\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2763633728027344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.16509844362735748\n",
      "        policy_loss: 0.04961930587887764\n",
      "        total_loss: 221.160400390625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 221.08363342285156\n",
      "    load_time_ms: 3.619\n",
      "    num_steps_sampled: 552000\n",
      "    num_steps_trained: 552000\n",
      "    sample_time_ms: 58186.436\n",
      "    update_time_ms: 52.151\n",
      "  iterations_since_restore: 276\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.87608695652175\n",
      "    ram_util_percent: 70.25108695652175\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.0567643753146\n",
      "    mean_inference_ms: 12.146205352172045\n",
      "    mean_processing_ms: 22.249346298495738\n",
      "  time_since_restore: 19368.27713751793\n",
      "  time_this_iter_s: 64.45943593978882\n",
      "  time_total_s: 19368.27713751793\n",
      "  timestamp: 1575784616\n",
      "  timesteps_since_restore: 552000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 552000\n",
      "  training_iteration: 276\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19368 s, 276 iter, 552000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-58-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.33152750196119\n",
      "  episode_reward_mean: 41.51297503267883\n",
      "  episode_reward_min: -11.25607768281472\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7745.683\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24687188863754272\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9757065773010254\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03365465998649597\n",
      "        policy_loss: -0.0042054015211761\n",
      "        total_loss: 183.88392639160156\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 183.8798065185547\n",
      "    load_time_ms: 3.585\n",
      "    num_steps_sampled: 554000\n",
      "    num_steps_trained: 554000\n",
      "    sample_time_ms: 58175.893\n",
      "    update_time_ms: 52.132\n",
      "  iterations_since_restore: 277\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1063829787234\n",
      "    ram_util_percent: 70.33510638297872\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.14947701563608\n",
      "    mean_inference_ms: 12.089550797340548\n",
      "    mean_processing_ms: 22.199290429137346\n",
      "  time_since_restore: 19433.7877907753\n",
      "  time_this_iter_s: 65.51065325737\n",
      "  time_total_s: 19433.7877907753\n",
      "  timestamp: 1575784682\n",
      "  timesteps_since_restore: 554000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 554000\n",
      "  training_iteration: 277\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19433 s, 277 iter, 554000 ts, 41.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_00-59-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.33152750196119\n",
      "  episode_reward_mean: 43.235891193466514\n",
      "  episode_reward_min: -8.844404003186868\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7720.468\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24687188863754272\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.435225009918213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028827594593167305\n",
      "        policy_loss: -0.0025656400248408318\n",
      "        total_loss: 206.52490234375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 206.5203399658203\n",
      "    load_time_ms: 3.563\n",
      "    num_steps_sampled: 556000\n",
      "    num_steps_trained: 556000\n",
      "    sample_time_ms: 58197.057\n",
      "    update_time_ms: 53.004\n",
      "  iterations_since_restore: 278\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.41354166666667\n",
      "    ram_util_percent: 70.41145833333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.11056907739075\n",
      "    mean_inference_ms: 12.150813677359137\n",
      "    mean_processing_ms: 22.247559445702333\n",
      "  time_since_restore: 19501.43467068672\n",
      "  time_this_iter_s: 67.64687991142273\n",
      "  time_total_s: 19501.43467068672\n",
      "  timestamp: 1575784750\n",
      "  timesteps_since_restore: 556000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 556000\n",
      "  training_iteration: 278\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19501 s, 278 iter, 556000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-00-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.33152750196119\n",
      "  episode_reward_mean: 45.34171217096151\n",
      "  episode_reward_min: -0.03220120524842595\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7706.901\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24687188863754272\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6296024322509766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0607110895216465\n",
      "        policy_loss: 0.013002315536141396\n",
      "        total_loss: 186.80178833007812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 186.7738037109375\n",
      "    load_time_ms: 3.647\n",
      "    num_steps_sampled: 558000\n",
      "    num_steps_trained: 558000\n",
      "    sample_time_ms: 58145.993\n",
      "    update_time_ms: 52.04\n",
      "  iterations_since_restore: 279\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.85263157894737\n",
      "    ram_util_percent: 70.48105263157893\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 145.082887683844\n",
      "    mean_inference_ms: 12.149567201345526\n",
      "    mean_processing_ms: 22.25523772977351\n",
      "  time_since_restore: 19567.551465511322\n",
      "  time_this_iter_s: 66.11679482460022\n",
      "  time_total_s: 19567.551465511322\n",
      "  timestamp: 1575784816\n",
      "  timesteps_since_restore: 558000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 558000\n",
      "  training_iteration: 279\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19567 s, 279 iter, 558000 ts, 45.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-01-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.33152750196119\n",
      "  episode_reward_mean: 46.27712651303128\n",
      "  episode_reward_min: -1.7028019435545256\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7713.281\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3703078329563141\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2843570709228516\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00821216031908989\n",
      "        policy_loss: -0.01496023591607809\n",
      "        total_loss: 197.61090087890625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 197.62290954589844\n",
      "    load_time_ms: 3.673\n",
      "    num_steps_sampled: 560000\n",
      "    num_steps_trained: 560000\n",
      "    sample_time_ms: 58159.787\n",
      "    update_time_ms: 51.766\n",
      "  iterations_since_restore: 280\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.78617021276595\n",
      "    ram_util_percent: 70.381914893617\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.2372757697539\n",
      "    mean_inference_ms: 12.097873719086728\n",
      "    mean_processing_ms: 22.216732841463465\n",
      "  time_since_restore: 19633.540730714798\n",
      "  time_this_iter_s: 65.98926520347595\n",
      "  time_total_s: 19633.540730714798\n",
      "  timestamp: 1575784882\n",
      "  timesteps_since_restore: 560000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 560000\n",
      "  training_iteration: 280\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19633 s, 280 iter, 560000 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-02-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.75395695027935\n",
      "  episode_reward_mean: 45.97786662771329\n",
      "  episode_reward_min: -1.7028019435545256\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7715.823\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.18515391647815704\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9462623596191406\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029550014063715935\n",
      "        policy_loss: -0.007091541308909655\n",
      "        total_loss: 194.4593505859375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 194.46096801757812\n",
      "    load_time_ms: 3.725\n",
      "    num_steps_sampled: 562000\n",
      "    num_steps_trained: 562000\n",
      "    sample_time_ms: 58231.916\n",
      "    update_time_ms: 51.506\n",
      "  iterations_since_restore: 281\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.28617021276595\n",
      "    ram_util_percent: 70.37553191489361\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.39441892370277\n",
      "    mean_inference_ms: 12.105713534917802\n",
      "    mean_processing_ms: 22.220152026071265\n",
      "  time_since_restore: 19699.426818609238\n",
      "  time_this_iter_s: 65.8860878944397\n",
      "  time_total_s: 19699.426818609238\n",
      "  timestamp: 1575784948\n",
      "  timesteps_since_restore: 562000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 562000\n",
      "  training_iteration: 281\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19699 s, 281 iter, 562000 ts, 46 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-03-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.75395695027935\n",
      "  episode_reward_mean: 46.92061284913799\n",
      "  episode_reward_min: -10.751409205281986\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7697.447\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.18515391647815704\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3776655197143555\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07961094379425049\n",
      "        policy_loss: 0.01233292929828167\n",
      "        total_loss: 227.54812622070312\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 227.52105712890625\n",
      "    load_time_ms: 3.591\n",
      "    num_steps_sampled: 564000\n",
      "    num_steps_trained: 564000\n",
      "    sample_time_ms: 58266.761\n",
      "    update_time_ms: 51.99\n",
      "  iterations_since_restore: 282\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.23870967741937\n",
      "    ram_util_percent: 70.447311827957\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.37000601593397\n",
      "    mean_inference_ms: 12.103023612538049\n",
      "    mean_processing_ms: 22.21839194118759\n",
      "  time_since_restore: 19764.737010002136\n",
      "  time_this_iter_s: 65.31019139289856\n",
      "  time_total_s: 19764.737010002136\n",
      "  timestamp: 1575785013\n",
      "  timesteps_since_restore: 564000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 564000\n",
      "  training_iteration: 282\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19764 s, 282 iter, 564000 ts, 46.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-04-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.75395695027935\n",
      "  episode_reward_mean: 46.766843868707966\n",
      "  episode_reward_min: -10.751409205281986\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7681.997\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2777308523654938\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9923423528671265\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.2532155513763428\n",
      "        policy_loss: 0.07918065041303635\n",
      "        total_loss: 193.7114715576172\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 193.5619659423828\n",
      "    load_time_ms: 3.582\n",
      "    num_steps_sampled: 566000\n",
      "    num_steps_trained: 566000\n",
      "    sample_time_ms: 58259.281\n",
      "    update_time_ms: 52.388\n",
      "  iterations_since_restore: 283\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.87395833333333\n",
      "    ram_util_percent: 70.52187500000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.35031469350662\n",
      "    mean_inference_ms: 12.03652046412789\n",
      "    mean_processing_ms: 22.163898225178073\n",
      "  time_since_restore: 19832.19700741768\n",
      "  time_this_iter_s: 67.4599974155426\n",
      "  time_total_s: 19832.19700741768\n",
      "  timestamp: 1575785081\n",
      "  timesteps_since_restore: 566000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 566000\n",
      "  training_iteration: 283\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19832 s, 283 iter, 566000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-05-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.77117751847598\n",
      "  episode_reward_mean: 46.90005213500294\n",
      "  episode_reward_min: -10.751409205281986\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7675.569\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41659629344940186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.222367763519287\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028415173292160034\n",
      "        policy_loss: -0.002057858509942889\n",
      "        total_loss: 217.37594604492188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 217.3660888671875\n",
      "    load_time_ms: 3.529\n",
      "    num_steps_sampled: 568000\n",
      "    num_steps_trained: 568000\n",
      "    sample_time_ms: 58159.936\n",
      "    update_time_ms: 52.443\n",
      "  iterations_since_restore: 284\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86881720430107\n",
      "    ram_util_percent: 70.60860215053765\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.72916633822237\n",
      "    mean_inference_ms: 11.997359874536208\n",
      "    mean_processing_ms: 22.115290487901287\n",
      "  time_since_restore: 19896.991362810135\n",
      "  time_this_iter_s: 64.79435539245605\n",
      "  time_total_s: 19896.991362810135\n",
      "  timestamp: 1575785145\n",
      "  timesteps_since_restore: 568000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 568000\n",
      "  training_iteration: 284\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19896 s, 284 iter, 568000 ts, 46.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-06-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.77117751847598\n",
      "  episode_reward_mean: 48.01491723895048\n",
      "  episode_reward_min: -10.751409205281986\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7682.436\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41659629344940186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7970852851867676\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02582743763923645\n",
      "        policy_loss: -0.007255479693412781\n",
      "        total_loss: 201.718994140625\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 201.7154998779297\n",
      "    load_time_ms: 3.224\n",
      "    num_steps_sampled: 570000\n",
      "    num_steps_trained: 570000\n",
      "    sample_time_ms: 58102.581\n",
      "    update_time_ms: 53.422\n",
      "  iterations_since_restore: 285\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.41075268817205\n",
      "    ram_util_percent: 70.66881720430105\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.76330059805147\n",
      "    mean_inference_ms: 11.999443196022439\n",
      "    mean_processing_ms: 22.110560474399872\n",
      "  time_since_restore: 19962.684329986572\n",
      "  time_this_iter_s: 65.69296717643738\n",
      "  time_total_s: 19962.684329986572\n",
      "  timestamp: 1575785211\n",
      "  timesteps_since_restore: 570000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 570000\n",
      "  training_iteration: 285\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 19962 s, 285 iter, 570000 ts, 48 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-07-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.77117751847598\n",
      "  episode_reward_mean: 49.10695382887453\n",
      "  episode_reward_min: -10.751409205281986\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7681.49\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41659629344940186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.080573797225952\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.037948980927467346\n",
      "        policy_loss: 0.010964469984173775\n",
      "        total_loss: 209.66098022460938\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.63421630859375\n",
      "    load_time_ms: 3.33\n",
      "    num_steps_sampled: 572000\n",
      "    num_steps_trained: 572000\n",
      "    sample_time_ms: 58271.978\n",
      "    update_time_ms: 52.79\n",
      "  iterations_since_restore: 286\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.26526315789475\n",
      "    ram_util_percent: 70.69789473684212\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.89322601658847\n",
      "    mean_inference_ms: 12.005764464229328\n",
      "    mean_processing_ms: 22.105982363798542\n",
      "  time_since_restore: 20028.81779885292\n",
      "  time_this_iter_s: 66.13346886634827\n",
      "  time_total_s: 20028.81779885292\n",
      "  timestamp: 1575785277\n",
      "  timesteps_since_restore: 572000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 572000\n",
      "  training_iteration: 286\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20028 s, 286 iter, 572000 ts, 49.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-09-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.77117751847598\n",
      "  episode_reward_mean: 47.72847018320236\n",
      "  episode_reward_min: -4.091442532651635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7680.323\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41659629344940186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.335606336593628\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015069637447595596\n",
      "        policy_loss: -0.007607810664921999\n",
      "        total_loss: 211.60736083984375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 211.6086883544922\n",
      "    load_time_ms: 3.337\n",
      "    num_steps_sampled: 574000\n",
      "    num_steps_trained: 574000\n",
      "    sample_time_ms: 58229.538\n",
      "    update_time_ms: 54.437\n",
      "  iterations_since_restore: 287\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86344086021505\n",
      "    ram_util_percent: 70.6247311827957\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.97737311990733\n",
      "    mean_inference_ms: 12.072589267203144\n",
      "    mean_processing_ms: 22.156032662417385\n",
      "  time_since_restore: 20093.898252487183\n",
      "  time_this_iter_s: 65.08045363426208\n",
      "  time_total_s: 20093.898252487183\n",
      "  timestamp: 1575785342\n",
      "  timesteps_since_restore: 574000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 574000\n",
      "  training_iteration: 287\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20093 s, 287 iter, 574000 ts, 47.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 94.77117751847598\n",
      "  episode_reward_mean: 47.125542603978126\n",
      "  episode_reward_min: -4.091442532651635\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7676.303\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41659629344940186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.282444953918457\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.036339469254016876\n",
      "        policy_loss: -0.0017397765768691897\n",
      "        total_loss: 237.49501037597656\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 237.48165893554688\n",
      "    load_time_ms: 3.383\n",
      "    num_steps_sampled: 576000\n",
      "    num_steps_trained: 576000\n",
      "    sample_time_ms: 58145.128\n",
      "    update_time_ms: 54.139\n",
      "  iterations_since_restore: 288\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.92736842105263\n",
      "    ram_util_percent: 70.69789473684213\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.8150772628682\n",
      "    mean_inference_ms: 12.063694633992968\n",
      "    mean_processing_ms: 22.14225644534473\n",
      "  time_since_restore: 20160.65937423706\n",
      "  time_this_iter_s: 66.76112174987793\n",
      "  time_total_s: 20160.65937423706\n",
      "  timestamp: 1575785409\n",
      "  timesteps_since_restore: 576000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 576000\n",
      "  training_iteration: 288\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20160 s, 288 iter, 576000 ts, 47.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-11-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35232304203723\n",
      "  episode_reward_mean: 47.25313707422118\n",
      "  episode_reward_min: -29.640072608275922\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7688.56\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41659629344940186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4803578853607178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04956141114234924\n",
      "        policy_loss: 0.025736235082149506\n",
      "        total_loss: 247.18411254882812\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 247.13775634765625\n",
      "    load_time_ms: 3.226\n",
      "    num_steps_sampled: 578000\n",
      "    num_steps_trained: 578000\n",
      "    sample_time_ms: 58108.809\n",
      "    update_time_ms: 54.769\n",
      "  iterations_since_restore: 289\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.13510638297873\n",
      "    ram_util_percent: 70.7904255319149\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.10933085583136\n",
      "    mean_inference_ms: 12.079595623600444\n",
      "    mean_processing_ms: 22.154852028273446\n",
      "  time_since_restore: 20226.54070353508\n",
      "  time_this_iter_s: 65.88132929801941\n",
      "  time_total_s: 20226.54070353508\n",
      "  timestamp: 1575785475\n",
      "  timesteps_since_restore: 578000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 578000\n",
      "  training_iteration: 289\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20226 s, 289 iter, 578000 ts, 47.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-12-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35232304203723\n",
      "  episode_reward_mean: 45.37338928500254\n",
      "  episode_reward_min: -29.640072608275922\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7674.074\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.6248944401741028\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7098605632781982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.00484665110707283\n",
      "        policy_loss: -0.008387565612792969\n",
      "        total_loss: 238.54275512695312\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 238.5481719970703\n",
      "    load_time_ms: 3.431\n",
      "    num_steps_sampled: 580000\n",
      "    num_steps_trained: 580000\n",
      "    sample_time_ms: 58065.412\n",
      "    update_time_ms: 54.989\n",
      "  iterations_since_restore: 290\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.74516129032257\n",
      "    ram_util_percent: 70.80537634408601\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.46334920800732\n",
      "    mean_inference_ms: 12.099950275999838\n",
      "    mean_processing_ms: 22.168008672356642\n",
      "  time_since_restore: 20291.96791267395\n",
      "  time_this_iter_s: 65.42720913887024\n",
      "  time_total_s: 20291.96791267395\n",
      "  timestamp: 1575785541\n",
      "  timesteps_since_restore: 580000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 580000\n",
      "  training_iteration: 290\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20291 s, 290 iter, 580000 ts, 45.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-13-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35232304203723\n",
      "  episode_reward_mean: 44.25400431693408\n",
      "  episode_reward_min: -29.640072608275922\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7674.493\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3124472200870514\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7489569187164307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023200275376439095\n",
      "        policy_loss: 0.0005996985710225999\n",
      "        total_loss: 220.63343811035156\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 220.62559509277344\n",
      "    load_time_ms: 3.772\n",
      "    num_steps_sampled: 582000\n",
      "    num_steps_trained: 582000\n",
      "    sample_time_ms: 57971.706\n",
      "    update_time_ms: 55.327\n",
      "  iterations_since_restore: 291\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.85483870967742\n",
      "    ram_util_percent: 70.89032258064519\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.83524679109985\n",
      "    mean_inference_ms: 12.06201506875973\n",
      "    mean_processing_ms: 22.141944306090892\n",
      "  time_since_restore: 20356.928527116776\n",
      "  time_this_iter_s: 64.96061444282532\n",
      "  time_total_s: 20356.928527116776\n",
      "  timestamp: 1575785606\n",
      "  timesteps_since_restore: 582000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 582000\n",
      "  training_iteration: 291\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20356 s, 291 iter, 582000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-14-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35232304203723\n",
      "  episode_reward_mean: 44.84469580936343\n",
      "  episode_reward_min: -29.640072608275922\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7705.826\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3124472200870514\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1217448711395264\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.046476319432258606\n",
      "        policy_loss: 0.0008096504025161266\n",
      "        total_loss: 224.48080444335938\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 224.46543884277344\n",
      "    load_time_ms: 3.715\n",
      "    num_steps_sampled: 584000\n",
      "    num_steps_trained: 584000\n",
      "    sample_time_ms: 57916.05\n",
      "    update_time_ms: 54.708\n",
      "  iterations_since_restore: 292\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.52473118279569\n",
      "    ram_util_percent: 70.97204301075271\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.53331538993336\n",
      "    mean_inference_ms: 12.04584340156533\n",
      "    mean_processing_ms: 22.13192859656858\n",
      "  time_since_restore: 20421.972802877426\n",
      "  time_this_iter_s: 65.04427576065063\n",
      "  time_total_s: 20421.972802877426\n",
      "  timestamp: 1575785671\n",
      "  timesteps_since_restore: 584000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 584000\n",
      "  training_iteration: 292\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20421 s, 292 iter, 584000 ts, 44.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-15-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.6444531305261\n",
      "  episode_reward_mean: 45.45509081417844\n",
      "  episode_reward_min: -29.640072608275922\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7704.97\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4686708450317383\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9473891258239746\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021709589287638664\n",
      "        policy_loss: -0.006505382712930441\n",
      "        total_loss: 234.3131103515625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 234.30931091308594\n",
      "    load_time_ms: 3.671\n",
      "    num_steps_sampled: 586000\n",
      "    num_steps_trained: 586000\n",
      "    sample_time_ms: 58110.507\n",
      "    update_time_ms: 54.128\n",
      "  iterations_since_restore: 293\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.10404040404039\n",
      "    ram_util_percent: 70.86161616161614\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.5718306559996\n",
      "    mean_inference_ms: 12.048890577952283\n",
      "    mean_processing_ms: 22.128005004278442\n",
      "  time_since_restore: 20491.359924316406\n",
      "  time_this_iter_s: 69.3871214389801\n",
      "  time_total_s: 20491.359924316406\n",
      "  timestamp: 1575785740\n",
      "  timesteps_since_restore: 586000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 586000\n",
      "  training_iteration: 293\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20491 s, 293 iter, 586000 ts, 45.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-16-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.6444531305261\n",
      "  episode_reward_mean: 42.74874832995658\n",
      "  episode_reward_min: -16.415524382898404\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7743.452\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4686708450317383\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2322733402252197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05905265733599663\n",
      "        policy_loss: -0.0069581326097249985\n",
      "        total_loss: 228.79942321777344\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 228.77871704101562\n",
      "    load_time_ms: 3.645\n",
      "    num_steps_sampled: 588000\n",
      "    num_steps_trained: 588000\n",
      "    sample_time_ms: 58316.103\n",
      "    update_time_ms: 53.733\n",
      "  iterations_since_restore: 294\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.80104166666668\n",
      "    ram_util_percent: 70.83958333333332\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.22915775178524\n",
      "    mean_inference_ms: 12.029348713746726\n",
      "    mean_processing_ms: 22.10528568817324\n",
      "  time_since_restore: 20558.579889059067\n",
      "  time_this_iter_s: 67.21996474266052\n",
      "  time_total_s: 20558.579889059067\n",
      "  timestamp: 1575785807\n",
      "  timesteps_since_restore: 588000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 588000\n",
      "  training_iteration: 294\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20558 s, 294 iter, 588000 ts, 42.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-17-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.6444531305261\n",
      "  episode_reward_mean: 44.531404045274904\n",
      "  episode_reward_min: -3.52178770194194\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7763.578\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7030062675476074\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.47843861579895\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006700785364955664\n",
      "        policy_loss: -0.002814232837408781\n",
      "        total_loss: 196.5897216796875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 196.58779907226562\n",
      "    load_time_ms: 3.606\n",
      "    num_steps_sampled: 590000\n",
      "    num_steps_trained: 590000\n",
      "    sample_time_ms: 58236.9\n",
      "    update_time_ms: 53.556\n",
      "  iterations_since_restore: 295\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.8763440860215\n",
      "    ram_util_percent: 70.9505376344086\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.23221367103514\n",
      "    mean_inference_ms: 12.029235146540517\n",
      "    mean_processing_ms: 22.09614483137938\n",
      "  time_since_restore: 20623.665856599808\n",
      "  time_this_iter_s: 65.08596754074097\n",
      "  time_total_s: 20623.665856599808\n",
      "  timestamp: 1575785873\n",
      "  timesteps_since_restore: 590000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 590000\n",
      "  training_iteration: 295\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20623 s, 295 iter, 590000 ts, 44.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-18-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.6444531305261\n",
      "  episode_reward_mean: 43.43365780720238\n",
      "  episode_reward_min: -3.52178770194194\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7767.282\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3515031337738037\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6832354068756104\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008930746465921402\n",
      "        policy_loss: -0.013989012688398361\n",
      "        total_loss: 215.87078857421875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 215.88162231445312\n",
      "    load_time_ms: 3.548\n",
      "    num_steps_sampled: 592000\n",
      "    num_steps_trained: 592000\n",
      "    sample_time_ms: 58161.726\n",
      "    update_time_ms: 53.659\n",
      "  iterations_since_restore: 296\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.51827956989248\n",
      "    ram_util_percent: 71.00645161290323\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.26754362188132\n",
      "    mean_inference_ms: 12.031584433429058\n",
      "    mean_processing_ms: 22.09250618088543\n",
      "  time_since_restore: 20689.089797735214\n",
      "  time_this_iter_s: 65.4239411354065\n",
      "  time_total_s: 20689.089797735214\n",
      "  timestamp: 1575785938\n",
      "  timesteps_since_restore: 592000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 592000\n",
      "  training_iteration: 296\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20689 s, 296 iter, 592000 ts, 43.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-20-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.6444531305261\n",
      "  episode_reward_mean: 42.521369755085146\n",
      "  episode_reward_min: -3.52178770194194\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8018.163\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17575156688690186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.293858051300049\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010241122916340828\n",
      "        policy_loss: -0.016707325354218483\n",
      "        total_loss: 201.05606079101562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 201.07093811035156\n",
      "    load_time_ms: 3.764\n",
      "    num_steps_sampled: 594000\n",
      "    num_steps_trained: 594000\n",
      "    sample_time_ms: 58305.399\n",
      "    update_time_ms: 52.01\n",
      "  iterations_since_restore: 297\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.36868686868686\n",
      "    ram_util_percent: 71.0969696969697\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.48802564577306\n",
      "    mean_inference_ms: 12.041766773419626\n",
      "    mean_processing_ms: 22.096531356920405\n",
      "  time_since_restore: 20758.096953868866\n",
      "  time_this_iter_s: 69.00715613365173\n",
      "  time_total_s: 20758.096953868866\n",
      "  timestamp: 1575786007\n",
      "  timesteps_since_restore: 594000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 594000\n",
      "  training_iteration: 297\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20758 s, 297 iter, 594000 ts, 42.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-21-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.07390582022629\n",
      "  episode_reward_mean: 42.44072641466412\n",
      "  episode_reward_min: -3.52178770194194\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8070.616\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17575156688690186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5343565940856934\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027154995128512383\n",
      "        policy_loss: -0.021599700674414635\n",
      "        total_loss: 218.49993896484375\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 218.51683044433594\n",
      "    load_time_ms: 3.747\n",
      "    num_steps_sampled: 596000\n",
      "    num_steps_trained: 596000\n",
      "    sample_time_ms: 58086.442\n",
      "    update_time_ms: 52.552\n",
      "  iterations_since_restore: 298\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.84239130434783\n",
      "    ram_util_percent: 71.19891304347826\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.747768048016\n",
      "    mean_inference_ms: 12.054611303835769\n",
      "    mean_processing_ms: 22.112824762305973\n",
      "  time_since_restore: 20823.19711637497\n",
      "  time_this_iter_s: 65.10016250610352\n",
      "  time_total_s: 20823.19711637497\n",
      "  timestamp: 1575786072\n",
      "  timesteps_since_restore: 596000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 596000\n",
      "  training_iteration: 298\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20823 s, 298 iter, 596000 ts, 42.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-22-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.07390582022629\n",
      "  episode_reward_mean: 45.43326020833315\n",
      "  episode_reward_min: -1.631740989760613\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 5980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8069.524\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17575156688690186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1609716415405273\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006931775249540806\n",
      "        policy_loss: -0.007543080952018499\n",
      "        total_loss: 219.4383087158203\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 219.4446258544922\n",
      "    load_time_ms: 3.738\n",
      "    num_steps_sampled: 598000\n",
      "    num_steps_trained: 598000\n",
      "    sample_time_ms: 57987.243\n",
      "    update_time_ms: 53.294\n",
      "  iterations_since_restore: 299\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.30967741935483\n",
      "    ram_util_percent: 71.21612903225804\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.79453416694025\n",
      "    mean_inference_ms: 12.0543691666138\n",
      "    mean_processing_ms: 22.114052229588733\n",
      "  time_since_restore: 20888.08593583107\n",
      "  time_this_iter_s: 64.88881945610046\n",
      "  time_total_s: 20888.08593583107\n",
      "  timestamp: 1575786137\n",
      "  timesteps_since_restore: 598000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 598000\n",
      "  training_iteration: 299\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20888 s, 299 iter, 598000 ts, 45.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-23-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.07390582022629\n",
      "  episode_reward_mean: 44.37651802717674\n",
      "  episode_reward_min: -20.624290843245756\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8079.202\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7413954734802246\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02193797007203102\n",
      "        policy_loss: -0.02049931138753891\n",
      "        total_loss: 229.15786743164062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 229.17649841308594\n",
      "    load_time_ms: 3.899\n",
      "    num_steps_sampled: 600000\n",
      "    num_steps_trained: 600000\n",
      "    sample_time_ms: 57982.124\n",
      "    update_time_ms: 53.873\n",
      "  iterations_since_restore: 300\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.73440860215052\n",
      "    ram_util_percent: 71.07419354838709\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.3032078698286\n",
      "    mean_inference_ms: 12.02310434344826\n",
      "    mean_processing_ms: 22.09526259301694\n",
      "  time_since_restore: 20953.556548833847\n",
      "  time_this_iter_s: 65.4706130027771\n",
      "  time_total_s: 20953.556548833847\n",
      "  timestamp: 1575786203\n",
      "  timesteps_since_restore: 600000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 600000\n",
      "  training_iteration: 300\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 20953 s, 300 iter, 600000 ts, 44.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-24-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.07390582022629\n",
      "  episode_reward_mean: 44.95008385852093\n",
      "  episode_reward_min: -20.624290843245756\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8084.211\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.48356294631958\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01202645804733038\n",
      "        policy_loss: -0.013747533783316612\n",
      "        total_loss: 228.94972229003906\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 228.9625244140625\n",
      "    load_time_ms: 3.479\n",
      "    num_steps_sampled: 602000\n",
      "    num_steps_trained: 602000\n",
      "    sample_time_ms: 58126.187\n",
      "    update_time_ms: 52.89\n",
      "  iterations_since_restore: 301\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.77578947368421\n",
      "    ram_util_percent: 71.15789473684211\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.28140557629325\n",
      "    mean_inference_ms: 12.020646463988928\n",
      "    mean_processing_ms: 22.09160702506467\n",
      "  time_since_restore: 21020.01215839386\n",
      "  time_this_iter_s: 66.45560956001282\n",
      "  time_total_s: 21020.01215839386\n",
      "  timestamp: 1575786269\n",
      "  timesteps_since_restore: 602000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 602000\n",
      "  training_iteration: 301\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21020 s, 301 iter, 602000 ts, 45 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 01:25:38,286\tWARNING util.py:145 -- The `process_trial` operation took 0.10903644561767578 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-25-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.9668034167035\n",
      "  episode_reward_mean: 46.83074935128216\n",
      "  episode_reward_min: -20.624290843245756\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8339.282\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4306716918945312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010581533424556255\n",
      "        policy_loss: -0.012990077957510948\n",
      "        total_loss: 232.55853271484375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 232.570556640625\n",
      "    load_time_ms: 3.553\n",
      "    num_steps_sampled: 604000\n",
      "    num_steps_trained: 604000\n",
      "    sample_time_ms: 58209.7\n",
      "    update_time_ms: 52.904\n",
      "  iterations_since_restore: 302\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.75306122448978\n",
      "    ram_util_percent: 71.23979591836736\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.45160517047037\n",
      "    mean_inference_ms: 11.96870414142197\n",
      "    mean_processing_ms: 22.043842084636143\n",
      "  time_since_restore: 21088.54184770584\n",
      "  time_this_iter_s: 68.5296893119812\n",
      "  time_total_s: 21088.54184770584\n",
      "  timestamp: 1575786338\n",
      "  timesteps_since_restore: 604000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 604000\n",
      "  training_iteration: 302\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21088 s, 302 iter, 604000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-26-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.9668034167035\n",
      "  episode_reward_mean: 44.83466537255449\n",
      "  episode_reward_min: -20.624290843245756\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8262.003\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9954581260681152\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021233344450592995\n",
      "        policy_loss: -0.002771311206743121\n",
      "        total_loss: 242.39956665039062\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 242.40037536621094\n",
      "    load_time_ms: 5.101\n",
      "    num_steps_sampled: 606000\n",
      "    num_steps_trained: 606000\n",
      "    sample_time_ms: 57890.948\n",
      "    update_time_ms: 59.658\n",
      "  iterations_since_restore: 303\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.94574468085108\n",
      "    ram_util_percent: 71.3468085106383\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.34510643204584\n",
      "    mean_inference_ms: 11.962847488630969\n",
      "    mean_processing_ms: 22.03301059627854\n",
      "  time_since_restore: 21154.043370962143\n",
      "  time_this_iter_s: 65.50152325630188\n",
      "  time_total_s: 21154.043370962143\n",
      "  timestamp: 1575786403\n",
      "  timesteps_since_restore: 606000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 606000\n",
      "  training_iteration: 303\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21154 s, 303 iter, 606000 ts, 44.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-27-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.9668034167035\n",
      "  episode_reward_mean: 41.17954563188441\n",
      "  episode_reward_min: -30.085675784000735\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8231.556\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.287428379058838\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.022694844752550125\n",
      "        policy_loss: -0.02145545557141304\n",
      "        total_loss: 286.2969665527344\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 286.3164978027344\n",
      "    load_time_ms: 5.188\n",
      "    num_steps_sampled: 608000\n",
      "    num_steps_trained: 608000\n",
      "    sample_time_ms: 57726.344\n",
      "    update_time_ms: 60.689\n",
      "  iterations_since_restore: 304\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59677419354838\n",
      "    ram_util_percent: 71.42043010752687\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.24461700565223\n",
      "    mean_inference_ms: 11.957547580209793\n",
      "    mean_processing_ms: 22.022718037109733\n",
      "  time_since_restore: 21219.33314037323\n",
      "  time_this_iter_s: 65.28976941108704\n",
      "  time_total_s: 21219.33314037323\n",
      "  timestamp: 1575786469\n",
      "  timesteps_since_restore: 608000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 608000\n",
      "  training_iteration: 304\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21219 s, 304 iter, 608000 ts, 41.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.9668034167035\n",
      "  episode_reward_mean: 42.26909900498972\n",
      "  episode_reward_min: -30.085675784000735\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8214.83\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.190182685852051\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03138982877135277\n",
      "        policy_loss: 0.00014416694466490299\n",
      "        total_loss: 216.14175415039062\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 216.13885498046875\n",
      "    load_time_ms: 5.191\n",
      "    num_steps_sampled: 610000\n",
      "    num_steps_trained: 610000\n",
      "    sample_time_ms: 57912.495\n",
      "    update_time_ms: 61.047\n",
      "  iterations_since_restore: 305\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.61999999999999\n",
      "    ram_util_percent: 71.45368421052629\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.5465205285492\n",
      "    mean_inference_ms: 11.97703686369715\n",
      "    mean_processing_ms: 22.04066262219799\n",
      "  time_since_restore: 21286.130027532578\n",
      "  time_this_iter_s: 66.79688715934753\n",
      "  time_total_s: 21286.130027532578\n",
      "  timestamp: 1575786535\n",
      "  timesteps_since_restore: 610000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 610000\n",
      "  training_iteration: 305\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21286 s, 305 iter, 610000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-30-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.9668034167035\n",
      "  episode_reward_mean: 41.651634711609155\n",
      "  episode_reward_min: -30.085675784000735\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8211.023\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08787578344345093\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8025760650634766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0873502716422081\n",
      "        policy_loss: 0.012680523097515106\n",
      "        total_loss: 228.27392578125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 228.25357055664062\n",
      "    load_time_ms: 5.106\n",
      "    num_steps_sampled: 612000\n",
      "    num_steps_trained: 612000\n",
      "    sample_time_ms: 58005.612\n",
      "    update_time_ms: 63.131\n",
      "  iterations_since_restore: 306\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.69578947368421\n",
      "    ram_util_percent: 71.46105263157898\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.72750887846576\n",
      "    mean_inference_ms: 11.987132326698134\n",
      "    mean_processing_ms: 22.051119765523108\n",
      "  time_since_restore: 21352.470033168793\n",
      "  time_this_iter_s: 66.34000563621521\n",
      "  time_total_s: 21352.470033168793\n",
      "  timestamp: 1575786602\n",
      "  timesteps_since_restore: 612000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 612000\n",
      "  training_iteration: 306\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 5.9/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21352 s, 306 iter, 612000 ts, 41.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-31-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.49047550116433\n",
      "  episode_reward_mean: 40.97389535251486\n",
      "  episode_reward_min: -30.085675784000735\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8214.928\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1318136751651764\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5773539543151855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.025855479761958122\n",
      "        policy_loss: -0.010473941452801228\n",
      "        total_loss: 201.04933166503906\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 201.05642700195312\n",
      "    load_time_ms: 4.889\n",
      "    num_steps_sampled: 614000\n",
      "    num_steps_trained: 614000\n",
      "    sample_time_ms: 57902.384\n",
      "    update_time_ms: 62.737\n",
      "  iterations_since_restore: 307\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.76701030927835\n",
      "    ram_util_percent: 71.41855670103095\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.35479080024703\n",
      "    mean_inference_ms: 12.025439888243504\n",
      "    mean_processing_ms: 22.088242293392636\n",
      "  time_since_restore: 21420.48320055008\n",
      "  time_this_iter_s: 68.01316738128662\n",
      "  time_total_s: 21420.48320055008\n",
      "  timestamp: 1575786670\n",
      "  timesteps_since_restore: 614000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 614000\n",
      "  training_iteration: 307\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21420 s, 307 iter, 614000 ts, 41 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-32-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.32627095771258\n",
      "  episode_reward_mean: 41.57439904579226\n",
      "  episode_reward_min: -30.085675784000735\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7961.427\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1318136751651764\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4579696655273438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03148595243692398\n",
      "        policy_loss: -0.0047604721039533615\n",
      "        total_loss: 195.58546447753906\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 195.58602905273438\n",
      "    load_time_ms: 4.933\n",
      "    num_steps_sampled: 616000\n",
      "    num_steps_trained: 616000\n",
      "    sample_time_ms: 58126.748\n",
      "    update_time_ms: 62.088\n",
      "  iterations_since_restore: 308\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.06521739130434\n",
      "    ram_util_percent: 71.48695652173915\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.66899899136274\n",
      "    mean_inference_ms: 12.042461744106127\n",
      "    mean_processing_ms: 22.09047916311148\n",
      "  time_since_restore: 21485.285676717758\n",
      "  time_this_iter_s: 64.80247616767883\n",
      "  time_total_s: 21485.285676717758\n",
      "  timestamp: 1575786735\n",
      "  timesteps_since_restore: 616000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 616000\n",
      "  training_iteration: 308\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21485 s, 308 iter, 616000 ts, 41.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-33-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35228383248885\n",
      "  episode_reward_mean: 44.27681224020571\n",
      "  episode_reward_min: -22.037642029227897\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7936.915\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1318136751651764\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1864006519317627\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.23671264946460724\n",
      "        policy_loss: 0.07070380449295044\n",
      "        total_loss: 211.71702575683594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 211.6151123046875\n",
      "    load_time_ms: 4.982\n",
      "    num_steps_sampled: 618000\n",
      "    num_steps_trained: 618000\n",
      "    sample_time_ms: 58221.807\n",
      "    update_time_ms: 61.592\n",
      "  iterations_since_restore: 309\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.38829787234043\n",
      "    ram_util_percent: 71.55744680851066\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.98006248250502\n",
      "    mean_inference_ms: 12.059649099061344\n",
      "    mean_processing_ms: 22.10617061056998\n",
      "  time_since_restore: 21550.87743496895\n",
      "  time_this_iter_s: 65.59175825119019\n",
      "  time_total_s: 21550.87743496895\n",
      "  timestamp: 1575786800\n",
      "  timesteps_since_restore: 618000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 618000\n",
      "  training_iteration: 309\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21550 s, 309 iter, 618000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-34-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35228383248885\n",
      "  episode_reward_mean: 45.69865308093514\n",
      "  episode_reward_min: -22.037642029227897\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7925.605\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1977205127477646\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6927168369293213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.021588515490293503\n",
      "        policy_loss: -0.006495798006653786\n",
      "        total_loss: 219.6693878173828\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 219.67156982421875\n",
      "    load_time_ms: 4.577\n",
      "    num_steps_sampled: 620000\n",
      "    num_steps_trained: 620000\n",
      "    sample_time_ms: 58321.853\n",
      "    update_time_ms: 61.915\n",
      "  iterations_since_restore: 310\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.44736842105263\n",
      "    ram_util_percent: 71.62526315789476\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.20351146548282\n",
      "    mean_inference_ms: 12.073300872120848\n",
      "    mean_processing_ms: 22.114829039889106\n",
      "  time_since_restore: 21617.244652986526\n",
      "  time_this_iter_s: 66.36721801757812\n",
      "  time_total_s: 21617.244652986526\n",
      "  timestamp: 1575786867\n",
      "  timesteps_since_restore: 620000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 620000\n",
      "  training_iteration: 310\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21617 s, 310 iter, 620000 ts, 45.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-35-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.35228383248885\n",
      "  episode_reward_mean: 46.25947420603457\n",
      "  episode_reward_min: -4.864908577609478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8098.24\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1977205127477646\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7611448764801025\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.24184592068195343\n",
      "        policy_loss: 0.05482921376824379\n",
      "        total_loss: 197.6098175048828\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 197.50717163085938\n",
      "    load_time_ms: 4.559\n",
      "    num_steps_sampled: 622000\n",
      "    num_steps_trained: 622000\n",
      "    sample_time_ms: 58397.908\n",
      "    update_time_ms: 62.893\n",
      "  iterations_since_restore: 311\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.69795918367346\n",
      "    ram_util_percent: 71.70714285714287\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.36329546618762\n",
      "    mean_inference_ms: 12.081953909496082\n",
      "    mean_processing_ms: 22.114415432054585\n",
      "  time_since_restore: 21686.274856090546\n",
      "  time_this_iter_s: 69.03020310401917\n",
      "  time_total_s: 21686.274856090546\n",
      "  timestamp: 1575786936\n",
      "  timesteps_since_restore: 622000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 622000\n",
      "  training_iteration: 311\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21686 s, 311 iter, 622000 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-36-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.09471674344826\n",
      "  episode_reward_mean: 47.89405466256023\n",
      "  episode_reward_min: -4.864908577609478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8068.399\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2965807616710663\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.150020122528076\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008678773418068886\n",
      "        policy_loss: -0.005335634108632803\n",
      "        total_loss: 218.54171752929688\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 218.5445556640625\n",
      "    load_time_ms: 4.638\n",
      "    num_steps_sampled: 624000\n",
      "    num_steps_trained: 624000\n",
      "    sample_time_ms: 58102.861\n",
      "    update_time_ms: 69.595\n",
      "  iterations_since_restore: 312\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.48817204301076\n",
      "    ram_util_percent: 71.67956989247313\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.07737557747558\n",
      "    mean_inference_ms: 12.063313212628687\n",
      "    mean_processing_ms: 22.09863268594307\n",
      "  time_since_restore: 21751.547607660294\n",
      "  time_this_iter_s: 65.27275156974792\n",
      "  time_total_s: 21751.547607660294\n",
      "  timestamp: 1575787001\n",
      "  timesteps_since_restore: 624000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 624000\n",
      "  training_iteration: 312\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21751 s, 312 iter, 624000 ts, 47.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-37-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.09471674344826\n",
      "  episode_reward_mean: 50.54292433760952\n",
      "  episode_reward_min: -4.864908577609478\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7918.888\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.14829038083553314\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.9438912868499756\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.23142296075820923\n",
      "        policy_loss: 0.048988014459609985\n",
      "        total_loss: 211.5122833251953\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 211.42893981933594\n",
      "    load_time_ms: 3.179\n",
      "    num_steps_sampled: 626000\n",
      "    num_steps_trained: 626000\n",
      "    sample_time_ms: 58241.313\n",
      "    update_time_ms: 62.796\n",
      "  iterations_since_restore: 313\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86451612903225\n",
      "    ram_util_percent: 71.62903225806451\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.91448670224497\n",
      "    mean_inference_ms: 12.054179058106033\n",
      "    mean_processing_ms: 22.087902071512207\n",
      "  time_since_restore: 21816.86248779297\n",
      "  time_this_iter_s: 65.31488013267517\n",
      "  time_total_s: 21816.86248779297\n",
      "  timestamp: 1575787067\n",
      "  timesteps_since_restore: 626000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 626000\n",
      "  training_iteration: 313\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21816 s, 313 iter, 626000 ts, 50.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-38-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.09471674344826\n",
      "  episode_reward_mean: 52.18079236196403\n",
      "  episode_reward_min: 8.013803216169812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7903.606\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2224355787038803\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7943949699401855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007167823612689972\n",
      "        policy_loss: -0.014143923297524452\n",
      "        total_loss: 222.4698944091797\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.48240661621094\n",
      "    load_time_ms: 3.219\n",
      "    num_steps_sampled: 628000\n",
      "    num_steps_trained: 628000\n",
      "    sample_time_ms: 58246.984\n",
      "    update_time_ms: 61.579\n",
      "  iterations_since_restore: 314\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.58172043010754\n",
      "    ram_util_percent: 71.69462365591397\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.81354801408844\n",
      "    mean_inference_ms: 12.049796653866927\n",
      "    mean_processing_ms: 22.091078959374258\n",
      "  time_since_restore: 21882.02544927597\n",
      "  time_this_iter_s: 65.16296148300171\n",
      "  time_total_s: 21882.02544927597\n",
      "  timestamp: 1575787132\n",
      "  timesteps_since_restore: 628000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 628000\n",
      "  training_iteration: 314\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21882 s, 314 iter, 628000 ts, 52.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-39-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.09471674344826\n",
      "  episode_reward_mean: 50.628236109239594\n",
      "  episode_reward_min: 8.013803216169812\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7908.075\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11121778935194016\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.418246269226074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.030543331056833267\n",
      "        policy_loss: 0.0035523499827831984\n",
      "        total_loss: 198.74383544921875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 198.7368927001953\n",
      "    load_time_ms: 3.233\n",
      "    num_steps_sampled: 630000\n",
      "    num_steps_trained: 630000\n",
      "    sample_time_ms: 58134.795\n",
      "    update_time_ms: 60.594\n",
      "  iterations_since_restore: 315\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.8563829787234\n",
      "    ram_util_percent: 71.7904255319149\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.8695102602728\n",
      "    mean_inference_ms: 12.051002339122105\n",
      "    mean_processing_ms: 22.098900068826207\n",
      "  time_since_restore: 21947.73711681366\n",
      "  time_this_iter_s: 65.71166753768921\n",
      "  time_total_s: 21947.73711681366\n",
      "  timestamp: 1575787198\n",
      "  timesteps_since_restore: 630000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 630000\n",
      "  training_iteration: 315\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 21947 s, 315 iter, 630000 ts, 50.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-41-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.09471674344826\n",
      "  episode_reward_mean: 50.854238602540164\n",
      "  episode_reward_min: 1.268275196659084\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7914.612\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11121778935194016\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.959251880645752\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.09135975688695908\n",
      "        policy_loss: 0.023442190140485764\n",
      "        total_loss: 214.55078125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 214.51708984375\n",
      "    load_time_ms: 3.261\n",
      "    num_steps_sampled: 632000\n",
      "    num_steps_trained: 632000\n",
      "    sample_time_ms: 58066.162\n",
      "    update_time_ms: 58.594\n",
      "  iterations_since_restore: 316\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.31595744680853\n",
      "    ram_util_percent: 71.8436170212766\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.3995195504143\n",
      "    mean_inference_ms: 12.021638022764172\n",
      "    mean_processing_ms: 22.07787345625509\n",
      "  time_since_restore: 22013.439993858337\n",
      "  time_this_iter_s: 65.70287704467773\n",
      "  time_total_s: 22013.439993858337\n",
      "  timestamp: 1575787263\n",
      "  timesteps_since_restore: 632000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 632000\n",
      "  training_iteration: 316\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22013 s, 316 iter, 632000 ts, 50.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-42-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.62751352903834\n",
      "  episode_reward_mean: 50.44067722391756\n",
      "  episode_reward_min: 1.268275196659084\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7877.077\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.16682668030261993\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.63771915435791\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.1852404922246933\n",
      "        policy_loss: 0.06770063936710358\n",
      "        total_loss: 202.69534301757812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 202.59669494628906\n",
      "    load_time_ms: 3.309\n",
      "    num_steps_sampled: 634000\n",
      "    num_steps_trained: 634000\n",
      "    sample_time_ms: 58061.716\n",
      "    update_time_ms: 61.942\n",
      "  iterations_since_restore: 317\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.42395833333335\n",
      "    ram_util_percent: 71.890625\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.49110289934336\n",
      "    mean_inference_ms: 12.028071584112336\n",
      "    mean_processing_ms: 22.066086266359072\n",
      "  time_since_restore: 22081.06168603897\n",
      "  time_this_iter_s: 67.62169218063354\n",
      "  time_total_s: 22081.06168603897\n",
      "  timestamp: 1575787331\n",
      "  timesteps_since_restore: 634000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 634000\n",
      "  training_iteration: 317\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22081 s, 317 iter, 634000 ts, 50.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-43-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.66334255048208\n",
      "  episode_reward_mean: 49.0878976227983\n",
      "  episode_reward_min: 1.268275196659084\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7872.506\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2502400279045105\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6314003467559814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.09556671977043152\n",
      "        policy_loss: 0.0345916822552681\n",
      "        total_loss: 207.35302734375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 207.29452514648438\n",
      "    load_time_ms: 3.267\n",
      "    num_steps_sampled: 636000\n",
      "    num_steps_trained: 636000\n",
      "    sample_time_ms: 58130.43\n",
      "    update_time_ms: 61.845\n",
      "  iterations_since_restore: 318\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.85212765957448\n",
      "    ram_util_percent: 71.83297872340422\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.84919219327372\n",
      "    mean_inference_ms: 12.048680514681239\n",
      "    mean_processing_ms: 22.080124345517678\n",
      "  time_since_restore: 22146.51347875595\n",
      "  time_this_iter_s: 65.45179271697998\n",
      "  time_total_s: 22146.51347875595\n",
      "  timestamp: 1575787396\n",
      "  timesteps_since_restore: 636000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 636000\n",
      "  training_iteration: 318\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22146 s, 318 iter, 636000 ts, 49.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-44-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.66334255048208\n",
      "  episode_reward_mean: 48.15804562256736\n",
      "  episode_reward_min: 1.268275196659084\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7880.267\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.37536004185676575\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.521101951599121\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.049258116632699966\n",
      "        policy_loss: 0.01662757433950901\n",
      "        total_loss: 203.27430725097656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 203.23915100097656\n",
      "    load_time_ms: 3.29\n",
      "    num_steps_sampled: 638000\n",
      "    num_steps_trained: 638000\n",
      "    sample_time_ms: 58121.778\n",
      "    update_time_ms: 61.164\n",
      "  iterations_since_restore: 319\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.61397849462364\n",
      "    ram_util_percent: 71.7956989247312\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.77114590704167\n",
      "    mean_inference_ms: 12.042318947618455\n",
      "    mean_processing_ms: 22.069536060648307\n",
      "  time_since_restore: 22212.087480783463\n",
      "  time_this_iter_s: 65.5740020275116\n",
      "  time_total_s: 22212.087480783463\n",
      "  timestamp: 1575787462\n",
      "  timesteps_since_restore: 638000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 638000\n",
      "  training_iteration: 319\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22212 s, 319 iter, 638000 ts, 48.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.66334255048208\n",
      "  episode_reward_mean: 47.453824016328454\n",
      "  episode_reward_min: 1.268275196659084\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7881.562\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.563040018081665\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4150424003601074\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03715916723012924\n",
      "        policy_loss: 0.0058843581937253475\n",
      "        total_loss: 209.46530151367188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 209.4384307861328\n",
      "    load_time_ms: 3.226\n",
      "    num_steps_sampled: 640000\n",
      "    num_steps_trained: 640000\n",
      "    sample_time_ms: 58038.309\n",
      "    update_time_ms: 59.627\n",
      "  iterations_since_restore: 320\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.60744680851062\n",
      "    ram_util_percent: 71.7904255319149\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.23400293011738\n",
      "    mean_inference_ms: 12.0069163873779\n",
      "    mean_processing_ms: 22.029187610675717\n",
      "  time_since_restore: 22277.621646881104\n",
      "  time_this_iter_s: 65.53416609764099\n",
      "  time_total_s: 22277.621646881104\n",
      "  timestamp: 1575787528\n",
      "  timesteps_since_restore: 640000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 640000\n",
      "  training_iteration: 320\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22277 s, 320 iter, 640000 ts, 47.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-46-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.66140108719681\n",
      "  episode_reward_mean: 47.93841289354768\n",
      "  episode_reward_min: 0.592968150719917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7699.817\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.563040018081665\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.567065954208374\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.041206490248441696\n",
      "        policy_loss: 0.009974927641451359\n",
      "        total_loss: 224.28375244140625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 224.25057983398438\n",
      "    load_time_ms: 3.339\n",
      "    num_steps_sampled: 642000\n",
      "    num_steps_trained: 642000\n",
      "    sample_time_ms: 57835.381\n",
      "    update_time_ms: 60.131\n",
      "  iterations_since_restore: 321\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.36989247311827\n",
      "    ram_util_percent: 71.83978494623653\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.47010361949975\n",
      "    mean_inference_ms: 12.021524463108456\n",
      "    mean_processing_ms: 22.04212911723169\n",
      "  time_since_restore: 22342.712802171707\n",
      "  time_this_iter_s: 65.09115529060364\n",
      "  time_total_s: 22342.712802171707\n",
      "  timestamp: 1575787593\n",
      "  timesteps_since_restore: 642000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 642000\n",
      "  training_iteration: 321\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22342 s, 321 iter, 642000 ts, 47.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-47-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.66140108719681\n",
      "  episode_reward_mean: 46.76361666222901\n",
      "  episode_reward_min: 0.592968150719917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7675.994\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8445600867271423\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0252256393432617\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015290454961359501\n",
      "        policy_loss: -0.001572340028360486\n",
      "        total_loss: 216.77835083007812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.7669677734375\n",
      "    load_time_ms: 3.171\n",
      "    num_steps_sampled: 644000\n",
      "    num_steps_trained: 644000\n",
      "    sample_time_ms: 58096.259\n",
      "    update_time_ms: 53.634\n",
      "  iterations_since_restore: 322\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.93541666666665\n",
      "    ram_util_percent: 71.93541666666665\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.31674288135218\n",
      "    mean_inference_ms: 12.009285600520279\n",
      "    mean_processing_ms: 22.03639880670965\n",
      "  time_since_restore: 22410.284716129303\n",
      "  time_this_iter_s: 67.57191395759583\n",
      "  time_total_s: 22410.284716129303\n",
      "  timestamp: 1575787660\n",
      "  timesteps_since_restore: 644000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 644000\n",
      "  training_iteration: 322\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22410 s, 322 iter, 644000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-48-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.67255446521061\n",
      "  episode_reward_mean: 47.05247655503838\n",
      "  episode_reward_min: 0.592968150719917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7679.103\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8445600867271423\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.933582067489624\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019214797765016556\n",
      "        policy_loss: 0.002522369846701622\n",
      "        total_loss: 218.1193389892578\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 218.10055541992188\n",
      "    load_time_ms: 3.17\n",
      "    num_steps_sampled: 646000\n",
      "    num_steps_trained: 646000\n",
      "    sample_time_ms: 58162.14\n",
      "    update_time_ms: 54.566\n",
      "  iterations_since_restore: 323\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.19999999999999\n",
      "    ram_util_percent: 72.04574468085104\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.90606747694514\n",
      "    mean_inference_ms: 11.983973225618927\n",
      "    mean_processing_ms: 22.016357468711032\n",
      "  time_since_restore: 22476.289382219315\n",
      "  time_this_iter_s: 66.0046660900116\n",
      "  time_total_s: 22476.289382219315\n",
      "  timestamp: 1575787726\n",
      "  timesteps_since_restore: 646000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 646000\n",
      "  training_iteration: 323\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22476 s, 323 iter, 646000 ts, 47.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-49-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.67255446521061\n",
      "  episode_reward_mean: 47.96569634157995\n",
      "  episode_reward_min: 0.592968150719917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7680.043\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8445600867271423\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.058048725128174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03360874205827713\n",
      "        policy_loss: 0.012668735347688198\n",
      "        total_loss: 226.16168212890625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 226.12062072753906\n",
      "    load_time_ms: 3.081\n",
      "    num_steps_sampled: 648000\n",
      "    num_steps_trained: 648000\n",
      "    sample_time_ms: 58189.224\n",
      "    update_time_ms: 55.083\n",
      "  iterations_since_restore: 324\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.94468085106382\n",
      "    ram_util_percent: 72.09042553191487\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.68245330013207\n",
      "    mean_inference_ms: 11.969498544985413\n",
      "    mean_processing_ms: 21.999574041737056\n",
      "  time_since_restore: 22541.737731933594\n",
      "  time_this_iter_s: 65.44834971427917\n",
      "  time_total_s: 22541.737731933594\n",
      "  timestamp: 1575787792\n",
      "  timesteps_since_restore: 648000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 648000\n",
      "  training_iteration: 324\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22541 s, 324 iter, 648000 ts, 48 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-50-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.67255446521061\n",
      "  episode_reward_mean: 48.323280607256955\n",
      "  episode_reward_min: 0.592968150719917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7666.322\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8445600867271423\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2261924743652344\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014536668546497822\n",
      "        policy_loss: -0.002651273738592863\n",
      "        total_loss: 217.296875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 217.28712463378906\n",
      "    load_time_ms: 3.103\n",
      "    num_steps_sampled: 650000\n",
      "    num_steps_trained: 650000\n",
      "    sample_time_ms: 58202.658\n",
      "    update_time_ms: 56.509\n",
      "  iterations_since_restore: 325\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.40967741935484\n",
      "    ram_util_percent: 71.95161290322581\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.58224987998116\n",
      "    mean_inference_ms: 11.964583707081585\n",
      "    mean_processing_ms: 21.990911993071176\n",
      "  time_since_restore: 22607.457850694656\n",
      "  time_this_iter_s: 65.72011876106262\n",
      "  time_total_s: 22607.457850694656\n",
      "  timestamp: 1575787858\n",
      "  timesteps_since_restore: 650000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 650000\n",
      "  training_iteration: 325\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22607 s, 325 iter, 650000 ts, 48.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 01:52:04,094\tWARNING util.py:145 -- The `process_trial` operation took 0.12715744972229004 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-52-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.67255446521061\n",
      "  episode_reward_mean: 48.298916359649176\n",
      "  episode_reward_min: 3.4091422524439325\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7652.92\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8445600867271423\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1132476329803467\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.006247576791793108\n",
      "        policy_loss: -0.001950915320776403\n",
      "        total_loss: 214.23251342773438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 214.2292022705078\n",
      "    load_time_ms: 3.154\n",
      "    num_steps_sampled: 652000\n",
      "    num_steps_trained: 652000\n",
      "    sample_time_ms: 58225.473\n",
      "    update_time_ms: 57.687\n",
      "  iterations_since_restore: 326\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.77127659574468\n",
      "    ram_util_percent: 72.01382978723407\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.6980948391846\n",
      "    mean_inference_ms: 11.969315525991933\n",
      "    mean_processing_ms: 21.989939393150568\n",
      "  time_since_restore: 22673.31477212906\n",
      "  time_this_iter_s: 65.85692143440247\n",
      "  time_total_s: 22673.31477212906\n",
      "  timestamp: 1575787923\n",
      "  timesteps_since_restore: 652000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 652000\n",
      "  training_iteration: 326\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22673 s, 326 iter, 652000 ts, 48.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 01:52:04,267\tWARNING util.py:145 -- The `on_step_begin` operation took 0.1218101978302002 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-53-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.67255446521061\n",
      "  episode_reward_mean: 49.63425567681815\n",
      "  episode_reward_min: 4.756393778155246\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7657.375\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.42228004336357117\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7014074325561523\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0029405783861875534\n",
      "        policy_loss: -0.008713132701814175\n",
      "        total_loss: 221.19195556640625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 221.1994171142578\n",
      "    load_time_ms: 3.161\n",
      "    num_steps_sampled: 654000\n",
      "    num_steps_trained: 654000\n",
      "    sample_time_ms: 58187.935\n",
      "    update_time_ms: 74.229\n",
      "  iterations_since_restore: 327\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.82268041237113\n",
      "    ram_util_percent: 72.10103092783505\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.62472976846445\n",
      "    mean_inference_ms: 11.963396873208254\n",
      "    mean_processing_ms: 21.973119545616342\n",
      "  time_since_restore: 22740.785384655\n",
      "  time_this_iter_s: 67.47061252593994\n",
      "  time_total_s: 22740.785384655\n",
      "  timestamp: 1575787991\n",
      "  timesteps_since_restore: 654000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 654000\n",
      "  training_iteration: 327\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22740 s, 327 iter, 654000 ts, 49.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-54-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.84039769781727\n",
      "  episode_reward_mean: 50.23955160333419\n",
      "  episode_reward_min: 4.995951638416961\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7664.647\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21114002168178558\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3337297439575195\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029122013598680496\n",
      "        policy_loss: -0.02736472897231579\n",
      "        total_loss: 225.50579833984375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 225.52706909179688\n",
      "    load_time_ms: 3.163\n",
      "    num_steps_sampled: 656000\n",
      "    num_steps_trained: 656000\n",
      "    sample_time_ms: 58167.248\n",
      "    update_time_ms: 75.086\n",
      "  iterations_since_restore: 328\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.7774193548387\n",
      "    ram_util_percent: 72.16989247311827\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.97081276430734\n",
      "    mean_inference_ms: 11.986552442333123\n",
      "    mean_processing_ms: 22.005110110481773\n",
      "  time_since_restore: 22806.117113113403\n",
      "  time_this_iter_s: 65.33172845840454\n",
      "  time_total_s: 22806.117113113403\n",
      "  timestamp: 1575788056\n",
      "  timesteps_since_restore: 656000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 656000\n",
      "  training_iteration: 328\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22806 s, 328 iter, 656000 ts, 50.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-55-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.84039769781727\n",
      "  episode_reward_mean: 47.79427224529075\n",
      "  episode_reward_min: -4.524551582623253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7668.396\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21114002168178558\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2604575157165527\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008870583027601242\n",
      "        policy_loss: -0.012596620246767998\n",
      "        total_loss: 244.317626953125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 244.3283233642578\n",
      "    load_time_ms: 3.113\n",
      "    num_steps_sampled: 658000\n",
      "    num_steps_trained: 658000\n",
      "    sample_time_ms: 58163.101\n",
      "    update_time_ms: 75.807\n",
      "  iterations_since_restore: 329\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.93510638297872\n",
      "    ram_util_percent: 72.21489361702126\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.45793249268226\n",
      "    mean_inference_ms: 12.01770500968203\n",
      "    mean_processing_ms: 22.038567064700572\n",
      "  time_since_restore: 22871.681527853012\n",
      "  time_this_iter_s: 65.56441473960876\n",
      "  time_total_s: 22871.681527853012\n",
      "  timestamp: 1575788122\n",
      "  timesteps_since_restore: 658000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 658000\n",
      "  training_iteration: 329\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22871 s, 329 iter, 658000 ts, 47.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.84039769781727\n",
      "  episode_reward_mean: 46.23005898209691\n",
      "  episode_reward_min: -4.524551582623253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7671.661\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10557001084089279\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1097817420959473\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007236388046294451\n",
      "        policy_loss: -0.012249398976564407\n",
      "        total_loss: 212.7178497314453\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 212.72943115234375\n",
      "    load_time_ms: 3.057\n",
      "    num_steps_sampled: 660000\n",
      "    num_steps_trained: 660000\n",
      "    sample_time_ms: 58346.219\n",
      "    update_time_ms: 74.87\n",
      "  iterations_since_restore: 330\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.53854166666667\n",
      "    ram_util_percent: 72.33125\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.83016708245475\n",
      "    mean_inference_ms: 12.039622404252881\n",
      "    mean_processing_ms: 22.048288605701956\n",
      "  time_since_restore: 22939.054789066315\n",
      "  time_this_iter_s: 67.37326121330261\n",
      "  time_total_s: 22939.054789066315\n",
      "  timestamp: 1575788189\n",
      "  timesteps_since_restore: 660000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 660000\n",
      "  training_iteration: 330\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 22939 s, 330 iter, 660000 ts, 46.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.84039769781727\n",
      "  episode_reward_mean: 47.727025668451034\n",
      "  episode_reward_min: -4.524551582623253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7909.627\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.052785005420446396\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8496931791305542\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01471223495900631\n",
      "        policy_loss: -0.021329326555132866\n",
      "        total_loss: 234.21914672851562\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 234.23965454101562\n",
      "    load_time_ms: 3.007\n",
      "    num_steps_sampled: 662000\n",
      "    num_steps_trained: 662000\n",
      "    sample_time_ms: 58485.758\n",
      "    update_time_ms: 72.998\n",
      "  iterations_since_restore: 331\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.9857142857143\n",
      "    ram_util_percent: 72.20612244897958\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.81354429933802\n",
      "    mean_inference_ms: 12.038258455723007\n",
      "    mean_processing_ms: 22.046157970739532\n",
      "  time_since_restore: 23007.902733802795\n",
      "  time_this_iter_s: 68.84794473648071\n",
      "  time_total_s: 23007.902733802795\n",
      "  timestamp: 1575788258\n",
      "  timesteps_since_restore: 662000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 662000\n",
      "  training_iteration: 331\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23007 s, 331 iter, 662000 ts, 47.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.93032502541433\n",
      "  episode_reward_mean: 47.49101048111561\n",
      "  episode_reward_min: -4.524551582623253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7835.659\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.052785005420446396\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4151811599731445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011924328282475471\n",
      "        policy_loss: -0.015671828761696815\n",
      "        total_loss: 224.2801513671875\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 224.2952117919922\n",
      "    load_time_ms: 4.126\n",
      "    num_steps_sampled: 664000\n",
      "    num_steps_trained: 664000\n",
      "    sample_time_ms: 58286.364\n",
      "    update_time_ms: 74.232\n",
      "  iterations_since_restore: 332\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.17282608695652\n",
      "    ram_util_percent: 72.21739130434784\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.72402429686406\n",
      "    mean_inference_ms: 12.035034292937356\n",
      "    mean_processing_ms: 22.062662773484043\n",
      "  time_since_restore: 23072.74828338623\n",
      "  time_this_iter_s: 64.84554958343506\n",
      "  time_total_s: 23072.74828338623\n",
      "  timestamp: 1575788323\n",
      "  timesteps_since_restore: 664000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 664000\n",
      "  training_iteration: 332\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23072 s, 332 iter, 664000 ts, 47.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_01-59-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.93032502541433\n",
      "  episode_reward_mean: 46.824915233318414\n",
      "  episode_reward_min: -9.041103659071055\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7844.098\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.052785005420446396\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.230348587036133\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.016868628561496735\n",
      "        policy_loss: -0.017770953476428986\n",
      "        total_loss: 226.8262481689453\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 226.8431396484375\n",
      "    load_time_ms: 4.029\n",
      "    num_steps_sampled: 666000\n",
      "    num_steps_trained: 666000\n",
      "    sample_time_ms: 58286.097\n",
      "    update_time_ms: 73.553\n",
      "  iterations_since_restore: 333\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.62105263157895\n",
      "    ram_util_percent: 72.26315789473684\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.32888563929606\n",
      "    mean_inference_ms: 12.00806599328605\n",
      "    mean_processing_ms: 22.024711501310197\n",
      "  time_since_restore: 23138.829224586487\n",
      "  time_this_iter_s: 66.08094120025635\n",
      "  time_total_s: 23138.829224586487\n",
      "  timestamp: 1575788389\n",
      "  timesteps_since_restore: 666000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 666000\n",
      "  training_iteration: 333\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23138 s, 333 iter, 666000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-00-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.93032502541433\n",
      "  episode_reward_mean: 48.637813176669376\n",
      "  episode_reward_min: -9.041103659071055\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7860.209\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.052785005420446396\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2919344902038574\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007380938623100519\n",
      "        policy_loss: -0.012938188388943672\n",
      "        total_loss: 238.6212921142578\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 238.63377380371094\n",
      "    load_time_ms: 4.105\n",
      "    num_steps_sampled: 668000\n",
      "    num_steps_trained: 668000\n",
      "    sample_time_ms: 58304.879\n",
      "    update_time_ms: 73.585\n",
      "  iterations_since_restore: 334\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.00212765957447\n",
      "    ram_util_percent: 72.3712765957447\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.13877077321172\n",
      "    mean_inference_ms: 11.993210864167214\n",
      "    mean_processing_ms: 22.004288321201933\n",
      "  time_since_restore: 23204.627925634384\n",
      "  time_this_iter_s: 65.79870104789734\n",
      "  time_total_s: 23204.627925634384\n",
      "  timestamp: 1575788455\n",
      "  timesteps_since_restore: 668000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 668000\n",
      "  training_iteration: 334\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23204 s, 334 iter, 668000 ts, 48.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-02-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.93032502541433\n",
      "  episode_reward_mean: 51.11396297666238\n",
      "  episode_reward_min: -9.041103659071055\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7882.841\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.026392502710223198\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3089029788970947\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02410157211124897\n",
      "        policy_loss: -0.026195796206593513\n",
      "        total_loss: 237.3877410888672\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 237.41322326660156\n",
      "    load_time_ms: 4.2\n",
      "    num_steps_sampled: 670000\n",
      "    num_steps_trained: 670000\n",
      "    sample_time_ms: 58244.693\n",
      "    update_time_ms: 72.367\n",
      "  iterations_since_restore: 335\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.55698924731182\n",
      "    ram_util_percent: 72.44516129032259\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.1140050459424\n",
      "    mean_inference_ms: 11.991336227986903\n",
      "    mean_processing_ms: 21.99982847749088\n",
      "  time_since_restore: 23269.947591781616\n",
      "  time_this_iter_s: 65.31966614723206\n",
      "  time_total_s: 23269.947591781616\n",
      "  timestamp: 1575788520\n",
      "  timesteps_since_restore: 670000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 670000\n",
      "  training_iteration: 335\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23269 s, 335 iter, 670000 ts, 51.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-03-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.93032502541433\n",
      "  episode_reward_mean: 49.96574293268662\n",
      "  episode_reward_min: -9.041103659071055\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8182.518\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.026392502710223198\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.6070622205734253\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 1.6369397640228271\n",
      "        policy_loss: 0.13182857632637024\n",
      "        total_loss: 239.75682067871094\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 239.58172607421875\n",
      "    load_time_ms: 4.568\n",
      "    num_steps_sampled: 672000\n",
      "    num_steps_trained: 672000\n",
      "    sample_time_ms: 58227.755\n",
      "    update_time_ms: 70.608\n",
      "  iterations_since_restore: 336\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.0438775510204\n",
      "    ram_util_percent: 72.52448979591838\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.3172253398659\n",
      "    mean_inference_ms: 12.007149974704335\n",
      "    mean_processing_ms: 22.012031706881316\n",
      "  time_since_restore: 23338.564746141434\n",
      "  time_this_iter_s: 68.6171543598175\n",
      "  time_total_s: 23338.564746141434\n",
      "  timestamp: 1575788589\n",
      "  timesteps_since_restore: 672000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 672000\n",
      "  training_iteration: 336\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23338 s, 336 iter, 672000 ts, 50 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-04-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.01149620128169\n",
      "  episode_reward_mean: 48.41876726214559\n",
      "  episode_reward_min: -9.041103659071055\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7994.266\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03958875313401222\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8056464195251465\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02248499169945717\n",
      "        policy_loss: -0.009204678237438202\n",
      "        total_loss: 222.91741943359375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 222.92578125\n",
      "    load_time_ms: 4.816\n",
      "    num_steps_sampled: 674000\n",
      "    num_steps_trained: 674000\n",
      "    sample_time_ms: 58191.477\n",
      "    update_time_ms: 51.419\n",
      "  iterations_since_restore: 337\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.39677419354838\n",
      "    ram_util_percent: 72.63010752688173\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.9320592623273\n",
      "    mean_inference_ms: 11.980125111554505\n",
      "    mean_processing_ms: 21.980557968675903\n",
      "  time_since_restore: 23403.587733507156\n",
      "  time_this_iter_s: 65.02298736572266\n",
      "  time_total_s: 23403.587733507156\n",
      "  timestamp: 1575788654\n",
      "  timesteps_since_restore: 674000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 674000\n",
      "  training_iteration: 337\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23403 s, 337 iter, 674000 ts, 48.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-05-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.01149620128169\n",
      "  episode_reward_mean: 48.23536322613407\n",
      "  episode_reward_min: -1.253201501191564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7980.078\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03958875313401222\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.42333984375\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03639727830886841\n",
      "        policy_loss: -0.023578418418765068\n",
      "        total_loss: 219.12347412109375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 219.14566040039062\n",
      "    load_time_ms: 4.803\n",
      "    num_steps_sampled: 676000\n",
      "    num_steps_trained: 676000\n",
      "    sample_time_ms: 58293.615\n",
      "    update_time_ms: 50.135\n",
      "  iterations_since_restore: 338\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.97978723404256\n",
      "    ram_util_percent: 72.42340425531916\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.1262358199597\n",
      "    mean_inference_ms: 11.991839933569327\n",
      "    mean_processing_ms: 22.01036125392316\n",
      "  time_since_restore: 23469.783274650574\n",
      "  time_this_iter_s: 66.19554114341736\n",
      "  time_total_s: 23469.783274650574\n",
      "  timestamp: 1575788720\n",
      "  timesteps_since_restore: 676000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 676000\n",
      "  training_iteration: 338\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23469 s, 338 iter, 676000 ts, 48.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-06-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.08075585644362\n",
      "  episode_reward_mean: 44.5943902306604\n",
      "  episode_reward_min: -1.253201501191564\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7978.899\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03958875313401222\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.680156707763672\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.018159568309783936\n",
      "        policy_loss: -0.01008882187306881\n",
      "        total_loss: 230.79190063476562\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 230.80123901367188\n",
      "    load_time_ms: 4.806\n",
      "    num_steps_sampled: 678000\n",
      "    num_steps_trained: 678000\n",
      "    sample_time_ms: 58392.306\n",
      "    update_time_ms: 50.823\n",
      "  iterations_since_restore: 339\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.31157894736843\n",
      "    ram_util_percent: 72.52736842105264\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.96586809151876\n",
      "    mean_inference_ms: 11.983175427936658\n",
      "    mean_processing_ms: 22.000122943226206\n",
      "  time_since_restore: 23536.330182790756\n",
      "  time_this_iter_s: 66.5469081401825\n",
      "  time_total_s: 23536.330182790756\n",
      "  timestamp: 1575788787\n",
      "  timesteps_since_restore: 678000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 678000\n",
      "  training_iteration: 339\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23536 s, 339 iter, 678000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-07-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.97409788627067\n",
      "  episode_reward_mean: 43.95307669824612\n",
      "  episode_reward_min: -9.591259096073271\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7986.109\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03958875313401222\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2711691856384277\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02117479406297207\n",
      "        policy_loss: -0.01695764623582363\n",
      "        total_loss: 239.60488891601562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 239.62109375\n",
      "    load_time_ms: 4.811\n",
      "    num_steps_sampled: 680000\n",
      "    num_steps_trained: 680000\n",
      "    sample_time_ms: 58221.486\n",
      "    update_time_ms: 52.717\n",
      "  iterations_since_restore: 340\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.6436170212766\n",
      "    ram_util_percent: 72.57978723404256\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.5645601871396\n",
      "    mean_inference_ms: 11.958819858031656\n",
      "    mean_processing_ms: 21.98465606658332\n",
      "  time_since_restore: 23602.090411901474\n",
      "  time_this_iter_s: 65.76022911071777\n",
      "  time_total_s: 23602.090411901474\n",
      "  timestamp: 1575788853\n",
      "  timesteps_since_restore: 680000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 680000\n",
      "  training_iteration: 340\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.0/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23602 s, 340 iter, 680000 ts, 44 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-08-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.83976543489813\n",
      "  episode_reward_mean: 42.653262465535946\n",
      "  episode_reward_min: -18.824672291569083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8021.74\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.03958875313401222\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6727075576782227\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.4898068308830261\n",
      "        policy_loss: 0.09363655745983124\n",
      "        total_loss: 246.6912384033203\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 246.57823181152344\n",
      "    load_time_ms: 4.832\n",
      "    num_steps_sampled: 682000\n",
      "    num_steps_trained: 682000\n",
      "    sample_time_ms: 58054.015\n",
      "    update_time_ms: 54.139\n",
      "  iterations_since_restore: 341\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90729166666667\n",
      "    ram_util_percent: 72.66770833333332\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.3724154216834\n",
      "    mean_inference_ms: 11.94394006954356\n",
      "    mean_processing_ms: 21.963670913696905\n",
      "  time_since_restore: 23669.637276887894\n",
      "  time_this_iter_s: 67.54686498641968\n",
      "  time_total_s: 23669.637276887894\n",
      "  timestamp: 1575788920\n",
      "  timesteps_since_restore: 682000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 682000\n",
      "  training_iteration: 341\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23669 s, 341 iter, 682000 ts, 42.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-09-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.83976543489813\n",
      "  episode_reward_mean: 40.74502170277436\n",
      "  episode_reward_min: -34.69760614389792\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7867.152\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05938313156366348\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.644580602645874\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01764349453151226\n",
      "        policy_loss: -0.01468455046415329\n",
      "        total_loss: 243.40061950683594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 243.414306640625\n",
      "    load_time_ms: 3.797\n",
      "    num_steps_sampled: 684000\n",
      "    num_steps_trained: 684000\n",
      "    sample_time_ms: 58387.422\n",
      "    update_time_ms: 52.68\n",
      "  iterations_since_restore: 342\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.14315789473683\n",
      "    ram_util_percent: 72.79263157894735\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.35708109365385\n",
      "    mean_inference_ms: 11.942892673867723\n",
      "    mean_processing_ms: 21.95747108692732\n",
      "  time_since_restore: 23736.24986100197\n",
      "  time_this_iter_s: 66.6125841140747\n",
      "  time_total_s: 23736.24986100197\n",
      "  timestamp: 1575788987\n",
      "  timesteps_since_restore: 684000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 684000\n",
      "  training_iteration: 342\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23736 s, 342 iter, 684000 ts, 40.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-10-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.45290085672285\n",
      "  episode_reward_mean: 40.21126079927017\n",
      "  episode_reward_min: -34.69760614389792\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7855.179\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.05938313156366348\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3307580947875977\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.1251233071088791\n",
      "        policy_loss: 0.035669486969709396\n",
      "        total_loss: 219.5609588623047\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 219.51791381835938\n",
      "    load_time_ms: 3.881\n",
      "    num_steps_sampled: 686000\n",
      "    num_steps_trained: 686000\n",
      "    sample_time_ms: 58343.246\n",
      "    update_time_ms: 52.226\n",
      "  iterations_since_restore: 343\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.96489361702129\n",
      "    ram_util_percent: 72.80425531914894\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.69964636129234\n",
      "    mean_inference_ms: 11.902245477648975\n",
      "    mean_processing_ms: 21.91175001307589\n",
      "  time_since_restore: 23801.76520204544\n",
      "  time_this_iter_s: 65.51534104347229\n",
      "  time_total_s: 23801.76520204544\n",
      "  timestamp: 1575789053\n",
      "  timesteps_since_restore: 686000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 686000\n",
      "  training_iteration: 343\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23801 s, 343 iter, 686000 ts, 40.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-11-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.45290085672285\n",
      "  episode_reward_mean: 42.79131125856284\n",
      "  episode_reward_min: -34.69760614389792\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7848.206\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08907469362020493\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8990116119384766\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015990540385246277\n",
      "        policy_loss: -0.015801984816789627\n",
      "        total_loss: 223.0015106201172\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 223.0159149169922\n",
      "    load_time_ms: 3.821\n",
      "    num_steps_sampled: 688000\n",
      "    num_steps_trained: 688000\n",
      "    sample_time_ms: 58378.917\n",
      "    update_time_ms: 51.435\n",
      "  iterations_since_restore: 344\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.25\n",
      "    ram_util_percent: 72.67446808510638\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.48757540383883\n",
      "    mean_inference_ms: 11.888698446491267\n",
      "    mean_processing_ms: 21.896341041145398\n",
      "  time_since_restore: 23867.8646569252\n",
      "  time_this_iter_s: 66.09945487976074\n",
      "  time_total_s: 23867.8646569252\n",
      "  timestamp: 1575789119\n",
      "  timesteps_since_restore: 688000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 688000\n",
      "  training_iteration: 344\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23867 s, 344 iter, 688000 ts, 42.8 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 02:13:05,583\tWARNING util.py:145 -- The `process_trial` operation took 0.1284186840057373 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-13-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.45290085672285\n",
      "  episode_reward_mean: 42.55227232485717\n",
      "  episode_reward_min: -34.69760614389792\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7937.466\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08907469362020493\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.637213706970215\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.12325534224510193\n",
      "        policy_loss: 0.03857117518782616\n",
      "        total_loss: 219.1698760986328\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 219.1202850341797\n",
      "    load_time_ms: 3.758\n",
      "    num_steps_sampled: 690000\n",
      "    num_steps_trained: 690000\n",
      "    sample_time_ms: 58359.3\n",
      "    update_time_ms: 50.834\n",
      "  iterations_since_restore: 345\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86276595744681\n",
      "    ram_util_percent: 72.71808510638297\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.05630489606662\n",
      "    mean_inference_ms: 11.922702681527017\n",
      "    mean_processing_ms: 21.92582076761463\n",
      "  time_since_restore: 23934.01150369644\n",
      "  time_this_iter_s: 66.14684677124023\n",
      "  time_total_s: 23934.01150369644\n",
      "  timestamp: 1575789185\n",
      "  timesteps_since_restore: 690000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 690000\n",
      "  training_iteration: 345\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 23934 s, 345 iter, 690000 ts, 42.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-14-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.45290085672285\n",
      "  episode_reward_mean: 44.15506359524697\n",
      "  episode_reward_min: -34.69760614389792\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7902.531\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1336120367050171\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2746522426605225\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.12354142963886261\n",
      "        policy_loss: 0.04499411955475807\n",
      "        total_loss: 216.10958862304688\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.04812622070312\n",
      "    load_time_ms: 3.379\n",
      "    num_steps_sampled: 692000\n",
      "    num_steps_trained: 692000\n",
      "    sample_time_ms: 58190.635\n",
      "    update_time_ms: 68.687\n",
      "  iterations_since_restore: 346\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.85208333333334\n",
      "    ram_util_percent: 72.85520833333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.89805651155427\n",
      "    mean_inference_ms: 11.914143209671243\n",
      "    mean_processing_ms: 21.91794816212132\n",
      "  time_since_restore: 24000.763486623764\n",
      "  time_this_iter_s: 66.75198292732239\n",
      "  time_total_s: 24000.763486623764\n",
      "  timestamp: 1575789252\n",
      "  timesteps_since_restore: 692000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 692000\n",
      "  training_iteration: 346\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24000 s, 346 iter, 692000 ts, 44.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-15-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.45290085672285\n",
      "  episode_reward_mean: 46.82855654841331\n",
      "  episode_reward_min: -8.800333755149625\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7867.458\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20041806995868683\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5011820793151855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.045491915196180344\n",
      "        policy_loss: 0.017771217972040176\n",
      "        total_loss: 229.20474243164062\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 229.17784118652344\n",
      "    load_time_ms: 3.102\n",
      "    num_steps_sampled: 694000\n",
      "    num_steps_trained: 694000\n",
      "    sample_time_ms: 58444.262\n",
      "    update_time_ms: 68.631\n",
      "  iterations_since_restore: 347\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.57083333333333\n",
      "    ram_util_percent: 72.92604166666666\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.78317120002131\n",
      "    mean_inference_ms: 11.908876952780346\n",
      "    mean_processing_ms: 21.909495434691074\n",
      "  time_since_restore: 24067.967913866043\n",
      "  time_this_iter_s: 67.20442724227905\n",
      "  time_total_s: 24067.967913866043\n",
      "  timestamp: 1575789319\n",
      "  timesteps_since_restore: 694000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 694000\n",
      "  training_iteration: 347\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24067 s, 347 iter, 694000 ts, 46.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-16-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.30722457386202\n",
      "  episode_reward_mean: 47.59930903299155\n",
      "  episode_reward_min: -8.800333755149625\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7860.053\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30062708258628845\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.121385097503662\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015673208981752396\n",
      "        policy_loss: -0.023104121908545494\n",
      "        total_loss: 230.92237854003906\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 230.94065856933594\n",
      "    load_time_ms: 3.087\n",
      "    num_steps_sampled: 696000\n",
      "    num_steps_trained: 696000\n",
      "    sample_time_ms: 58389.398\n",
      "    update_time_ms: 68.536\n",
      "  iterations_since_restore: 348\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.18924731182796\n",
      "    ram_util_percent: 72.96666666666664\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.08008616449982\n",
      "    mean_inference_ms: 11.927784067425616\n",
      "    mean_processing_ms: 21.92744124781599\n",
      "  time_since_restore: 24133.52683520317\n",
      "  time_this_iter_s: 65.55892133712769\n",
      "  time_total_s: 24133.52683520317\n",
      "  timestamp: 1575789385\n",
      "  timesteps_since_restore: 696000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 696000\n",
      "  training_iteration: 348\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24133 s, 348 iter, 696000 ts, 47.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-17-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.30722457386202\n",
      "  episode_reward_mean: 48.09235577779088\n",
      "  episode_reward_min: -9.87300407149322\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 6980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7860.617\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.30062708258628845\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1907637119293213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0539713092148304\n",
      "        policy_loss: 0.014426439069211483\n",
      "        total_loss: 239.1343536376953\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 239.10365295410156\n",
      "    load_time_ms: 3.12\n",
      "    num_steps_sampled: 698000\n",
      "    num_steps_trained: 698000\n",
      "    sample_time_ms: 58327.8\n",
      "    update_time_ms: 67.87\n",
      "  iterations_since_restore: 349\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.80744680851063\n",
      "    ram_util_percent: 73.02659574468085\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.95271031219937\n",
      "    mean_inference_ms: 11.981430373470452\n",
      "    mean_processing_ms: 21.976963656138913\n",
      "  time_since_restore: 24199.46952676773\n",
      "  time_this_iter_s: 65.94269156455994\n",
      "  time_total_s: 24199.46952676773\n",
      "  timestamp: 1575789451\n",
      "  timesteps_since_restore: 698000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 698000\n",
      "  training_iteration: 349\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24199 s, 349 iter, 698000 ts, 48.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-18-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.30722457386202\n",
      "  episode_reward_mean: 48.77313607262986\n",
      "  episode_reward_min: -16.35651390775458\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8119.498\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.45094063878059387\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4382822513580322\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019399479031562805\n",
      "        policy_loss: -0.009516607038676739\n",
      "        total_loss: 245.6101531982422\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 245.61087036132812\n",
      "    load_time_ms: 3.109\n",
      "    num_steps_sampled: 700000\n",
      "    num_steps_trained: 700000\n",
      "    sample_time_ms: 58250.087\n",
      "    update_time_ms: 66.756\n",
      "  iterations_since_restore: 350\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.78659793814434\n",
      "    ram_util_percent: 73.0278350515464\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.7462464793683\n",
      "    mean_inference_ms: 11.968733117655052\n",
      "    mean_processing_ms: 21.95563655573645\n",
      "  time_since_restore: 24267.040343523026\n",
      "  time_this_iter_s: 67.5708167552948\n",
      "  time_total_s: 24267.040343523026\n",
      "  timestamp: 1575789518\n",
      "  timesteps_since_restore: 700000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 700000\n",
      "  training_iteration: 350\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24267 s, 350 iter, 700000 ts, 48.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-19-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.30722457386202\n",
      "  episode_reward_mean: 47.26185815030529\n",
      "  episode_reward_min: -16.35651390775458\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8025.487\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.45094063878059387\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6591475009918213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.047486562281847\n",
      "        policy_loss: 0.02718883939087391\n",
      "        total_loss: 251.87669372558594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 251.82809448242188\n",
      "    load_time_ms: 4.163\n",
      "    num_steps_sampled: 702000\n",
      "    num_steps_trained: 702000\n",
      "    sample_time_ms: 58108.465\n",
      "    update_time_ms: 66.565\n",
      "  iterations_since_restore: 351\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.2989247311828\n",
      "    ram_util_percent: 72.98817204301075\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.20567477174367\n",
      "    mean_inference_ms: 11.99459680228556\n",
      "    mean_processing_ms: 21.974963182394085\n",
      "  time_since_restore: 24332.23738026619\n",
      "  time_this_iter_s: 65.19703674316406\n",
      "  time_total_s: 24332.23738026619\n",
      "  timestamp: 1575789584\n",
      "  timesteps_since_restore: 702000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 702000\n",
      "  training_iteration: 351\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24332 s, 351 iter, 702000 ts, 47.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-20-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 95.30722457386202\n",
      "  episode_reward_mean: 46.688246186309726\n",
      "  episode_reward_min: -16.35651390775458\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8023.19\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.676410973072052\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.770083427429199\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023070644587278366\n",
      "        policy_loss: -0.008169245906174183\n",
      "        total_loss: 218.81869506835938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.81124877929688\n",
      "    load_time_ms: 4.1\n",
      "    num_steps_sampled: 704000\n",
      "    num_steps_trained: 704000\n",
      "    sample_time_ms: 58008.937\n",
      "    update_time_ms: 67.019\n",
      "  iterations_since_restore: 352\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.13723404255319\n",
      "    ram_util_percent: 73.02553191489365\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.2875072376154\n",
      "    mean_inference_ms: 11.997245773107329\n",
      "    mean_processing_ms: 21.98015113617357\n",
      "  time_since_restore: 24397.84453868866\n",
      "  time_this_iter_s: 65.60715842247009\n",
      "  time_total_s: 24397.84453868866\n",
      "  timestamp: 1575789649\n",
      "  timesteps_since_restore: 704000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 704000\n",
      "  training_iteration: 352\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24397 s, 352 iter, 704000 ts, 46.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-21-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.16708010859024\n",
      "  episode_reward_mean: 44.707523498456574\n",
      "  episode_reward_min: -16.35651390775458\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8018.07\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.676410973072052\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2479960918426514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03318558633327484\n",
      "        policy_loss: -0.0035979500971734524\n",
      "        total_loss: 228.1835479736328\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 228.1647186279297\n",
      "    load_time_ms: 4.011\n",
      "    num_steps_sampled: 706000\n",
      "    num_steps_trained: 706000\n",
      "    sample_time_ms: 58032.346\n",
      "    update_time_ms: 66.381\n",
      "  iterations_since_restore: 353\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.41612903225807\n",
      "    ram_util_percent: 73.1225806451613\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.98997017408448\n",
      "    mean_inference_ms: 11.978215336641465\n",
      "    mean_processing_ms: 21.954614378756595\n",
      "  time_since_restore: 24463.553826093674\n",
      "  time_this_iter_s: 65.70928740501404\n",
      "  time_total_s: 24463.553826093674\n",
      "  timestamp: 1575789715\n",
      "  timesteps_since_restore: 706000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 706000\n",
      "  training_iteration: 353\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24463 s, 353 iter, 706000 ts, 44.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-23-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.16708010859024\n",
      "  episode_reward_mean: 42.73726746257524\n",
      "  episode_reward_min: -21.014601913504087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8010.841\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.676410973072052\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3144428730010986\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04848172515630722\n",
      "        policy_loss: 0.013664485886693\n",
      "        total_loss: 216.7348175048828\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 216.6882781982422\n",
      "    load_time_ms: 4.236\n",
      "    num_steps_sampled: 708000\n",
      "    num_steps_trained: 708000\n",
      "    sample_time_ms: 58177.106\n",
      "    update_time_ms: 66.853\n",
      "  iterations_since_restore: 354\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.40721649484536\n",
      "    ram_util_percent: 73.17319587628867\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.27999070957543\n",
      "    mean_inference_ms: 11.934426618562378\n",
      "    mean_processing_ms: 21.914817637297276\n",
      "  time_since_restore: 24531.01363992691\n",
      "  time_this_iter_s: 67.4598138332367\n",
      "  time_total_s: 24531.01363992691\n",
      "  timestamp: 1575789782\n",
      "  timesteps_since_restore: 708000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 708000\n",
      "  training_iteration: 354\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24531 s, 354 iter, 708000 ts, 42.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-24-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.27144315691274\n",
      "  episode_reward_mean: 43.23292176952393\n",
      "  episode_reward_min: -21.014601913504087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8127.929\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0146164894104004\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.575624942779541\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03259385749697685\n",
      "        policy_loss: 0.009252733550965786\n",
      "        total_loss: 234.09190368652344\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 234.04953002929688\n",
      "    load_time_ms: 4.201\n",
      "    num_steps_sampled: 710000\n",
      "    num_steps_trained: 710000\n",
      "    sample_time_ms: 58289.576\n",
      "    update_time_ms: 66.574\n",
      "  iterations_since_restore: 355\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.05670103092784\n",
      "    ram_util_percent: 73.30927835051547\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.97063891023748\n",
      "    mean_inference_ms: 11.91342821208396\n",
      "    mean_processing_ms: 21.907129467763575\n",
      "  time_since_restore: 24599.316561222076\n",
      "  time_this_iter_s: 68.30292129516602\n",
      "  time_total_s: 24599.316561222076\n",
      "  timestamp: 1575789851\n",
      "  timesteps_since_restore: 710000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 710000\n",
      "  training_iteration: 355\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24599 s, 355 iter, 710000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-25-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.27144315691274\n",
      "  episode_reward_mean: 44.04054175016732\n",
      "  episode_reward_min: -21.014601913504087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7899.729\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.0146164894104004\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.750626564025879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005281556397676468\n",
      "        policy_loss: -0.014660829678177834\n",
      "        total_loss: 222.2782440185547\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.28750610351562\n",
      "    load_time_ms: 4.194\n",
      "    num_steps_sampled: 712000\n",
      "    num_steps_trained: 712000\n",
      "    sample_time_ms: 58498.327\n",
      "    update_time_ms: 49.735\n",
      "  iterations_since_restore: 356\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.71368421052631\n",
      "    ram_util_percent: 73.17789473684209\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.26096689701706\n",
      "    mean_inference_ms: 11.931478442864481\n",
      "    mean_processing_ms: 21.924953021199816\n",
      "  time_since_restore: 24665.70643210411\n",
      "  time_this_iter_s: 66.3898708820343\n",
      "  time_total_s: 24665.70643210411\n",
      "  timestamp: 1575789917\n",
      "  timesteps_since_restore: 712000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 712000\n",
      "  training_iteration: 356\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24665 s, 356 iter, 712000 ts, 44 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-26-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.27144315691274\n",
      "  episode_reward_mean: 45.94064881746318\n",
      "  episode_reward_min: -21.014601913504087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7899.476\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5073082447052002\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.05519437789917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03625289350748062\n",
      "        policy_loss: 0.007004992570728064\n",
      "        total_loss: 221.6446075439453\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 221.6192169189453\n",
      "    load_time_ms: 4.138\n",
      "    num_steps_sampled: 714000\n",
      "    num_steps_trained: 714000\n",
      "    sample_time_ms: 58640.267\n",
      "    update_time_ms: 49.874\n",
      "  iterations_since_restore: 357\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.85714285714286\n",
      "    ram_util_percent: 73.20102040816326\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.50514666979345\n",
      "    mean_inference_ms: 11.946821656095846\n",
      "    mean_processing_ms: 21.934369772351296\n",
      "  time_since_restore: 24734.32641005516\n",
      "  time_this_iter_s: 68.6199779510498\n",
      "  time_total_s: 24734.32641005516\n",
      "  timestamp: 1575789986\n",
      "  timesteps_since_restore: 714000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 714000\n",
      "  training_iteration: 357\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24734 s, 357 iter, 714000 ts, 45.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-27-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.71192891716471\n",
      "  episode_reward_mean: 47.70612357301329\n",
      "  episode_reward_min: -21.014601913504087\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7907.887\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5073082447052002\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8515052795410156\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01828095316886902\n",
      "        policy_loss: -0.025060249492526054\n",
      "        total_loss: 222.1776885986328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.19338989257812\n",
      "    load_time_ms: 4.138\n",
      "    num_steps_sampled: 716000\n",
      "    num_steps_trained: 716000\n",
      "    sample_time_ms: 58717.876\n",
      "    update_time_ms: 50.011\n",
      "  iterations_since_restore: 358\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.63684210526316\n",
      "    ram_util_percent: 73.26105263157895\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.19191668993872\n",
      "    mean_inference_ms: 11.988916383256438\n",
      "    mean_processing_ms: 21.96882903681871\n",
      "  time_since_restore: 24800.748379945755\n",
      "  time_this_iter_s: 66.42196989059448\n",
      "  time_total_s: 24800.748379945755\n",
      "  timestamp: 1575790052\n",
      "  timesteps_since_restore: 716000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 716000\n",
      "  training_iteration: 358\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24800 s, 358 iter, 716000 ts, 47.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-28-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.71192891716471\n",
      "  episode_reward_mean: 49.233177750221714\n",
      "  episode_reward_min: -3.843697895591676\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8142.863\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5073082447052002\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.816941976547241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013010246679186821\n",
      "        policy_loss: -0.00852438434958458\n",
      "        total_loss: 212.2217254638672\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 212.2236785888672\n",
      "    load_time_ms: 4.12\n",
      "    num_steps_sampled: 718000\n",
      "    num_steps_trained: 718000\n",
      "    sample_time_ms: 58699.08\n",
      "    update_time_ms: 49.612\n",
      "  iterations_since_restore: 359\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.10309278350516\n",
      "    ram_util_percent: 73.39278350515464\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.03074019754703\n",
      "    mean_inference_ms: 12.038536895187333\n",
      "    mean_processing_ms: 22.0029384080108\n",
      "  time_since_restore: 24868.83583879471\n",
      "  time_this_iter_s: 68.08745884895325\n",
      "  time_total_s: 24868.83583879471\n",
      "  timestamp: 1575790121\n",
      "  timesteps_since_restore: 718000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 718000\n",
      "  training_iteration: 359\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24868 s, 359 iter, 718000 ts, 49.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-29-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.71192891716471\n",
      "  episode_reward_mean: 50.30122959290506\n",
      "  episode_reward_min: -3.843697895591676\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7882.853\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5073082447052002\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0106964111328125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024073684588074684\n",
      "        policy_loss: -0.03148958459496498\n",
      "        total_loss: 223.81088256835938\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 223.83009338378906\n",
      "    load_time_ms: 4.139\n",
      "    num_steps_sampled: 720000\n",
      "    num_steps_trained: 720000\n",
      "    sample_time_ms: 58742.379\n",
      "    update_time_ms: 50.61\n",
      "  iterations_since_restore: 360\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90752688172043\n",
      "    ram_util_percent: 73.44193548387094\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 144.009412307014\n",
      "    mean_inference_ms: 12.036989989412115\n",
      "    mean_processing_ms: 22.002495544177933\n",
      "  time_since_restore: 24934.24784398079\n",
      "  time_this_iter_s: 65.41200518608093\n",
      "  time_total_s: 24934.24784398079\n",
      "  timestamp: 1575790186\n",
      "  timesteps_since_restore: 720000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 720000\n",
      "  training_iteration: 360\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24934 s, 360 iter, 720000 ts, 50.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-30-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.71192891716471\n",
      "  episode_reward_mean: 50.60851585064819\n",
      "  episode_reward_min: -2.6503656150474524\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7709.251\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5073082447052002\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1075439453125\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014763624407351017\n",
      "        policy_loss: -0.0033458692487329245\n",
      "        total_loss: 238.4409942626953\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 238.43690490722656\n",
      "    load_time_ms: 3.134\n",
      "    num_steps_sampled: 722000\n",
      "    num_steps_trained: 722000\n",
      "    sample_time_ms: 58880.131\n",
      "    update_time_ms: 50.679\n",
      "  iterations_since_restore: 361\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.42903225806454\n",
      "    ram_util_percent: 73.52150537634408\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.34974599932642\n",
      "    mean_inference_ms: 11.996787393599723\n",
      "    mean_processing_ms: 21.962038598149306\n",
      "  time_since_restore: 24999.087898015976\n",
      "  time_this_iter_s: 64.84005403518677\n",
      "  time_total_s: 24999.087898015976\n",
      "  timestamp: 1575790251\n",
      "  timesteps_since_restore: 722000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 722000\n",
      "  training_iteration: 361\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 24999 s, 361 iter, 722000 ts, 50.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-31-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.71192891716471\n",
      "  episode_reward_mean: 47.61771263175965\n",
      "  episode_reward_min: -2.6503656150474524\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7715.022\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5073082447052002\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5071401596069336\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0529344417154789\n",
      "        policy_loss: 0.006150859873741865\n",
      "        total_loss: 237.4343719482422\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 237.4014129638672\n",
      "    load_time_ms: 3.115\n",
      "    num_steps_sampled: 724000\n",
      "    num_steps_trained: 724000\n",
      "    sample_time_ms: 58952.617\n",
      "    update_time_ms: 50.082\n",
      "  iterations_since_restore: 362\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.91595744680849\n",
      "    ram_util_percent: 73.42340425531916\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.32577120971203\n",
      "    mean_inference_ms: 11.995295766628969\n",
      "    mean_processing_ms: 21.960074496360807\n",
      "  time_since_restore: 25065.471071720123\n",
      "  time_this_iter_s: 66.38317370414734\n",
      "  time_total_s: 25065.471071720123\n",
      "  timestamp: 1575790317\n",
      "  timesteps_since_restore: 724000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 724000\n",
      "  training_iteration: 362\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25065 s, 362 iter, 724000 ts, 47.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-33-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.34618738377814\n",
      "  episode_reward_mean: 46.204877988980186\n",
      "  episode_reward_min: 0.4408005846509053\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7723.068\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7609623074531555\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.135422706604004\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002548189600929618\n",
      "        policy_loss: -0.009179932065308094\n",
      "        total_loss: 251.32803344726562\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 251.33531188964844\n",
      "    load_time_ms: 3.221\n",
      "    num_steps_sampled: 726000\n",
      "    num_steps_trained: 726000\n",
      "    sample_time_ms: 58976.165\n",
      "    update_time_ms: 50.561\n",
      "  iterations_since_restore: 363\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.02021276595747\n",
      "    ram_util_percent: 73.43085106382979\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.86397885963973\n",
      "    mean_inference_ms: 11.967645254601155\n",
      "    mean_processing_ms: 21.93380938847513\n",
      "  time_since_restore: 25131.497752189636\n",
      "  time_this_iter_s: 66.02668046951294\n",
      "  time_total_s: 25131.497752189636\n",
      "  timestamp: 1575790383\n",
      "  timesteps_since_restore: 726000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 726000\n",
      "  training_iteration: 363\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25131 s, 363 iter, 726000 ts, 46.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-34-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.34618738377814\n",
      "  episode_reward_mean: 44.128412024752215\n",
      "  episode_reward_min: -16.605826891328196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7961.464\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.38048115372657776\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.6422841548919678\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004579482134431601\n",
      "        policy_loss: -0.011612094938755035\n",
      "        total_loss: 244.20059204101562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 244.21043395996094\n",
      "    load_time_ms: 3.03\n",
      "    num_steps_sampled: 728000\n",
      "    num_steps_trained: 728000\n",
      "    sample_time_ms: 58789.34\n",
      "    update_time_ms: 50.691\n",
      "  iterations_since_restore: 364\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.95\n",
      "    ram_util_percent: 73.5173469387755\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.9002016608366\n",
      "    mean_inference_ms: 11.907506708791589\n",
      "    mean_processing_ms: 21.887632452303922\n",
      "  time_since_restore: 25199.481638669968\n",
      "  time_this_iter_s: 67.98388648033142\n",
      "  time_total_s: 25199.481638669968\n",
      "  timestamp: 1575790451\n",
      "  timesteps_since_restore: 728000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 728000\n",
      "  training_iteration: 364\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25199 s, 364 iter, 728000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-35-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.34618738377814\n",
      "  episode_reward_mean: 41.28933067684235\n",
      "  episode_reward_min: -16.605826891328196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7736.882\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19024057686328888\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8886330127716064\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014855892397463322\n",
      "        policy_loss: -0.006009659264236689\n",
      "        total_loss: 228.92369079589844\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 228.9269256591797\n",
      "    load_time_ms: 3.042\n",
      "    num_steps_sampled: 730000\n",
      "    num_steps_trained: 730000\n",
      "    sample_time_ms: 58915.545\n",
      "    update_time_ms: 50.418\n",
      "  iterations_since_restore: 365\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.49791666666665\n",
      "    ram_util_percent: 73.60520833333332\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.12203001947427\n",
      "    mean_inference_ms: 11.922187023606915\n",
      "    mean_processing_ms: 21.89680889814092\n",
      "  time_since_restore: 25266.805514097214\n",
      "  time_this_iter_s: 67.3238754272461\n",
      "  time_total_s: 25266.805514097214\n",
      "  timestamp: 1575790519\n",
      "  timesteps_since_restore: 730000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 730000\n",
      "  training_iteration: 365\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25266 s, 365 iter, 730000 ts, 41.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-36-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.34618738377814\n",
      "  episode_reward_mean: 40.318657662529304\n",
      "  episode_reward_min: -16.605826891328196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7703.084\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19024057686328888\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5097758769989014\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011293555609881878\n",
      "        policy_loss: -0.012941419146955013\n",
      "        total_loss: 216.39434814453125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 216.40518188476562\n",
      "    load_time_ms: 3.417\n",
      "    num_steps_sampled: 732000\n",
      "    num_steps_trained: 732000\n",
      "    sample_time_ms: 58909.746\n",
      "    update_time_ms: 49.592\n",
      "  iterations_since_restore: 366\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.25106382978724\n",
      "    ram_util_percent: 73.68404255319147\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.05248390418757\n",
      "    mean_inference_ms: 11.917156657775585\n",
      "    mean_processing_ms: 21.90049217380124\n",
      "  time_since_restore: 25332.80396962166\n",
      "  time_this_iter_s: 65.99845552444458\n",
      "  time_total_s: 25332.80396962166\n",
      "  timestamp: 1575790585\n",
      "  timesteps_since_restore: 732000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 732000\n",
      "  training_iteration: 366\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25332 s, 366 iter, 732000 ts, 40.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-37-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.29539830889829\n",
      "  episode_reward_mean: 42.276061119581506\n",
      "  episode_reward_min: -16.605826891328196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7700.182\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19024057686328888\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2053349018096924\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.033968109637498856\n",
      "        policy_loss: 0.000638697121758014\n",
      "        total_loss: 204.6669464111328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 204.65985107421875\n",
      "    load_time_ms: 3.491\n",
      "    num_steps_sampled: 734000\n",
      "    num_steps_trained: 734000\n",
      "    sample_time_ms: 58706.842\n",
      "    update_time_ms: 49.185\n",
      "  iterations_since_restore: 367\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.24\n",
      "    ram_util_percent: 73.75578947368417\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.692042844794\n",
      "    mean_inference_ms: 11.895916631677906\n",
      "    mean_processing_ms: 21.881253019695794\n",
      "  time_since_restore: 25399.379539966583\n",
      "  time_this_iter_s: 66.57557034492493\n",
      "  time_total_s: 25399.379539966583\n",
      "  timestamp: 1575790651\n",
      "  timesteps_since_restore: 734000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 734000\n",
      "  training_iteration: 367\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25399 s, 367 iter, 734000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-38-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.60174499719373\n",
      "  episode_reward_mean: 41.81422709649564\n",
      "  episode_reward_min: -16.605826891328196\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7938.816\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19024057686328888\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0911929607391357\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.19056981801986694\n",
      "        policy_loss: 0.04755288362503052\n",
      "        total_loss: 218.4210968017578\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.33731079101562\n",
      "    load_time_ms: 3.475\n",
      "    num_steps_sampled: 736000\n",
      "    num_steps_trained: 736000\n",
      "    sample_time_ms: 58635.608\n",
      "    update_time_ms: 48.968\n",
      "  iterations_since_restore: 368\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.55360824742269\n",
      "    ram_util_percent: 73.59072164948455\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.80625678955684\n",
      "    mean_inference_ms: 11.900458092110128\n",
      "    mean_processing_ms: 21.883341765207078\n",
      "  time_since_restore: 25467.472044706345\n",
      "  time_this_iter_s: 68.09250473976135\n",
      "  time_total_s: 25467.472044706345\n",
      "  timestamp: 1575790720\n",
      "  timesteps_since_restore: 736000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 736000\n",
      "  training_iteration: 368\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25467 s, 368 iter, 736000 ts, 41.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-39-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.60174499719373\n",
      "  episode_reward_mean: 42.23622158583358\n",
      "  episode_reward_min: -10.767128791604115\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7710.967\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2853608727455139\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1886837482452393\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.048171546310186386\n",
      "        policy_loss: 0.004047927912324667\n",
      "        total_loss: 232.1220245361328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 232.10430908203125\n",
      "    load_time_ms: 3.46\n",
      "    num_steps_sampled: 738000\n",
      "    num_steps_trained: 738000\n",
      "    sample_time_ms: 58698.922\n",
      "    update_time_ms: 48.988\n",
      "  iterations_since_restore: 369\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.46000000000001\n",
      "    ram_util_percent: 73.63894736842106\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.67958495764248\n",
      "    mean_inference_ms: 11.893655829055719\n",
      "    mean_processing_ms: 21.87699258887505\n",
      "  time_since_restore: 25533.926785945892\n",
      "  time_this_iter_s: 66.45474123954773\n",
      "  time_total_s: 25533.926785945892\n",
      "  timestamp: 1575790786\n",
      "  timesteps_since_restore: 738000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 738000\n",
      "  training_iteration: 369\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25533 s, 369 iter, 738000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-40-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.60174499719373\n",
      "  episode_reward_mean: 41.468806435108164\n",
      "  episode_reward_min: -10.767128791604115\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7713.644\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4280413091182709\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.8481709957122803\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.036760419607162476\n",
      "        policy_loss: 0.018414275720715523\n",
      "        total_loss: 213.27952575683594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 213.24533081054688\n",
      "    load_time_ms: 3.65\n",
      "    num_steps_sampled: 740000\n",
      "    num_steps_trained: 740000\n",
      "    sample_time_ms: 58718.404\n",
      "    update_time_ms: 48.745\n",
      "  iterations_since_restore: 370\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.81075268817204\n",
      "    ram_util_percent: 73.69569892473119\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.58778085216537\n",
      "    mean_inference_ms: 11.889098732364145\n",
      "    mean_processing_ms: 21.867508639759482\n",
      "  time_since_restore: 25599.54725766182\n",
      "  time_this_iter_s: 65.62047171592712\n",
      "  time_total_s: 25599.54725766182\n",
      "  timestamp: 1575790852\n",
      "  timesteps_since_restore: 740000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 740000\n",
      "  training_iteration: 370\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25599 s, 370 iter, 740000 ts, 41.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-41-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.44687526839496\n",
      "  episode_reward_mean: 42.99998194924538\n",
      "  episode_reward_min: -10.767128791604115\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7724.509\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4280413091182709\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.172337055206299\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024601547047495842\n",
      "        policy_loss: 0.0007976258057169616\n",
      "        total_loss: 236.7256622314453\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 236.7144012451172\n",
      "    load_time_ms: 3.518\n",
      "    num_steps_sampled: 742000\n",
      "    num_steps_trained: 742000\n",
      "    sample_time_ms: 58830.802\n",
      "    update_time_ms: 48.126\n",
      "  iterations_since_restore: 371\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.5578947368421\n",
      "    ram_util_percent: 73.78105263157896\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.7403057418018\n",
      "    mean_inference_ms: 11.895730967093959\n",
      "    mean_processing_ms: 21.867272739226202\n",
      "  time_since_restore: 25665.60740995407\n",
      "  time_this_iter_s: 66.06015229225159\n",
      "  time_total_s: 25665.60740995407\n",
      "  timestamp: 1575790918\n",
      "  timesteps_since_restore: 742000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 742000\n",
      "  training_iteration: 371\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25665 s, 371 iter, 742000 ts, 43 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-43-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.44687526839496\n",
      "  episode_reward_mean: 42.24169887111453\n",
      "  episode_reward_min: -10.767128791604115\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7925.076\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4280413091182709\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.095108985900879\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05689201503992081\n",
      "        policy_loss: 0.004238185938447714\n",
      "        total_loss: 205.7963104248047\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 205.76771545410156\n",
      "    load_time_ms: 3.566\n",
      "    num_steps_sampled: 744000\n",
      "    num_steps_trained: 744000\n",
      "    sample_time_ms: 58815.622\n",
      "    update_time_ms: 47.812\n",
      "  iterations_since_restore: 372\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1020618556701\n",
      "    ram_util_percent: 73.82783505154639\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.48793124567018\n",
      "    mean_inference_ms: 11.939152779066731\n",
      "    mean_processing_ms: 21.89037879328909\n",
      "  time_since_restore: 25733.911390542984\n",
      "  time_this_iter_s: 68.30398058891296\n",
      "  time_total_s: 25733.911390542984\n",
      "  timestamp: 1575790986\n",
      "  timesteps_since_restore: 744000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 744000\n",
      "  training_iteration: 372\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25733 s, 372 iter, 744000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-44-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.44687526839496\n",
      "  episode_reward_mean: 43.33581402110696\n",
      "  episode_reward_min: -10.767128791604115\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8128.044\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.6420619487762451\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8290886878967285\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03042263723909855\n",
      "        policy_loss: 0.0037778406403958797\n",
      "        total_loss: 224.467529296875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 224.44427490234375\n",
      "    load_time_ms: 6.126\n",
      "    num_steps_sampled: 746000\n",
      "    num_steps_trained: 746000\n",
      "    sample_time_ms: 58772.419\n",
      "    update_time_ms: 54.316\n",
      "  iterations_since_restore: 373\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.90515463917525\n",
      "    ram_util_percent: 73.95154639175257\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.44515262738005\n",
      "    mean_inference_ms: 11.934640205072117\n",
      "    mean_processing_ms: 21.89206736419118\n",
      "  time_since_restore: 25801.61075592041\n",
      "  time_this_iter_s: 67.69936537742615\n",
      "  time_total_s: 25801.61075592041\n",
      "  timestamp: 1575791054\n",
      "  timesteps_since_restore: 746000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 746000\n",
      "  training_iteration: 373\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25801 s, 373 iter, 746000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-45-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.44687526839496\n",
      "  episode_reward_mean: 42.96726441434387\n",
      "  episode_reward_min: -12.18829292586079\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7898.859\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.6420619487762451\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.888730764389038\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0355539508163929\n",
      "        policy_loss: 0.013039332814514637\n",
      "        total_loss: 231.63909912109375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 231.6031494140625\n",
      "    load_time_ms: 6.052\n",
      "    num_steps_sampled: 748000\n",
      "    num_steps_trained: 748000\n",
      "    sample_time_ms: 58856.432\n",
      "    update_time_ms: 53.516\n",
      "  iterations_since_restore: 374\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.39894736842106\n",
      "    ram_util_percent: 73.79368421052634\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.06274302526745\n",
      "    mean_inference_ms: 11.970647819474088\n",
      "    mean_processing_ms: 21.93146829721438\n",
      "  time_since_restore: 25868.137351989746\n",
      "  time_this_iter_s: 66.52659606933594\n",
      "  time_total_s: 25868.137351989746\n",
      "  timestamp: 1575791121\n",
      "  timesteps_since_restore: 748000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 748000\n",
      "  training_iteration: 374\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25868 s, 374 iter, 748000 ts, 43 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-46-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.44687526839496\n",
      "  episode_reward_mean: 45.32002135265302\n",
      "  episode_reward_min: -12.18829292586079\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7906.058\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.6420619487762451\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.149973154067993\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005191884934902191\n",
      "        policy_loss: -0.014286468736827374\n",
      "        total_loss: 235.33131408691406\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 235.34225463867188\n",
      "    load_time_ms: 6.036\n",
      "    num_steps_sampled: 750000\n",
      "    num_steps_trained: 750000\n",
      "    sample_time_ms: 58748.626\n",
      "    update_time_ms: 55.145\n",
      "  iterations_since_restore: 375\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.45473684210525\n",
      "    ram_util_percent: 73.84210526315792\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.7780001201177\n",
      "    mean_inference_ms: 11.950639303714782\n",
      "    mean_processing_ms: 21.92032615376961\n",
      "  time_since_restore: 25934.48526120186\n",
      "  time_this_iter_s: 66.34790921211243\n",
      "  time_total_s: 25934.48526120186\n",
      "  timestamp: 1575791187\n",
      "  timesteps_since_restore: 750000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 750000\n",
      "  training_iteration: 375\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.1/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 25934 s, 375 iter, 750000 ts, 45.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-47-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.62205784261923\n",
      "  episode_reward_mean: 43.77727600160289\n",
      "  episode_reward_min: -12.18829292586079\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7930.977\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32103097438812256\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.704664468765259\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013537950813770294\n",
      "        policy_loss: -0.019496621564030647\n",
      "        total_loss: 244.025390625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 244.04051208496094\n",
      "    load_time_ms: 5.633\n",
      "    num_steps_sampled: 752000\n",
      "    num_steps_trained: 752000\n",
      "    sample_time_ms: 58698.41\n",
      "    update_time_ms: 56.342\n",
      "  iterations_since_restore: 376\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.51505376344086\n",
      "    ram_util_percent: 73.91075268817202\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.2521511066025\n",
      "    mean_inference_ms: 11.92007128097007\n",
      "    mean_processing_ms: 21.900917605063196\n",
      "  time_since_restore: 26000.234358787537\n",
      "  time_this_iter_s: 65.7490975856781\n",
      "  time_total_s: 26000.234358787537\n",
      "  timestamp: 1575791253\n",
      "  timesteps_since_restore: 752000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 752000\n",
      "  training_iteration: 376\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26000 s, 376 iter, 752000 ts, 43.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-48-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.62205784261923\n",
      "  episode_reward_mean: 45.143789066671495\n",
      "  episode_reward_min: -15.41042725591821\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8216.572\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32103097438812256\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1844494342803955\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04810921475291252\n",
      "        policy_loss: 0.0008801312651485205\n",
      "        total_loss: 226.70069885253906\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 226.68434143066406\n",
      "    load_time_ms: 5.67\n",
      "    num_steps_sampled: 754000\n",
      "    num_steps_trained: 754000\n",
      "    sample_time_ms: 58660.035\n",
      "    update_time_ms: 55.843\n",
      "  iterations_since_restore: 377\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.45050505050503\n",
      "    ram_util_percent: 74.04343434343434\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.20259458769095\n",
      "    mean_inference_ms: 11.916340053236272\n",
      "    mean_processing_ms: 21.902264260154354\n",
      "  time_since_restore: 26069.27090072632\n",
      "  time_this_iter_s: 69.03654193878174\n",
      "  time_total_s: 26069.27090072632\n",
      "  timestamp: 1575791322\n",
      "  timesteps_since_restore: 754000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 754000\n",
      "  training_iteration: 377\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26069 s, 377 iter, 754000 ts, 45.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-49-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.62205784261923\n",
      "  episode_reward_mean: 44.236490646569465\n",
      "  episode_reward_min: -15.41042725591821\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7982.773\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.48154646158218384\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7900876998901367\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.050408750772476196\n",
      "        policy_loss: 0.01818959414958954\n",
      "        total_loss: 216.8685302734375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.8260955810547\n",
      "    load_time_ms: 5.718\n",
      "    num_steps_sampled: 756000\n",
      "    num_steps_trained: 756000\n",
      "    sample_time_ms: 58663.783\n",
      "    update_time_ms: 56.856\n",
      "  iterations_since_restore: 378\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.60744680851064\n",
      "    ram_util_percent: 74.10638297872339\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.21979295764936\n",
      "    mean_inference_ms: 11.917950700661361\n",
      "    mean_processing_ms: 21.89419709811019\n",
      "  time_since_restore: 26135.078278303146\n",
      "  time_this_iter_s: 65.807377576828\n",
      "  time_total_s: 26135.078278303146\n",
      "  timestamp: 1575791388\n",
      "  timesteps_since_restore: 756000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 756000\n",
      "  training_iteration: 378\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26135 s, 378 iter, 756000 ts, 44.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-50-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.62205784261923\n",
      "  episode_reward_mean: 45.4776285999566\n",
      "  episode_reward_min: -15.41042725591821\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7978.32\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7223197221755981\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.652068853378296\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014827581122517586\n",
      "        policy_loss: -0.0016036415472626686\n",
      "        total_loss: 212.59576416015625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 212.5866241455078\n",
      "    load_time_ms: 5.696\n",
      "    num_steps_sampled: 758000\n",
      "    num_steps_trained: 758000\n",
      "    sample_time_ms: 58620.377\n",
      "    update_time_ms: 56.968\n",
      "  iterations_since_restore: 379\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.15531914893616\n",
      "    ram_util_percent: 74.09680851063831\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.10132847571973\n",
      "    mean_inference_ms: 11.909379000613772\n",
      "    mean_processing_ms: 21.861536897290588\n",
      "  time_since_restore: 26201.059396266937\n",
      "  time_this_iter_s: 65.9811179637909\n",
      "  time_total_s: 26201.059396266937\n",
      "  timestamp: 1575791454\n",
      "  timesteps_since_restore: 758000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 758000\n",
      "  training_iteration: 379\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26201 s, 379 iter, 758000 ts, 45.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-52-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.62205784261923\n",
      "  episode_reward_mean: 42.798121573320756\n",
      "  episode_reward_min: -15.41042725591821\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7976.271\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7223197221755981\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.6537463665008545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015258300118148327\n",
      "        policy_loss: -0.0011967334430664778\n",
      "        total_loss: 210.3105010986328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.30062866210938\n",
      "    load_time_ms: 6.208\n",
      "    num_steps_sampled: 760000\n",
      "    num_steps_trained: 760000\n",
      "    sample_time_ms: 58763.486\n",
      "    update_time_ms: 56.97\n",
      "  iterations_since_restore: 380\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.88437499999999\n",
      "    ram_util_percent: 73.97395833333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.41446529983742\n",
      "    mean_inference_ms: 11.927233036269461\n",
      "    mean_processing_ms: 21.877878634182476\n",
      "  time_since_restore: 26268.09313392639\n",
      "  time_this_iter_s: 67.03373765945435\n",
      "  time_total_s: 26268.09313392639\n",
      "  timestamp: 1575791521\n",
      "  timesteps_since_restore: 760000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 760000\n",
      "  training_iteration: 380\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26268 s, 380 iter, 760000 ts, 42.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-53-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.07573208809147\n",
      "  episode_reward_mean: 43.313215726952265\n",
      "  episode_reward_min: -15.41042725591821\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8254.835\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7223197221755981\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4819815158843994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005176248960196972\n",
      "        policy_loss: -0.004941062070429325\n",
      "        total_loss: 203.57064819335938\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 203.57186889648438\n",
      "    load_time_ms: 6.265\n",
      "    num_steps_sampled: 762000\n",
      "    num_steps_trained: 762000\n",
      "    sample_time_ms: 58704.933\n",
      "    update_time_ms: 57.72\n",
      "  iterations_since_restore: 381\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86288659793816\n",
      "    ram_util_percent: 74.06288659793813\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.75277262996696\n",
      "    mean_inference_ms: 11.947147743123537\n",
      "    mean_processing_ms: 21.891309899520046\n",
      "  time_since_restore: 26336.360436439514\n",
      "  time_this_iter_s: 68.26730251312256\n",
      "  time_total_s: 26336.360436439514\n",
      "  timestamp: 1575791589\n",
      "  timesteps_since_restore: 762000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 762000\n",
      "  training_iteration: 381\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26336 s, 381 iter, 762000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-54-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.28204038645424\n",
      "  episode_reward_mean: 41.24000413919708\n",
      "  episode_reward_min: -7.627143363269422\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8066.682\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3611598610877991\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.131652593612671\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005283299367874861\n",
      "        policy_loss: -0.01156844012439251\n",
      "        total_loss: 209.47584533691406\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 209.48550415039062\n",
      "    load_time_ms: 6.298\n",
      "    num_steps_sampled: 764000\n",
      "    num_steps_trained: 764000\n",
      "    sample_time_ms: 58628.461\n",
      "    update_time_ms: 58.687\n",
      "  iterations_since_restore: 382\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.46382978723405\n",
      "    ram_util_percent: 74.17446808510638\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.2276950004096\n",
      "    mean_inference_ms: 11.917095105640287\n",
      "    mean_processing_ms: 21.874944447701814\n",
      "  time_since_restore: 26401.945798397064\n",
      "  time_this_iter_s: 65.58536195755005\n",
      "  time_total_s: 26401.945798397064\n",
      "  timestamp: 1575791655\n",
      "  timesteps_since_restore: 764000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 764000\n",
      "  training_iteration: 382\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26401 s, 382 iter, 764000 ts, 41.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-55-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 80.28204038645424\n",
      "  episode_reward_mean: 42.107102101255535\n",
      "  episode_reward_min: -7.627143363269422\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7852.606\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.18057993054389954\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.60890531539917\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013096358627080917\n",
      "        policy_loss: -0.018953196704387665\n",
      "        total_loss: 205.6236572265625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 205.64022827148438\n",
      "    load_time_ms: 3.68\n",
      "    num_steps_sampled: 766000\n",
      "    num_steps_trained: 766000\n",
      "    sample_time_ms: 58697.309\n",
      "    update_time_ms: 52.122\n",
      "  iterations_since_restore: 383\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.43085106382979\n",
      "    ram_util_percent: 74.21489361702126\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.32078982262442\n",
      "    mean_inference_ms: 11.920498666950639\n",
      "    mean_processing_ms: 21.877736968253995\n",
      "  time_since_restore: 26468.112825870514\n",
      "  time_this_iter_s: 66.1670274734497\n",
      "  time_total_s: 26468.112825870514\n",
      "  timestamp: 1575791721\n",
      "  timesteps_since_restore: 766000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 766000\n",
      "  training_iteration: 383\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26468 s, 383 iter, 766000 ts, 42.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-56-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.14528865159977\n",
      "  episode_reward_mean: 41.88744381445275\n",
      "  episode_reward_min: -7.627143363269422\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7858.992\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.18057993054389954\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2861273288726807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008731975220143795\n",
      "        policy_loss: -0.013073896057903767\n",
      "        total_loss: 206.94427490234375\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 206.95578002929688\n",
      "    load_time_ms: 3.681\n",
      "    num_steps_sampled: 768000\n",
      "    num_steps_trained: 768000\n",
      "    sample_time_ms: 58851.488\n",
      "    update_time_ms: 52.901\n",
      "  iterations_since_restore: 384\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.98041237113402\n",
      "    ram_util_percent: 74.31649484536082\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.7854929187143\n",
      "    mean_inference_ms: 11.889484159228594\n",
      "    mean_processing_ms: 21.864472205465745\n",
      "  time_since_restore: 26536.252264738083\n",
      "  time_this_iter_s: 68.13943886756897\n",
      "  time_total_s: 26536.252264738083\n",
      "  timestamp: 1575791789\n",
      "  timesteps_since_restore: 768000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 768000\n",
      "  training_iteration: 384\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26536 s, 384 iter, 768000 ts, 41.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-57-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.07807919863492\n",
      "  episode_reward_mean: 45.787627107503575\n",
      "  episode_reward_min: -1.3011878979533384\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8123.319\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09028996527194977\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0975069999694824\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010435516014695168\n",
      "        policy_loss: -0.01184635516256094\n",
      "        total_loss: 210.25421142578125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 210.26512145996094\n",
      "    load_time_ms: 3.672\n",
      "    num_steps_sampled: 770000\n",
      "    num_steps_trained: 770000\n",
      "    sample_time_ms: 58867.717\n",
      "    update_time_ms: 52.439\n",
      "  iterations_since_restore: 385\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.289898989899\n",
      "    ram_util_percent: 74.4141414141414\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.72779170856325\n",
      "    mean_inference_ms: 11.888621596676295\n",
      "    mean_processing_ms: 21.850045266371986\n",
      "  time_since_restore: 26605.388981103897\n",
      "  time_this_iter_s: 69.13671636581421\n",
      "  time_total_s: 26605.388981103897\n",
      "  timestamp: 1575791858\n",
      "  timesteps_since_restore: 770000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 770000\n",
      "  training_iteration: 385\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26605 s, 385 iter, 770000 ts, 45.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-58-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.81801748202903\n",
      "  episode_reward_mean: 46.27585273475179\n",
      "  episode_reward_min: 0.8289492745314756\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8306.961\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09028996527194977\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3183295726776123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03022613562643528\n",
      "        policy_loss: -0.025927184149622917\n",
      "        total_loss: 222.9444122314453\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.96768188476562\n",
      "    load_time_ms: 4.889\n",
      "    num_steps_sampled: 772000\n",
      "    num_steps_trained: 772000\n",
      "    sample_time_ms: 58671.784\n",
      "    update_time_ms: 51.643\n",
      "  iterations_since_restore: 386\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.88494623655913\n",
      "    ram_util_percent: 74.35913978494624\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.9104840902701\n",
      "    mean_inference_ms: 11.902152490792927\n",
      "    mean_processing_ms: 21.86439201746962\n",
      "  time_since_restore: 26671.01133084297\n",
      "  time_this_iter_s: 65.6223497390747\n",
      "  time_total_s: 26671.01133084297\n",
      "  timestamp: 1575791924\n",
      "  timesteps_since_restore: 772000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 772000\n",
      "  training_iteration: 386\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26671 s, 386 iter, 772000 ts, 46.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_02-59-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.81801748202903\n",
      "  episode_reward_mean: 45.039122751693824\n",
      "  episode_reward_min: -13.418122476705253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8031.817\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09028996527194977\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.7222158908843994\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017256924882531166\n",
      "        policy_loss: -0.014247708953917027\n",
      "        total_loss: 226.61709594726562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 226.62985229492188\n",
      "    load_time_ms: 4.82\n",
      "    num_steps_sampled: 774000\n",
      "    num_steps_trained: 774000\n",
      "    sample_time_ms: 58741.49\n",
      "    update_time_ms: 51.553\n",
      "  iterations_since_restore: 387\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.96979166666667\n",
      "    ram_util_percent: 74.30520833333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.0777925559276\n",
      "    mean_inference_ms: 11.911177165979785\n",
      "    mean_processing_ms: 21.855961393119983\n",
      "  time_since_restore: 26737.993429899216\n",
      "  time_this_iter_s: 66.9820990562439\n",
      "  time_total_s: 26737.993429899216\n",
      "  timestamp: 1575791991\n",
      "  timesteps_since_restore: 774000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 774000\n",
      "  training_iteration: 387\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26737 s, 387 iter, 774000 ts, 45 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-00-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.81801748202903\n",
      "  episode_reward_mean: 43.32387579866035\n",
      "  episode_reward_min: -24.043855342208857\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8032.972\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09028996527194977\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.366281509399414\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02909090742468834\n",
      "        policy_loss: -0.003015565685927868\n",
      "        total_loss: 228.94778442382812\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 228.94821166992188\n",
      "    load_time_ms: 4.835\n",
      "    num_steps_sampled: 776000\n",
      "    num_steps_trained: 776000\n",
      "    sample_time_ms: 58866.079\n",
      "    update_time_ms: 50.261\n",
      "  iterations_since_restore: 388\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.778125\n",
      "    ram_util_percent: 74.38333333333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.69285713196797\n",
      "    mean_inference_ms: 11.88803161705255\n",
      "    mean_processing_ms: 21.84128642217943\n",
      "  time_since_restore: 26805.04524421692\n",
      "  time_this_iter_s: 67.05181431770325\n",
      "  time_total_s: 26805.04524421692\n",
      "  timestamp: 1575792058\n",
      "  timesteps_since_restore: 776000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 776000\n",
      "  training_iteration: 388\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26805 s, 388 iter, 776000 ts, 43.3 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:02:06,580\tWARNING util.py:145 -- The `process_trial` operation took 0.1649329662322998 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-02-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.81801748202903\n",
      "  episode_reward_mean: 43.86011950191112\n",
      "  episode_reward_min: -24.043855342208857\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8197.25\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09028996527194977\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.012434720993042\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03239030763506889\n",
      "        policy_loss: -0.012761090882122517\n",
      "        total_loss: 229.48171997070312\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 229.4915313720703\n",
      "    load_time_ms: 4.933\n",
      "    num_steps_sampled: 778000\n",
      "    num_steps_trained: 778000\n",
      "    sample_time_ms: 58878.309\n",
      "    update_time_ms: 50.504\n",
      "  iterations_since_restore: 389\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.57708333333333\n",
      "    ram_util_percent: 74.48854166666666\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.99464947911815\n",
      "    mean_inference_ms: 11.907130146844922\n",
      "    mean_processing_ms: 21.85764549068494\n",
      "  time_since_restore: 26872.914989471436\n",
      "  time_this_iter_s: 67.8697452545166\n",
      "  time_total_s: 26872.914989471436\n",
      "  timestamp: 1575792126\n",
      "  timesteps_since_restore: 778000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 778000\n",
      "  training_iteration: 389\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26872 s, 389 iter, 778000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-03-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.81801748202903\n",
      "  episode_reward_mean: 42.243267799552925\n",
      "  episode_reward_min: -24.043855342208857\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8446.731\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.09028996527194977\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.7951401472091675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.4584072232246399\n",
      "        policy_loss: 0.07860424369573593\n",
      "        total_loss: 207.8755645751953\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 207.7555694580078\n",
      "    load_time_ms: 4.485\n",
      "    num_steps_sampled: 780000\n",
      "    num_steps_trained: 780000\n",
      "    sample_time_ms: 58577.63\n",
      "    update_time_ms: 58.848\n",
      "  iterations_since_restore: 390\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.91562499999999\n",
      "    ram_util_percent: 74.596875\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.38206249597815\n",
      "    mean_inference_ms: 11.928689655805238\n",
      "    mean_processing_ms: 21.879780494594314\n",
      "  time_since_restore: 26939.515054941177\n",
      "  time_this_iter_s: 66.60006546974182\n",
      "  time_total_s: 26939.515054941177\n",
      "  timestamp: 1575792193\n",
      "  timesteps_since_restore: 780000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 780000\n",
      "  training_iteration: 390\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 26939 s, 390 iter, 780000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-04-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.82223199414038\n",
      "  episode_reward_mean: 41.8852340182097\n",
      "  episode_reward_min: -24.043855342208857\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8169.999\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13543494045734406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.589548349380493\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03315134719014168\n",
      "        policy_loss: -0.0044466303661465645\n",
      "        total_loss: 246.82421875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 246.82423400878906\n",
      "    load_time_ms: 4.428\n",
      "    num_steps_sampled: 782000\n",
      "    num_steps_trained: 782000\n",
      "    sample_time_ms: 58678.107\n",
      "    update_time_ms: 58.245\n",
      "  iterations_since_restore: 391\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.96631578947368\n",
      "    ram_util_percent: 74.66105263157895\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.83361537987705\n",
      "    mean_inference_ms: 11.893023416043583\n",
      "    mean_processing_ms: 21.84118082100283\n",
      "  time_since_restore: 27006.018738031387\n",
      "  time_this_iter_s: 66.50368309020996\n",
      "  time_total_s: 27006.018738031387\n",
      "  timestamp: 1575792259\n",
      "  timesteps_since_restore: 782000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 782000\n",
      "  training_iteration: 391\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27006 s, 391 iter, 782000 ts, 41.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:05:27,608\tWARNING util.py:145 -- The `process_trial` operation took 0.11884236335754395 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-05-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.82223199414038\n",
      "  episode_reward_mean: 43.3073571078071\n",
      "  episode_reward_min: -24.043855342208857\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8147.262\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13543494045734406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9406211376190186\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02556709013879299\n",
      "        policy_loss: -0.0016321887960657477\n",
      "        total_loss: 223.71156311035156\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 223.70974731445312\n",
      "    load_time_ms: 4.335\n",
      "    num_steps_sampled: 784000\n",
      "    num_steps_trained: 784000\n",
      "    sample_time_ms: 58907.897\n",
      "    update_time_ms: 57.901\n",
      "  iterations_since_restore: 392\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.390625\n",
      "    ram_util_percent: 74.55104166666668\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.783816333674\n",
      "    mean_inference_ms: 11.888437095890264\n",
      "    mean_processing_ms: 21.841403527514245\n",
      "  time_since_restore: 27073.676395893097\n",
      "  time_this_iter_s: 67.6576578617096\n",
      "  time_total_s: 27073.676395893097\n",
      "  timestamp: 1575792327\n",
      "  timesteps_since_restore: 784000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 784000\n",
      "  training_iteration: 392\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27073 s, 392 iter, 784000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-06-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.81192334950875\n",
      "  episode_reward_mean: 44.93317828041807\n",
      "  episode_reward_min: -15.632865466988306\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8154.622\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13543494045734406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9355859756469727\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01985206827521324\n",
      "        policy_loss: -0.012385280802845955\n",
      "        total_loss: 218.40780639648438\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 218.4175262451172\n",
      "    load_time_ms: 4.305\n",
      "    num_steps_sampled: 786000\n",
      "    num_steps_trained: 786000\n",
      "    sample_time_ms: 58866.366\n",
      "    update_time_ms: 62.176\n",
      "  iterations_since_restore: 393\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90638297872341\n",
      "    ram_util_percent: 74.6063829787234\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.16654014088027\n",
      "    mean_inference_ms: 11.914878201183745\n",
      "    mean_processing_ms: 21.86755873725513\n",
      "  time_since_restore: 27139.55024266243\n",
      "  time_this_iter_s: 65.87384676933289\n",
      "  time_total_s: 27139.55024266243\n",
      "  timestamp: 1575792393\n",
      "  timesteps_since_restore: 786000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 786000\n",
      "  training_iteration: 393\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27139 s, 393 iter, 786000 ts, 44.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-07-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.81192334950875\n",
      "  episode_reward_mean: 43.9212680919412\n",
      "  episode_reward_min: -15.632865466988306\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8413.792\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13543494045734406\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.043720006942749\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.3007521331310272\n",
      "        policy_loss: 0.030167272314429283\n",
      "        total_loss: 205.0624237060547\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 204.9915008544922\n",
      "    load_time_ms: 4.361\n",
      "    num_steps_sampled: 788000\n",
      "    num_steps_trained: 788000\n",
      "    sample_time_ms: 58637.378\n",
      "    update_time_ms: 62.896\n",
      "  iterations_since_restore: 394\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.92755102040816\n",
      "    ram_util_percent: 74.6387755102041\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.63012119419557\n",
      "    mean_inference_ms: 11.944894382282078\n",
      "    mean_processing_ms: 21.884057478099486\n",
      "  time_since_restore: 27208.000918149948\n",
      "  time_this_iter_s: 68.45067548751831\n",
      "  time_total_s: 27208.000918149948\n",
      "  timestamp: 1575792461\n",
      "  timesteps_since_restore: 788000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 788000\n",
      "  training_iteration: 394\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27208 s, 394 iter, 788000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-08-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.81192334950875\n",
      "  episode_reward_mean: 41.938324567311135\n",
      "  episode_reward_min: -15.632865466988306\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8153.23\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20315241813659668\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1417038440704346\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06446963548660278\n",
      "        policy_loss: 0.02997875213623047\n",
      "        total_loss: 215.46664428710938\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 215.42359924316406\n",
      "    load_time_ms: 4.35\n",
      "    num_steps_sampled: 790000\n",
      "    num_steps_trained: 790000\n",
      "    sample_time_ms: 58629.08\n",
      "    update_time_ms: 62.277\n",
      "  iterations_since_restore: 395\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.27684210526316\n",
      "    ram_util_percent: 74.7684210526316\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.94932044097695\n",
      "    mean_inference_ms: 11.903762578420183\n",
      "    mean_processing_ms: 21.84468153747619\n",
      "  time_since_restore: 27274.433744192123\n",
      "  time_this_iter_s: 66.4328260421753\n",
      "  time_total_s: 27274.433744192123\n",
      "  timestamp: 1575792528\n",
      "  timesteps_since_restore: 790000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 790000\n",
      "  training_iteration: 395\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27274 s, 395 iter, 790000 ts, 41.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-09-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.81192334950875\n",
      "  episode_reward_mean: 39.6296112981993\n",
      "  episode_reward_min: -15.632865466988306\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7949.381\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.304728627204895\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.168959140777588\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06514399498701096\n",
      "        policy_loss: 0.030252819880843163\n",
      "        total_loss: 199.56214904785156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 199.51202392578125\n",
      "    load_time_ms: 3.09\n",
      "    num_steps_sampled: 792000\n",
      "    num_steps_trained: 792000\n",
      "    sample_time_ms: 58887.883\n",
      "    update_time_ms: 63.118\n",
      "  iterations_since_restore: 396\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86808510638298\n",
      "    ram_util_percent: 74.81808510638301\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.3532169072897\n",
      "    mean_inference_ms: 11.928723614201658\n",
      "    mean_processing_ms: 21.848679404967296\n",
      "  time_since_restore: 27340.638334274292\n",
      "  time_this_iter_s: 66.20459008216858\n",
      "  time_total_s: 27340.638334274292\n",
      "  timestamp: 1575792594\n",
      "  timesteps_since_restore: 792000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 792000\n",
      "  training_iteration: 396\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27340 s, 396 iter, 792000 ts, 39.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-11-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.81192334950875\n",
      "  episode_reward_mean: 40.78286674368185\n",
      "  episode_reward_min: -15.632865466988306\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7950.509\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.45709294080734253\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.709411859512329\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007926597259938717\n",
      "        policy_loss: -0.01357627846300602\n",
      "        total_loss: 226.11312866210938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 226.12306213378906\n",
      "    load_time_ms: 3.079\n",
      "    num_steps_sampled: 794000\n",
      "    num_steps_trained: 794000\n",
      "    sample_time_ms: 58849.37\n",
      "    update_time_ms: 62.908\n",
      "  iterations_since_restore: 397\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.40105263157895\n",
      "    ram_util_percent: 74.89263157894737\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.75820223011036\n",
      "    mean_inference_ms: 11.957163596970204\n",
      "    mean_processing_ms: 21.874339558974025\n",
      "  time_since_restore: 27407.23481965065\n",
      "  time_this_iter_s: 66.59648537635803\n",
      "  time_total_s: 27407.23481965065\n",
      "  timestamp: 1575792661\n",
      "  timesteps_since_restore: 794000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 794000\n",
      "  training_iteration: 397\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27407 s, 397 iter, 794000 ts, 40.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-12-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.0023343495238\n",
      "  episode_reward_mean: 41.75547727142457\n",
      "  episode_reward_min: -13.709633997616907\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8193.584\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.22854647040367126\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3922231197357178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04255376383662224\n",
      "        policy_loss: 0.004531664773821831\n",
      "        total_loss: 196.07568359375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 196.06138610839844\n",
      "    load_time_ms: 3.05\n",
      "    num_steps_sampled: 796000\n",
      "    num_steps_trained: 796000\n",
      "    sample_time_ms: 58670.098\n",
      "    update_time_ms: 63.791\n",
      "  iterations_since_restore: 398\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.81134020618558\n",
      "    ram_util_percent: 74.81340206185567\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.23364021702275\n",
      "    mean_inference_ms: 11.923729258396348\n",
      "    mean_processing_ms: 21.836645934135504\n",
      "  time_since_restore: 27474.930019378662\n",
      "  time_this_iter_s: 67.69519972801208\n",
      "  time_total_s: 27474.930019378662\n",
      "  timestamp: 1575792729\n",
      "  timesteps_since_restore: 796000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 796000\n",
      "  training_iteration: 398\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27474 s, 398 iter, 796000 ts, 41.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-13-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.0023343495238\n",
      "  episode_reward_mean: 41.215350779259445\n",
      "  episode_reward_min: -13.709633997616907\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 7980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8154.788\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3428197205066681\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2345011234283447\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03407100960612297\n",
      "        policy_loss: 0.0023599995765835047\n",
      "        total_loss: 199.27984619140625\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 199.26583862304688\n",
      "    load_time_ms: 4.061\n",
      "    num_steps_sampled: 798000\n",
      "    num_steps_trained: 798000\n",
      "    sample_time_ms: 58440.299\n",
      "    update_time_ms: 64.264\n",
      "  iterations_since_restore: 399\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.76989247311829\n",
      "    ram_util_percent: 74.88172043010752\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.43259684323723\n",
      "    mean_inference_ms: 11.871983400160016\n",
      "    mean_processing_ms: 21.79184536642392\n",
      "  time_since_restore: 27539.991948604584\n",
      "  time_this_iter_s: 65.06192922592163\n",
      "  time_total_s: 27539.991948604584\n",
      "  timestamp: 1575792794\n",
      "  timesteps_since_restore: 798000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 798000\n",
      "  training_iteration: 399\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27539 s, 399 iter, 798000 ts, 41.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-14-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.35894410326841\n",
      "  episode_reward_mean: 42.89043735925864\n",
      "  episode_reward_min: -10.334047763537722\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7906.553\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3428197205066681\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4379043579101562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02115282043814659\n",
      "        policy_loss: -0.006019468419253826\n",
      "        total_loss: 211.78091430664062\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 211.77963256835938\n",
      "    load_time_ms: 3.829\n",
      "    num_steps_sampled: 800000\n",
      "    num_steps_trained: 800000\n",
      "    sample_time_ms: 58672.339\n",
      "    update_time_ms: 56.205\n",
      "  iterations_since_restore: 400\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.30319148936171\n",
      "    ram_util_percent: 74.96914893617021\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.9188617102805\n",
      "    mean_inference_ms: 11.902693019637168\n",
      "    mean_processing_ms: 21.824953240019568\n",
      "  time_since_restore: 27606.348482370377\n",
      "  time_this_iter_s: 66.35653376579285\n",
      "  time_total_s: 27606.348482370377\n",
      "  timestamp: 1575792860\n",
      "  timesteps_since_restore: 800000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 800000\n",
      "  training_iteration: 400\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27606 s, 400 iter, 800000 ts, 42.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-15-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.70701611425812\n",
      "  episode_reward_mean: 43.988863608243975\n",
      "  episode_reward_min: -12.591537936194118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7905.062\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3428197205066681\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.096497058868408\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07378964871168137\n",
      "        policy_loss: 0.013911174610257149\n",
      "        total_loss: 232.46522521972656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 232.4260711669922\n",
      "    load_time_ms: 3.93\n",
      "    num_steps_sampled: 802000\n",
      "    num_steps_trained: 802000\n",
      "    sample_time_ms: 58675.624\n",
      "    update_time_ms: 57.684\n",
      "  iterations_since_restore: 401\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.12421052631578\n",
      "    ram_util_percent: 75.06105263157892\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.79850128728586\n",
      "    mean_inference_ms: 11.895476650509496\n",
      "    mean_processing_ms: 21.835579023986806\n",
      "  time_since_restore: 27672.890874385834\n",
      "  time_this_iter_s: 66.54239201545715\n",
      "  time_total_s: 27672.890874385834\n",
      "  timestamp: 1575792927\n",
      "  timesteps_since_restore: 802000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 802000\n",
      "  training_iteration: 401\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27672 s, 401 iter, 802000 ts, 44 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:16:34,711\tWARNING util.py:145 -- The `process_trial` operation took 0.13249683380126953 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-16-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.70701611425812\n",
      "  episode_reward_mean: 43.61100778330859\n",
      "  episode_reward_min: -12.591537936194118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7948.289\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5142295360565186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9686391353607178\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.019441736862063408\n",
      "        policy_loss: 0.0016572466120123863\n",
      "        total_loss: 214.40493774414062\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 214.393310546875\n",
      "    load_time_ms: 4.616\n",
      "    num_steps_sampled: 804000\n",
      "    num_steps_trained: 804000\n",
      "    sample_time_ms: 58596.695\n",
      "    update_time_ms: 57.632\n",
      "  iterations_since_restore: 402\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.81958762886597\n",
      "    ram_util_percent: 75.09381443298967\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.0019992281017\n",
      "    mean_inference_ms: 11.844185034422113\n",
      "    mean_processing_ms: 21.790204583374948\n",
      "  time_since_restore: 27740.3249065876\n",
      "  time_this_iter_s: 67.43403220176697\n",
      "  time_total_s: 27740.3249065876\n",
      "  timestamp: 1575792994\n",
      "  timesteps_since_restore: 804000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 804000\n",
      "  training_iteration: 402\n",
      "  trial_id: 42ddae0c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:16:34,906\tWARNING util.py:145 -- The `on_step_begin` operation took 0.10283923149108887 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27740 s, 402 iter, 804000 ts, 43.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-17-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.70701611425812\n",
      "  episode_reward_mean: 41.26483607830621\n",
      "  episode_reward_min: -12.591537936194118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8217.666\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5142295360565186\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2025773525238037\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07864521443843842\n",
      "        policy_loss: 0.038644757121801376\n",
      "        total_loss: 201.2407684326172\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 201.1616973876953\n",
      "    load_time_ms: 4.668\n",
      "    num_steps_sampled: 806000\n",
      "    num_steps_trained: 806000\n",
      "    sample_time_ms: 58523.359\n",
      "    update_time_ms: 74.241\n",
      "  iterations_since_restore: 403\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.88247422680412\n",
      "    ram_util_percent: 75.20618556701031\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.0322251617056\n",
      "    mean_inference_ms: 11.846488744377918\n",
      "    mean_processing_ms: 21.78525277179027\n",
      "  time_since_restore: 27808.329281806946\n",
      "  time_this_iter_s: 68.00437521934509\n",
      "  time_total_s: 27808.329281806946\n",
      "  timestamp: 1575793062\n",
      "  timesteps_since_restore: 806000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 806000\n",
      "  training_iteration: 403\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.2/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27808 s, 403 iter, 806000 ts, 41.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-18-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.70701611425812\n",
      "  episode_reward_mean: 44.194745567590935\n",
      "  episode_reward_min: -12.591537936194118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7966.008\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7713443636894226\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.687759280204773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03408899903297424\n",
      "        policy_loss: 0.014975769445300102\n",
      "        total_loss: 208.805908203125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 208.76458740234375\n",
      "    load_time_ms: 4.619\n",
      "    num_steps_sampled: 808000\n",
      "    num_steps_trained: 808000\n",
      "    sample_time_ms: 58522.696\n",
      "    update_time_ms: 75.625\n",
      "  iterations_since_restore: 404\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1436170212766\n",
      "    ram_util_percent: 75.04042553191489\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.0202889242661\n",
      "    mean_inference_ms: 11.845861752544765\n",
      "    mean_processing_ms: 21.781069851006077\n",
      "  time_since_restore: 27874.266339302063\n",
      "  time_this_iter_s: 65.93705749511719\n",
      "  time_total_s: 27874.266339302063\n",
      "  timestamp: 1575793128\n",
      "  timesteps_since_restore: 808000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 808000\n",
      "  training_iteration: 404\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27874 s, 404 iter, 808000 ts, 44.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-19-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.82985746602984\n",
      "  episode_reward_mean: 42.84990071102965\n",
      "  episode_reward_min: -12.591537936194118\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7980.293\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7713443636894226\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.036473035812378\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005334912799298763\n",
      "        policy_loss: -0.00863648485392332\n",
      "        total_loss: 221.5968475341797\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 221.6014404296875\n",
      "    load_time_ms: 4.608\n",
      "    num_steps_sampled: 810000\n",
      "    num_steps_trained: 810000\n",
      "    sample_time_ms: 58415.84\n",
      "    update_time_ms: 77.013\n",
      "  iterations_since_restore: 405\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.18191489361702\n",
      "    ram_util_percent: 75.11489361702125\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.6416155174764\n",
      "    mean_inference_ms: 11.819511656813006\n",
      "    mean_processing_ms: 21.74759446917259\n",
      "  time_since_restore: 27939.788544893265\n",
      "  time_this_iter_s: 65.52220559120178\n",
      "  time_total_s: 27939.788544893265\n",
      "  timestamp: 1575793194\n",
      "  timesteps_since_restore: 810000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 810000\n",
      "  training_iteration: 405\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 27939 s, 405 iter, 810000 ts, 42.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-21-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.82985746602984\n",
      "  episode_reward_mean: 44.44304615913851\n",
      "  episode_reward_min: -9.181340504314713\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7996.925\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3856721818447113\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.978229522705078\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03323660418391228\n",
      "        policy_loss: -0.006922796368598938\n",
      "        total_loss: 207.4080047607422\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 207.40213012695312\n",
      "    load_time_ms: 4.658\n",
      "    num_steps_sampled: 812000\n",
      "    num_steps_trained: 812000\n",
      "    sample_time_ms: 58462.928\n",
      "    update_time_ms: 76.656\n",
      "  iterations_since_restore: 406\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.06842105263158\n",
      "    ram_util_percent: 75.17894736842103\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.29430566167943\n",
      "    mean_inference_ms: 11.860903080885889\n",
      "    mean_processing_ms: 21.762909084000547\n",
      "  time_since_restore: 28006.590493440628\n",
      "  time_this_iter_s: 66.80194854736328\n",
      "  time_total_s: 28006.590493440628\n",
      "  timestamp: 1575793261\n",
      "  timesteps_since_restore: 812000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 812000\n",
      "  training_iteration: 406\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28006 s, 406 iter, 812000 ts, 44.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-22-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.82985746602984\n",
      "  episode_reward_mean: 44.73628410709152\n",
      "  episode_reward_min: -9.181340504314713\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8281.546\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3856721818447113\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9570014476776123\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03735382854938507\n",
      "        policy_loss: 0.007231101859360933\n",
      "        total_loss: 195.3370361328125\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 195.31536865234375\n",
      "    load_time_ms: 4.687\n",
      "    num_steps_sampled: 814000\n",
      "    num_steps_trained: 814000\n",
      "    sample_time_ms: 58532.561\n",
      "    update_time_ms: 77.004\n",
      "  iterations_since_restore: 407\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.012\n",
      "    ram_util_percent: 75.26499999999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.65953617860998\n",
      "    mean_inference_ms: 11.882233522135415\n",
      "    mean_processing_ms: 21.774600750779186\n",
      "  time_since_restore: 28076.733413219452\n",
      "  time_this_iter_s: 70.14291977882385\n",
      "  time_total_s: 28076.733413219452\n",
      "  timestamp: 1575793331\n",
      "  timesteps_since_restore: 814000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 814000\n",
      "  training_iteration: 407\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28076 s, 407 iter, 814000 ts, 44.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-23-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.82985746602984\n",
      "  episode_reward_mean: 48.0780884091412\n",
      "  episode_reward_min: -3.739132097980091\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8036.502\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3856721818447113\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.2605464458465576\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03540903329849243\n",
      "        policy_loss: 0.0024638862814754248\n",
      "        total_loss: 231.703125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 231.68701171875\n",
      "    load_time_ms: 4.743\n",
      "    num_steps_sampled: 816000\n",
      "    num_steps_trained: 816000\n",
      "    sample_time_ms: 58767.686\n",
      "    update_time_ms: 76.937\n",
      "  iterations_since_restore: 408\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.49062499999998\n",
      "    ram_util_percent: 75.36458333333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.2901435889047\n",
      "    mean_inference_ms: 11.919751015288844\n",
      "    mean_processing_ms: 21.807769379606206\n",
      "  time_since_restore: 28144.34791469574\n",
      "  time_this_iter_s: 67.61450147628784\n",
      "  time_total_s: 28144.34791469574\n",
      "  timestamp: 1575793399\n",
      "  timesteps_since_restore: 816000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 816000\n",
      "  training_iteration: 408\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28144 s, 408 iter, 816000 ts, 48.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-24-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.82985746602984\n",
      "  episode_reward_mean: 48.071801183425926\n",
      "  episode_reward_min: -3.9680306138729247\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7902.692\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3856721818447113\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0212016105651855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04512147605419159\n",
      "        policy_loss: 0.02131740190088749\n",
      "        total_loss: 220.21644592285156\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 220.17776489257812\n",
      "    load_time_ms: 3.634\n",
      "    num_steps_sampled: 818000\n",
      "    num_steps_trained: 818000\n",
      "    sample_time_ms: 59100.876\n",
      "    update_time_ms: 76.443\n",
      "  iterations_since_restore: 409\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.609375\n",
      "    ram_util_percent: 75.39270833333335\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.28104501066136\n",
      "    mean_inference_ms: 11.919028161766686\n",
      "    mean_processing_ms: 21.806068500850824\n",
      "  time_since_restore: 28211.389704942703\n",
      "  time_this_iter_s: 67.0417902469635\n",
      "  time_total_s: 28211.389704942703\n",
      "  timestamp: 1575793466\n",
      "  timesteps_since_restore: 818000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 818000\n",
      "  training_iteration: 409\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28211 s, 409 iter, 818000 ts, 48.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:25:35,004\tWARNING util.py:145 -- The `process_trial` operation took 0.15108346939086914 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-25-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.84175214227051\n",
      "  episode_reward_mean: 49.949162050564546\n",
      "  episode_reward_min: -3.9680306138729247\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8059.126\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5785082578659058\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4293594360351562\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0415521003305912\n",
      "        policy_loss: 0.019723720848560333\n",
      "        total_loss: 209.8416748046875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 209.79788208007812\n",
      "    load_time_ms: 3.615\n",
      "    num_steps_sampled: 820000\n",
      "    num_steps_trained: 820000\n",
      "    sample_time_ms: 59161.398\n",
      "    update_time_ms: 75.559\n",
      "  iterations_since_restore: 410\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.32857142857144\n",
      "    ram_util_percent: 75.24693877551022\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.75415927516863\n",
      "    mean_inference_ms: 11.949695088682061\n",
      "    mean_processing_ms: 21.8265511113912\n",
      "  time_since_restore: 28280.101606845856\n",
      "  time_this_iter_s: 68.71190190315247\n",
      "  time_total_s: 28280.101606845856\n",
      "  timestamp: 1575793534\n",
      "  timesteps_since_restore: 820000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 820000\n",
      "  training_iteration: 410\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28280 s, 410 iter, 820000 ts, 49.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.88171800942224\n",
      "  episode_reward_mean: 50.52271752531455\n",
      "  episode_reward_min: -3.9680306138729247\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8343.226\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8677623867988586\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.569941759109497\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02799781784415245\n",
      "        policy_loss: 0.001176362973637879\n",
      "        total_loss: 208.29127502441406\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 208.26585388183594\n",
      "    load_time_ms: 3.947\n",
      "    num_steps_sampled: 822000\n",
      "    num_steps_trained: 822000\n",
      "    sample_time_ms: 59108.319\n",
      "    update_time_ms: 89.435\n",
      "  iterations_since_restore: 411\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.10909090909092\n",
      "    ram_util_percent: 75.36060606060606\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.42091902181147\n",
      "    mean_inference_ms: 11.926229956556172\n",
      "    mean_processing_ms: 21.824182832324478\n",
      "  time_since_restore: 28349.147657632828\n",
      "  time_this_iter_s: 69.04605078697205\n",
      "  time_total_s: 28349.147657632828\n",
      "  timestamp: 1575793604\n",
      "  timesteps_since_restore: 822000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 822000\n",
      "  training_iteration: 411\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28349 s, 411 iter, 822000 ts, 50.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-27-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.41931859050399\n",
      "  episode_reward_mean: 50.98365058738484\n",
      "  episode_reward_min: -3.9680306138729247\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8323.09\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8677623867988586\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8821759223937988\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04775942116975784\n",
      "        policy_loss: 0.021534381434321404\n",
      "        total_loss: 223.29039001464844\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 223.22743225097656\n",
      "    load_time_ms: 3.379\n",
      "    num_steps_sampled: 824000\n",
      "    num_steps_trained: 824000\n",
      "    sample_time_ms: 59014.159\n",
      "    update_time_ms: 88.928\n",
      "  iterations_since_restore: 412\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.97765957446808\n",
      "    ram_util_percent: 75.44042553191487\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.64857408527115\n",
      "    mean_inference_ms: 11.940833967986954\n",
      "    mean_processing_ms: 21.833605391893705\n",
      "  time_since_restore: 28415.345564842224\n",
      "  time_this_iter_s: 66.19790720939636\n",
      "  time_total_s: 28415.345564842224\n",
      "  timestamp: 1575793670\n",
      "  timesteps_since_restore: 824000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 824000\n",
      "  training_iteration: 412\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28415 s, 412 iter, 824000 ts, 51 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-28-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.41931859050399\n",
      "  episode_reward_mean: 51.24683406465012\n",
      "  episode_reward_min: -3.9680306138729247\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8050.841\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.3016436100006104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.886464834213257\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010626915842294693\n",
      "        policy_loss: -0.0011949081672355533\n",
      "        total_loss: 217.34645080566406\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 217.3338165283203\n",
      "    load_time_ms: 3.423\n",
      "    num_steps_sampled: 826000\n",
      "    num_steps_trained: 826000\n",
      "    sample_time_ms: 59174.967\n",
      "    update_time_ms: 71.498\n",
      "  iterations_since_restore: 413\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.059375\n",
      "    ram_util_percent: 75.49791666666665\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.2827276596124\n",
      "    mean_inference_ms: 11.917117591381556\n",
      "    mean_processing_ms: 21.814038283842578\n",
      "  time_since_restore: 28482.041836738586\n",
      "  time_this_iter_s: 66.6962718963623\n",
      "  time_total_s: 28482.041836738586\n",
      "  timestamp: 1575793737\n",
      "  timesteps_since_restore: 826000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 826000\n",
      "  training_iteration: 413\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28482 s, 413 iter, 826000 ts, 51.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-30-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.41931859050399\n",
      "  episode_reward_mean: 51.48491458153636\n",
      "  episode_reward_min: -0.7659178320412188\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8039.531\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.3016436100006104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.280940532684326\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029442831873893738\n",
      "        policy_loss: 0.007517232093960047\n",
      "        total_loss: 210.35940551757812\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 210.31369018554688\n",
      "    load_time_ms: 3.425\n",
      "    num_steps_sampled: 828000\n",
      "    num_steps_trained: 828000\n",
      "    sample_time_ms: 59199.986\n",
      "    update_time_ms: 68.945\n",
      "  iterations_since_restore: 414\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86276595744681\n",
      "    ram_util_percent: 75.57872340425533\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.7059322925498\n",
      "    mean_inference_ms: 11.943004081343439\n",
      "    mean_processing_ms: 21.820122296491263\n",
      "  time_since_restore: 28548.089806079865\n",
      "  time_this_iter_s: 66.04796934127808\n",
      "  time_total_s: 28548.089806079865\n",
      "  timestamp: 1575793803\n",
      "  timesteps_since_restore: 828000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 828000\n",
      "  training_iteration: 414\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28548 s, 414 iter, 828000 ts, 51.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-31-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.21357288100937\n",
      "  episode_reward_mean: 51.191262851069716\n",
      "  episode_reward_min: -3.233496855042886\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8310.918\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.3016436100006104\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5329806804656982\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0064702108502388\n",
      "        policy_loss: -0.0014197005657479167\n",
      "        total_loss: 232.73822021484375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 232.7312774658203\n",
      "    load_time_ms: 3.451\n",
      "    num_steps_sampled: 830000\n",
      "    num_steps_trained: 830000\n",
      "    sample_time_ms: 59267.781\n",
      "    update_time_ms: 67.642\n",
      "  iterations_since_restore: 415\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.72551020408163\n",
      "    ram_util_percent: 75.58061224489798\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.6266059324858\n",
      "    mean_inference_ms: 11.939682322033242\n",
      "    mean_processing_ms: 21.816283621317616\n",
      "  time_since_restore: 28617.002115011215\n",
      "  time_this_iter_s: 68.91230893135071\n",
      "  time_total_s: 28617.002115011215\n",
      "  timestamp: 1575793872\n",
      "  timesteps_since_restore: 830000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 830000\n",
      "  training_iteration: 415\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28617 s, 415 iter, 830000 ts, 51.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-32-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.21357288100937\n",
      "  episode_reward_mean: 50.46589461141676\n",
      "  episode_reward_min: -3.233496855042886\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8312.324\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.6508218050003052\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.0872769355773926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.023450080305337906\n",
      "        policy_loss: -0.034274231642484665\n",
      "        total_loss: 210.64984130859375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.66883850097656\n",
      "    load_time_ms: 3.469\n",
      "    num_steps_sampled: 832000\n",
      "    num_steps_trained: 832000\n",
      "    sample_time_ms: 59204.2\n",
      "    update_time_ms: 68.339\n",
      "  iterations_since_restore: 416\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90105263157893\n",
      "    ram_util_percent: 75.46210526315792\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.2479681461426\n",
      "    mean_inference_ms: 11.91865514909602\n",
      "    mean_processing_ms: 21.780041861345918\n",
      "  time_since_restore: 28683.190323352814\n",
      "  time_this_iter_s: 66.18820834159851\n",
      "  time_total_s: 28683.190323352814\n",
      "  timestamp: 1575793938\n",
      "  timesteps_since_restore: 832000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 832000\n",
      "  training_iteration: 416\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28683 s, 416 iter, 832000 ts, 50.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-33-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.21357288100937\n",
      "  episode_reward_mean: 50.00147970744367\n",
      "  episode_reward_min: -13.65709346398624\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8034.412\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.6508218050003052\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.820777177810669\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.002687441185116768\n",
      "        policy_loss: -0.008625966496765614\n",
      "        total_loss: 242.42462158203125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 242.43153381347656\n",
      "    load_time_ms: 3.454\n",
      "    num_steps_sampled: 834000\n",
      "    num_steps_trained: 834000\n",
      "    sample_time_ms: 59072.468\n",
      "    update_time_ms: 68.918\n",
      "  iterations_since_restore: 417\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.95212765957447\n",
      "    ram_util_percent: 75.50531914893618\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.49266395563828\n",
      "    mean_inference_ms: 11.933317588406162\n",
      "    mean_processing_ms: 21.796153349171306\n",
      "  time_since_restore: 28749.251848697662\n",
      "  time_this_iter_s: 66.06152534484863\n",
      "  time_total_s: 28749.251848697662\n",
      "  timestamp: 1575794004\n",
      "  timesteps_since_restore: 834000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 834000\n",
      "  training_iteration: 417\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28749 s, 417 iter, 834000 ts, 50 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-34-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.21357288100937\n",
      "  episode_reward_mean: 47.81771807047787\n",
      "  episode_reward_min: -13.65709346398624\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8019.724\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3254109025001526\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7671358585357666\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004679624456912279\n",
      "        policy_loss: -0.009952101856470108\n",
      "        total_loss: 227.57069396972656\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 227.57911682128906\n",
      "    load_time_ms: 3.36\n",
      "    num_steps_sampled: 836000\n",
      "    num_steps_trained: 836000\n",
      "    sample_time_ms: 59056.852\n",
      "    update_time_ms: 69.381\n",
      "  iterations_since_restore: 418\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.75104166666665\n",
      "    ram_util_percent: 75.57604166666667\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.84345148461026\n",
      "    mean_inference_ms: 11.95624415219229\n",
      "    mean_processing_ms: 21.80577474886988\n",
      "  time_since_restore: 28816.55894136429\n",
      "  time_this_iter_s: 67.30709266662598\n",
      "  time_total_s: 28816.55894136429\n",
      "  timestamp: 1575794071\n",
      "  timesteps_since_restore: 836000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 836000\n",
      "  training_iteration: 418\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28816 s, 418 iter, 836000 ts, 47.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-35-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.21357288100937\n",
      "  episode_reward_mean: 45.19968758332639\n",
      "  episode_reward_min: -13.65709346398624\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8287.916\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.1627054512500763\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.4154469966888428\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0069913980551064014\n",
      "        policy_loss: -0.011197956278920174\n",
      "        total_loss: 219.30184936523438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 219.31190490722656\n",
      "    load_time_ms: 3.428\n",
      "    num_steps_sampled: 838000\n",
      "    num_steps_trained: 838000\n",
      "    sample_time_ms: 59097.217\n",
      "    update_time_ms: 69.105\n",
      "  iterations_since_restore: 419\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.579\n",
      "    ram_util_percent: 75.73200000000003\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.76505474816997\n",
      "    mean_inference_ms: 11.94950806132745\n",
      "    mean_processing_ms: 21.80641843186746\n",
      "  time_since_restore: 28886.697951078415\n",
      "  time_this_iter_s: 70.13900971412659\n",
      "  time_total_s: 28886.697951078415\n",
      "  timestamp: 1575794142\n",
      "  timesteps_since_restore: 838000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 838000\n",
      "  training_iteration: 419\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28886 s, 419 iter, 838000 ts, 45.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-36-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.39268200971047\n",
      "  episode_reward_mean: 43.06500144438953\n",
      "  episode_reward_min: -13.65709346398624\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8141.639\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8154592514038086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03408030793070793\n",
      "        policy_loss: -0.025154827162623405\n",
      "        total_loss: 210.489013671875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.51138305664062\n",
      "    load_time_ms: 3.513\n",
      "    num_steps_sampled: 840000\n",
      "    num_steps_trained: 840000\n",
      "    sample_time_ms: 59097.693\n",
      "    update_time_ms: 68.685\n",
      "  iterations_since_restore: 420\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.16249999999998\n",
      "    ram_util_percent: 75.82187500000002\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.35556378935874\n",
      "    mean_inference_ms: 11.920480697118114\n",
      "    mean_processing_ms: 21.782130588761905\n",
      "  time_since_restore: 28953.76606488228\n",
      "  time_this_iter_s: 67.06811380386353\n",
      "  time_total_s: 28953.76606488228\n",
      "  timestamp: 1575794209\n",
      "  timesteps_since_restore: 840000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 840000\n",
      "  training_iteration: 420\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 28953 s, 420 iter, 840000 ts, 43.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-37-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.6266655438017\n",
      "  episode_reward_mean: 41.374066534095235\n",
      "  episode_reward_min: -13.65709346398624\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7865.438\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3377020359039307\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013315570540726185\n",
      "        policy_loss: -0.016538646072149277\n",
      "        total_loss: 225.76649475097656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 225.78199768066406\n",
      "    load_time_ms: 3.16\n",
      "    num_steps_sampled: 842000\n",
      "    num_steps_trained: 842000\n",
      "    sample_time_ms: 59330.255\n",
      "    update_time_ms: 53.827\n",
      "  iterations_since_restore: 421\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.93298969072166\n",
      "    ram_util_percent: 75.64226804123712\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.8250412617381\n",
      "    mean_inference_ms: 11.946399092208445\n",
      "    mean_processing_ms: 21.801334772405326\n",
      "  time_since_restore: 29022.15639424324\n",
      "  time_this_iter_s: 68.39032936096191\n",
      "  time_total_s: 29022.15639424324\n",
      "  timestamp: 1575794277\n",
      "  timesteps_since_restore: 842000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 842000\n",
      "  training_iteration: 421\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29022 s, 421 iter, 842000 ts, 41.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:39:04,697\tWARNING util.py:145 -- The `process_trial` operation took 0.14560246467590332 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-39-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.44294979947047\n",
      "  episode_reward_mean: 41.431497092588614\n",
      "  episode_reward_min: -7.56517764513083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7862.989\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.389782667160034\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011886080726981163\n",
      "        policy_loss: -0.012140102684497833\n",
      "        total_loss: 211.68600463867188\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 211.69725036621094\n",
      "    load_time_ms: 3.403\n",
      "    num_steps_sampled: 844000\n",
      "    num_steps_trained: 844000\n",
      "    sample_time_ms: 59381.285\n",
      "    update_time_ms: 54.639\n",
      "  iterations_since_restore: 422\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59270833333335\n",
      "    ram_util_percent: 75.66562499999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.55414646392634\n",
      "    mean_inference_ms: 11.92971877828824\n",
      "    mean_processing_ms: 21.778933900984416\n",
      "  time_since_restore: 29088.97492623329\n",
      "  time_this_iter_s: 66.81853199005127\n",
      "  time_total_s: 29088.97492623329\n",
      "  timestamp: 1575794344\n",
      "  timesteps_since_restore: 844000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 844000\n",
      "  training_iteration: 422\n",
      "  trial_id: 42ddae0c\n",
      "  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:39:04,929\tWARNING util.py:145 -- The `on_step_begin` operation took 0.12060236930847168 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29088 s, 422 iter, 844000 ts, 41.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-40-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.44294979947047\n",
      "  episode_reward_mean: 42.24801551435731\n",
      "  episode_reward_min: -7.56517764513083\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8032.954\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.404134511947632\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01642674021422863\n",
      "        policy_loss: -0.019676562398672104\n",
      "        total_loss: 221.60409545898438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 221.62249755859375\n",
      "    load_time_ms: 4.285\n",
      "    num_steps_sampled: 846000\n",
      "    num_steps_trained: 846000\n",
      "    sample_time_ms: 59490.084\n",
      "    update_time_ms: 69.413\n",
      "  iterations_since_restore: 423\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.80202020202022\n",
      "    ram_util_percent: 75.77171717171717\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.189762332734\n",
      "    mean_inference_ms: 11.90798315683598\n",
      "    mean_processing_ms: 21.766328233960767\n",
      "  time_since_restore: 29158.61394405365\n",
      "  time_this_iter_s: 69.63901782035828\n",
      "  time_total_s: 29158.61394405365\n",
      "  timestamp: 1575794414\n",
      "  timesteps_since_restore: 846000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 846000\n",
      "  training_iteration: 423\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29158 s, 423 iter, 846000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.44294979947047\n",
      "  episode_reward_mean: 44.62645771231638\n",
      "  episode_reward_min: -5.819967289099783\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8020.123\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7284839153289795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010514850728213787\n",
      "        policy_loss: -0.013263258151710033\n",
      "        total_loss: 214.42581176757812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 214.43820190429688\n",
      "    load_time_ms: 4.288\n",
      "    num_steps_sampled: 848000\n",
      "    num_steps_trained: 848000\n",
      "    sample_time_ms: 59462.955\n",
      "    update_time_ms: 70.431\n",
      "  iterations_since_restore: 424\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86063829787234\n",
      "    ram_util_percent: 75.83297872340428\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.18125491269282\n",
      "    mean_inference_ms: 11.907461795225213\n",
      "    mean_processing_ms: 21.762511094523553\n",
      "  time_since_restore: 29224.27920126915\n",
      "  time_this_iter_s: 65.66525721549988\n",
      "  time_total_s: 29224.27920126915\n",
      "  timestamp: 1575794480\n",
      "  timesteps_since_restore: 848000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 848000\n",
      "  training_iteration: 424\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29224 s, 424 iter, 848000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-42-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.44294979947047\n",
      "  episode_reward_mean: 45.67208424544422\n",
      "  episode_reward_min: -5.819967289099783\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7734.894\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6485378742218018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.027737103402614594\n",
      "        policy_loss: -0.023193787783384323\n",
      "        total_loss: 226.47113037109375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 226.49203491210938\n",
      "    load_time_ms: 4.37\n",
      "    num_steps_sampled: 850000\n",
      "    num_steps_trained: 850000\n",
      "    sample_time_ms: 59542.792\n",
      "    update_time_ms: 70.706\n",
      "  iterations_since_restore: 425\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.63368421052633\n",
      "    ram_util_percent: 75.88421052631578\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.46134241239287\n",
      "    mean_inference_ms: 11.927447425168403\n",
      "    mean_processing_ms: 21.764010012088335\n",
      "  time_since_restore: 29291.12759256363\n",
      "  time_this_iter_s: 66.84839129447937\n",
      "  time_total_s: 29291.12759256363\n",
      "  timestamp: 1575794546\n",
      "  timesteps_since_restore: 850000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 850000\n",
      "  training_iteration: 425\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29291 s, 425 iter, 850000 ts, 45.7 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:43:36,234\tWARNING util.py:145 -- The `process_trial` operation took 0.1996300220489502 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-43-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.44294979947047\n",
      "  episode_reward_mean: 46.89840341092116\n",
      "  episode_reward_min: -4.646977633687869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7862.225\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.472339153289795\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.015566040761768818\n",
      "        policy_loss: -0.020016226917505264\n",
      "        total_loss: 231.56468200683594\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 231.58343505859375\n",
      "    load_time_ms: 4.405\n",
      "    num_steps_sampled: 852000\n",
      "    num_steps_trained: 852000\n",
      "    sample_time_ms: 59683.634\n",
      "    update_time_ms: 71.068\n",
      "  iterations_since_restore: 426\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.75959595959597\n",
      "    ram_util_percent: 75.96464646464648\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.02828999635878\n",
      "    mean_inference_ms: 11.900576244011512\n",
      "    mean_processing_ms: 21.75029297908477\n",
      "  time_since_restore: 29360.130524396896\n",
      "  time_this_iter_s: 69.00293183326721\n",
      "  time_total_s: 29360.130524396896\n",
      "  timestamp: 1575794616\n",
      "  timesteps_since_restore: 852000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 852000\n",
      "  training_iteration: 426\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29360 s, 426 iter, 852000 ts, 46.9 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 03:43:36,518\tWARNING util.py:145 -- The `on_step_begin` operation took 0.1888437271118164 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-44-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.41544600537311\n",
      "  episode_reward_mean: 46.88422813881291\n",
      "  episode_reward_min: -4.646977633687869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8123.529\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3620824813842773\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01888294331729412\n",
      "        policy_loss: -0.015348576009273529\n",
      "        total_loss: 230.37127685546875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 230.38507080078125\n",
      "    load_time_ms: 4.361\n",
      "    num_steps_sampled: 854000\n",
      "    num_steps_trained: 854000\n",
      "    sample_time_ms: 59537.888\n",
      "    update_time_ms: 89.835\n",
      "  iterations_since_restore: 427\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.43854166666667\n",
      "    ram_util_percent: 76.10625\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.42835302264206\n",
      "    mean_inference_ms: 11.864163886264992\n",
      "    mean_processing_ms: 21.7206369675012\n",
      "  time_since_restore: 29427.52384352684\n",
      "  time_this_iter_s: 67.39331912994385\n",
      "  time_total_s: 29427.52384352684\n",
      "  timestamp: 1575794683\n",
      "  timesteps_since_restore: 854000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 854000\n",
      "  training_iteration: 427\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29427 s, 427 iter, 854000 ts, 46.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-45-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.41544600537311\n",
      "  episode_reward_mean: 46.0325186599467\n",
      "  episode_reward_min: -4.646977633687869\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8151.819\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.08135272562503815\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7322072982788086\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.19931060075759888\n",
      "        policy_loss: 0.06689003854990005\n",
      "        total_loss: 215.03868103027344\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 214.9556121826172\n",
      "    load_time_ms: 4.381\n",
      "    num_steps_sampled: 856000\n",
      "    num_steps_trained: 856000\n",
      "    sample_time_ms: 59455.804\n",
      "    update_time_ms: 88.741\n",
      "  iterations_since_restore: 428\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.63368421052633\n",
      "    ram_util_percent: 75.91789473684209\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.41674064205955\n",
      "    mean_inference_ms: 11.861039432228717\n",
      "    mean_processing_ms: 21.714445862049832\n",
      "  time_since_restore: 29494.279807329178\n",
      "  time_this_iter_s: 66.75596380233765\n",
      "  time_total_s: 29494.279807329178\n",
      "  timestamp: 1575794750\n",
      "  timesteps_since_restore: 856000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 856000\n",
      "  training_iteration: 428\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29494 s, 428 iter, 856000 ts, 46 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-46-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.41544600537311\n",
      "  episode_reward_mean: 44.066580345575886\n",
      "  episode_reward_min: -14.934712380138638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7897.014\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12202908843755722\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3036768436431885\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011604041792452335\n",
      "        policy_loss: -0.0159470085054636\n",
      "        total_loss: 213.89988708496094\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 213.91444396972656\n",
      "    load_time_ms: 4.333\n",
      "    num_steps_sampled: 858000\n",
      "    num_steps_trained: 858000\n",
      "    sample_time_ms: 59308.916\n",
      "    update_time_ms: 89.659\n",
      "  iterations_since_restore: 429\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.8578947368421\n",
      "    ram_util_percent: 75.97368421052633\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.4771197968572\n",
      "    mean_inference_ms: 11.865131923889868\n",
      "    mean_processing_ms: 21.71300877659417\n",
      "  time_since_restore: 29560.39387488365\n",
      "  time_this_iter_s: 66.11406755447388\n",
      "  time_total_s: 29560.39387488365\n",
      "  timestamp: 1575794816\n",
      "  timesteps_since_restore: 858000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 858000\n",
      "  training_iteration: 429\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29560 s, 429 iter, 858000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.03428877518935\n",
      "  episode_reward_mean: 43.99018234848929\n",
      "  episode_reward_min: -14.934712380138638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7903.304\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12202908843755722\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.6186490058898926\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03891012445092201\n",
      "        policy_loss: 0.00413546385243535\n",
      "        total_loss: 192.02415466308594\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 192.01515197753906\n",
      "    load_time_ms: 4.236\n",
      "    num_steps_sampled: 860000\n",
      "    num_steps_trained: 860000\n",
      "    sample_time_ms: 59321.097\n",
      "    update_time_ms: 90.042\n",
      "  iterations_since_restore: 430\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.24479166666667\n",
      "    ram_util_percent: 76.05520833333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.31890414570339\n",
      "    mean_inference_ms: 11.856150040002813\n",
      "    mean_processing_ms: 21.70817274376355\n",
      "  time_since_restore: 29627.635889053345\n",
      "  time_this_iter_s: 67.242014169693\n",
      "  time_total_s: 29627.635889053345\n",
      "  timestamp: 1575794883\n",
      "  timesteps_since_restore: 860000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 860000\n",
      "  training_iteration: 430\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29627 s, 430 iter, 860000 ts, 44 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-49-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.03428877518935\n",
      "  episode_reward_mean: 43.73375343543274\n",
      "  episode_reward_min: -14.934712380138638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8188.327\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.12202908843755722\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1568803787231445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.11229000240564346\n",
      "        policy_loss: 0.017796184867620468\n",
      "        total_loss: 200.66937255859375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 200.63784790039062\n",
      "    load_time_ms: 4.139\n",
      "    num_steps_sampled: 862000\n",
      "    num_steps_trained: 862000\n",
      "    sample_time_ms: 59142.329\n",
      "    update_time_ms: 89.463\n",
      "  iterations_since_restore: 431\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.12323232323233\n",
      "    ram_util_percent: 76.17777777777778\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.13726657756743\n",
      "    mean_inference_ms: 11.844398156902075\n",
      "    mean_processing_ms: 21.692716537678454\n",
      "  time_since_restore: 29697.086213350296\n",
      "  time_this_iter_s: 69.4503242969513\n",
      "  time_total_s: 29697.086213350296\n",
      "  timestamp: 1575794953\n",
      "  timesteps_since_restore: 862000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 862000\n",
      "  training_iteration: 431\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29697 s, 431 iter, 862000 ts, 43.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-50-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.63846516510716\n",
      "  episode_reward_mean: 44.12876542844953\n",
      "  episode_reward_min: -14.934712380138638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8190.84\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.18304362893104553\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.705251932144165\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.17487816512584686\n",
      "        policy_loss: 0.040458694100379944\n",
      "        total_loss: 221.040283203125\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 220.96775817871094\n",
      "    load_time_ms: 3.85\n",
      "    num_steps_sampled: 864000\n",
      "    num_steps_trained: 864000\n",
      "    sample_time_ms: 59189.109\n",
      "    update_time_ms: 89.102\n",
      "  iterations_since_restore: 432\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.22499999999998\n",
      "    ram_util_percent: 76.25\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.3766920643897\n",
      "    mean_inference_ms: 11.860317239672945\n",
      "    mean_processing_ms: 21.704282537758335\n",
      "  time_since_restore: 29764.216297864914\n",
      "  time_this_iter_s: 67.13008451461792\n",
      "  time_total_s: 29764.216297864914\n",
      "  timestamp: 1575795020\n",
      "  timesteps_since_restore: 864000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 864000\n",
      "  training_iteration: 432\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29764 s, 432 iter, 864000 ts, 44.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-51-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.02070083895954\n",
      "  episode_reward_mean: 44.990171090345164\n",
      "  episode_reward_min: -14.934712380138638\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8021.557\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2745654284954071\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8898805379867554\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.04327892139554024\n",
      "        policy_loss: 0.0010871600825339556\n",
      "        total_loss: 223.21266174316406\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 223.19970703125\n",
      "    load_time_ms: 2.896\n",
      "    num_steps_sampled: 866000\n",
      "    num_steps_trained: 866000\n",
      "    sample_time_ms: 59007.44\n",
      "    update_time_ms: 70.862\n",
      "  iterations_since_restore: 433\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.88404255319149\n",
      "    ram_util_percent: 76.09787234042554\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.62583907778398\n",
      "    mean_inference_ms: 11.875611693279216\n",
      "    mean_processing_ms: 21.699754411424905\n",
      "  time_since_restore: 29830.17465877533\n",
      "  time_this_iter_s: 65.95836091041565\n",
      "  time_total_s: 29830.17465877533\n",
      "  timestamp: 1575795086\n",
      "  timesteps_since_restore: 866000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 866000\n",
      "  training_iteration: 433\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29830 s, 433 iter, 866000 ts, 45 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-52-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.02070083895954\n",
      "  episode_reward_mean: 44.732391251947476\n",
      "  episode_reward_min: -8.052772429832268\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8027.623\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1301462650299072\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03515034168958664\n",
      "        policy_loss: -0.0006417593685910106\n",
      "        total_loss: 219.79354858398438\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 219.77972412109375\n",
      "    load_time_ms: 2.963\n",
      "    num_steps_sampled: 868000\n",
      "    num_steps_trained: 868000\n",
      "    sample_time_ms: 59175.597\n",
      "    update_time_ms: 70.715\n",
      "  iterations_since_restore: 434\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.203125\n",
      "    ram_util_percent: 76.20729166666668\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.70591522689136\n",
      "    mean_inference_ms: 11.879025839243246\n",
      "    mean_processing_ms: 21.697256488045966\n",
      "  time_since_restore: 29897.571293115616\n",
      "  time_this_iter_s: 67.39663434028625\n",
      "  time_total_s: 29897.571293115616\n",
      "  timestamp: 1575795153\n",
      "  timesteps_since_restore: 868000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 868000\n",
      "  training_iteration: 434\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29897 s, 434 iter, 868000 ts, 44.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-53-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.02070083895954\n",
      "  episode_reward_mean: 44.270369645982655\n",
      "  episode_reward_min: -15.640634577062169\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8319.109\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.1590633392333984\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0369085967540741\n",
      "        policy_loss: -0.012015746906399727\n",
      "        total_loss: 232.51840209960938\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 232.5151824951172\n",
      "    load_time_ms: 2.855\n",
      "    num_steps_sampled: 870000\n",
      "    num_steps_trained: 870000\n",
      "    sample_time_ms: 59148.613\n",
      "    update_time_ms: 71.659\n",
      "  iterations_since_restore: 435\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.57070707070707\n",
      "    ram_util_percent: 76.28484848484847\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.00008114920547\n",
      "    mean_inference_ms: 11.897469698038558\n",
      "    mean_processing_ms: 21.722241264204495\n",
      "  time_since_restore: 29967.086474895477\n",
      "  time_this_iter_s: 69.51518177986145\n",
      "  time_total_s: 29967.086474895477\n",
      "  timestamp: 1575795223\n",
      "  timesteps_since_restore: 870000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 870000\n",
      "  training_iteration: 435\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 29967 s, 435 iter, 870000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-54-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.02070083895954\n",
      "  episode_reward_mean: 43.373549674359\n",
      "  episode_reward_min: -16.216870605861075\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8176.636\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0310585498809814\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03383190557360649\n",
      "        policy_loss: 0.0066319676116108894\n",
      "        total_loss: 214.94725036621094\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 214.92666625976562\n",
      "    load_time_ms: 2.772\n",
      "    num_steps_sampled: 872000\n",
      "    num_steps_trained: 872000\n",
      "    sample_time_ms: 59087.246\n",
      "    update_time_ms: 78.696\n",
      "  iterations_since_restore: 436\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.48842105263158\n",
      "    ram_util_percent: 76.40105263157892\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.26604651030001\n",
      "    mean_inference_ms: 11.914746374790552\n",
      "    mean_processing_ms: 21.728500233842105\n",
      "  time_since_restore: 30033.9932076931\n",
      "  time_this_iter_s: 66.90673279762268\n",
      "  time_total_s: 30033.9932076931\n",
      "  timestamp: 1575795290\n",
      "  timesteps_since_restore: 872000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 872000\n",
      "  training_iteration: 436\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30033 s, 436 iter, 872000 ts, 43.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.84725981357023\n",
      "  episode_reward_mean: 41.75185703994749\n",
      "  episode_reward_min: -19.145855314336554\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7911.754\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4842817783355713\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.026403523981571198\n",
      "        policy_loss: -0.003577211871743202\n",
      "        total_loss: 238.30784606933594\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 238.30055236816406\n",
      "    load_time_ms: 2.83\n",
      "    num_steps_sampled: 874000\n",
      "    num_steps_trained: 874000\n",
      "    sample_time_ms: 59561.433\n",
      "    update_time_ms: 60.203\n",
      "  iterations_since_restore: 437\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 96.74848484848484\n",
      "    ram_util_percent: 76.4181818181818\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.02339235156325\n",
      "    mean_inference_ms: 11.898455658339866\n",
      "    mean_processing_ms: 21.712879196341255\n",
      "  time_since_restore: 30103.297795295715\n",
      "  time_this_iter_s: 69.30458760261536\n",
      "  time_total_s: 30103.297795295715\n",
      "  timestamp: 1575795359\n",
      "  timesteps_since_restore: 874000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 874000\n",
      "  training_iteration: 437\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30103 s, 437 iter, 874000 ts, 41.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-57-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.84725981357023\n",
      "  episode_reward_mean: 40.93480498388241\n",
      "  episode_reward_min: -30.54570267734614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8156.17\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7707996368408203\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01741655543446541\n",
      "        policy_loss: -0.0005747160757891834\n",
      "        total_loss: 235.60601806640625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 235.5994415283203\n",
      "    load_time_ms: 2.821\n",
      "    num_steps_sampled: 876000\n",
      "    num_steps_trained: 876000\n",
      "    sample_time_ms: 59621.878\n",
      "    update_time_ms: 60.021\n",
      "  iterations_since_restore: 438\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.492\n",
      "    ram_util_percent: 76.532\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.7701355150864\n",
      "    mean_inference_ms: 11.882716206262266\n",
      "    mean_processing_ms: 21.713173823121387\n",
      "  time_since_restore: 30173.10195684433\n",
      "  time_this_iter_s: 69.8041615486145\n",
      "  time_total_s: 30173.10195684433\n",
      "  timestamp: 1575795429\n",
      "  timesteps_since_restore: 876000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 876000\n",
      "  training_iteration: 438\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30173 s, 438 iter, 876000 ts, 40.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.84725981357023\n",
      "  episode_reward_mean: 42.7739215228395\n",
      "  episode_reward_min: -30.54570267734614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8355.45\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.5085644721984863\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01850038208067417\n",
      "        policy_loss: -0.013399112969636917\n",
      "        total_loss: 222.27670288085938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 222.28250122070312\n",
      "    load_time_ms: 3.719\n",
      "    num_steps_sampled: 878000\n",
      "    num_steps_trained: 878000\n",
      "    sample_time_ms: 59364.69\n",
      "    update_time_ms: 58.287\n",
      "  iterations_since_restore: 439\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 76.4967741935484\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.26525597449174\n",
      "    mean_inference_ms: 11.854241733153083\n",
      "    mean_processing_ms: 21.69788403007422\n",
      "  time_since_restore: 30238.630120515823\n",
      "  time_this_iter_s: 65.52816367149353\n",
      "  time_total_s: 30238.630120515823\n",
      "  timestamp: 1575795495\n",
      "  timesteps_since_restore: 878000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 878000\n",
      "  training_iteration: 439\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30238 s, 439 iter, 878000 ts, 42.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_03-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.84725981357023\n",
      "  episode_reward_mean: 43.501405716265324\n",
      "  episode_reward_min: -30.54570267734614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8331.453\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.397636651992798\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01289545651525259\n",
      "        policy_loss: -0.020923947915434837\n",
      "        total_loss: 234.89044189453125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 234.90602111816406\n",
      "    load_time_ms: 3.732\n",
      "    num_steps_sampled: 880000\n",
      "    num_steps_trained: 880000\n",
      "    sample_time_ms: 59372.529\n",
      "    update_time_ms: 57.993\n",
      "  iterations_since_restore: 440\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.37291666666665\n",
      "    ram_util_percent: 76.44583333333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.00474802208376\n",
      "    mean_inference_ms: 11.834505726811953\n",
      "    mean_processing_ms: 21.67804613031867\n",
      "  time_since_restore: 30305.708303928375\n",
      "  time_this_iter_s: 67.07818341255188\n",
      "  time_total_s: 30305.708303928375\n",
      "  timestamp: 1575795562\n",
      "  timesteps_since_restore: 880000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 880000\n",
      "  training_iteration: 440\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30305 s, 440 iter, 880000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-00-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.84725981357023\n",
      "  episode_reward_mean: 43.19297033371417\n",
      "  episode_reward_min: -30.54570267734614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8028.853\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.41184815764427185\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.294785499572754\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0072338092140853405\n",
      "        policy_loss: -0.014725590124726295\n",
      "        total_loss: 214.7352752685547\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 214.7469940185547\n",
      "    load_time_ms: 3.793\n",
      "    num_steps_sampled: 882000\n",
      "    num_steps_trained: 882000\n",
      "    sample_time_ms: 59339.325\n",
      "    update_time_ms: 57.609\n",
      "  iterations_since_restore: 441\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.86276595744681\n",
      "    ram_util_percent: 76.50425531914897\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.65443889404895\n",
      "    mean_inference_ms: 11.814592970667269\n",
      "    mean_processing_ms: 21.664041004689924\n",
      "  time_since_restore: 30371.7929520607\n",
      "  time_this_iter_s: 66.08464813232422\n",
      "  time_total_s: 30371.7929520607\n",
      "  timestamp: 1575795628\n",
      "  timesteps_since_restore: 882000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 882000\n",
      "  training_iteration: 441\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30371 s, 441 iter, 882000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-01-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.28435781482162\n",
      "  episode_reward_mean: 41.50898343635336\n",
      "  episode_reward_min: -30.54570267734614\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8282.92\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20592407882213593\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0598764419555664\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.010934236459434032\n",
      "        policy_loss: -0.012242304161190987\n",
      "        total_loss: 249.02667236328125\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 249.03656005859375\n",
      "    load_time_ms: 3.831\n",
      "    num_steps_sampled: 884000\n",
      "    num_steps_trained: 884000\n",
      "    sample_time_ms: 59307.368\n",
      "    update_time_ms: 58.003\n",
      "  iterations_since_restore: 442\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1020202020202\n",
      "    ram_util_percent: 76.6010101010101\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.71359348520193\n",
      "    mean_inference_ms: 11.818641102952931\n",
      "    mean_processing_ms: 21.663412034642377\n",
      "  time_since_restore: 30441.153554201126\n",
      "  time_this_iter_s: 69.36060214042664\n",
      "  time_total_s: 30441.153554201126\n",
      "  timestamp: 1575795697\n",
      "  timesteps_since_restore: 884000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 884000\n",
      "  training_iteration: 442\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30441 s, 442 iter, 884000 ts, 41.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-02-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.28435781482162\n",
      "  episode_reward_mean: 43.244506477423045\n",
      "  episode_reward_min: -23.08790098767089\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8420.669\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20592407882213593\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.337733268737793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017801105976104736\n",
      "        policy_loss: -0.020973170176148415\n",
      "        total_loss: 206.86935424804688\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 206.88673400878906\n",
      "    load_time_ms: 6.146\n",
      "    num_steps_sampled: 886000\n",
      "    num_steps_trained: 886000\n",
      "    sample_time_ms: 59233.899\n",
      "    update_time_ms: 58.123\n",
      "  iterations_since_restore: 443\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.278125\n",
      "    ram_util_percent: 76.74479166666669\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.71581269033416\n",
      "    mean_inference_ms: 11.820411945285159\n",
      "    mean_processing_ms: 21.663799238364426\n",
      "  time_since_restore: 30507.774104118347\n",
      "  time_this_iter_s: 66.62054991722107\n",
      "  time_total_s: 30507.774104118347\n",
      "  timestamp: 1575795764\n",
      "  timesteps_since_restore: 886000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 886000\n",
      "  training_iteration: 443\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30507 s, 443 iter, 886000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-03-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.28435781482162\n",
      "  episode_reward_mean: 41.62392874803799\n",
      "  episode_reward_min: -23.08790098767089\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8423.865\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20592407882213593\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2905282974243164\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017065327614545822\n",
      "        policy_loss: -0.012905143201351166\n",
      "        total_loss: 205.9540252685547\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 205.96353149414062\n",
      "    load_time_ms: 6.082\n",
      "    num_steps_sampled: 888000\n",
      "    num_steps_trained: 888000\n",
      "    sample_time_ms: 59195.875\n",
      "    update_time_ms: 57.758\n",
      "  iterations_since_restore: 444\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.57157894736841\n",
      "    ram_util_percent: 75.82315789473685\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.21137003772392\n",
      "    mean_inference_ms: 11.851429388301622\n",
      "    mean_processing_ms: 21.69333226857808\n",
      "  time_since_restore: 30574.829161405563\n",
      "  time_this_iter_s: 67.05505728721619\n",
      "  time_total_s: 30574.829161405563\n",
      "  timestamp: 1575795831\n",
      "  timesteps_since_restore: 888000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 888000\n",
      "  training_iteration: 444\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30574 s, 444 iter, 888000 ts, 41.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-04-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.28435781482162\n",
      "  episode_reward_mean: 42.27884373226957\n",
      "  episode_reward_min: -23.08790098767089\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8126.388\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20592407882213593\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.1031785011291504\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.014794866554439068\n",
      "        policy_loss: -0.016365330666303635\n",
      "        total_loss: 205.691650390625\n",
      "        vf_explained_var: 1.7881393432617188e-07\n",
      "        vf_loss: 205.7049560546875\n",
      "    load_time_ms: 6.11\n",
      "    num_steps_sampled: 890000\n",
      "    num_steps_trained: 890000\n",
      "    sample_time_ms: 59380.285\n",
      "    update_time_ms: 56.43\n",
      "  iterations_since_restore: 445\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.0438775510204\n",
      "    ram_util_percent: 75.62244897959184\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.95976338073459\n",
      "    mean_inference_ms: 11.83789806928396\n",
      "    mean_processing_ms: 21.674508882776166\n",
      "  time_since_restore: 30643.189787626266\n",
      "  time_this_iter_s: 68.36062622070312\n",
      "  time_total_s: 30643.189787626266\n",
      "  timestamp: 1575795899\n",
      "  timesteps_since_restore: 890000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 890000\n",
      "  training_iteration: 445\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30643 s, 445 iter, 890000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-06-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 92.28435781482162\n",
      "  episode_reward_mean: 45.7172262417309\n",
      "  episode_reward_min: -15.550823798660236\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8388.746\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20592407882213593\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8049451112747192\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03567105531692505\n",
      "        policy_loss: -0.008450916036963463\n",
      "        total_loss: 216.54965209960938\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 216.5507354736328\n",
      "    load_time_ms: 6.144\n",
      "    num_steps_sampled: 892000\n",
      "    num_steps_trained: 892000\n",
      "    sample_time_ms: 59306.705\n",
      "    update_time_ms: 47.613\n",
      "  iterations_since_restore: 446\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.81020408163265\n",
      "    ram_util_percent: 75.69795918367349\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.5270172692968\n",
      "    mean_inference_ms: 11.870287482226413\n",
      "    mean_processing_ms: 21.70805435746406\n",
      "  time_since_restore: 30711.895246505737\n",
      "  time_this_iter_s: 68.70545887947083\n",
      "  time_total_s: 30711.895246505737\n",
      "  timestamp: 1575795968\n",
      "  timesteps_since_restore: 892000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 892000\n",
      "  training_iteration: 446\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30711 s, 446 iter, 892000 ts, 45.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-07-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.54686870852959\n",
      "  episode_reward_mean: 46.723647941494754\n",
      "  episode_reward_min: -15.550823798660236\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8442.889\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.20592407882213593\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.6070616245269775\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007557462435215712\n",
      "        policy_loss: -0.013750688172876835\n",
      "        total_loss: 212.9307861328125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 212.94296264648438\n",
      "    load_time_ms: 7.08\n",
      "    num_steps_sampled: 894000\n",
      "    num_steps_trained: 894000\n",
      "    sample_time_ms: 58916.187\n",
      "    update_time_ms: 47.184\n",
      "  iterations_since_restore: 447\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.42021276595744\n",
      "    ram_util_percent: 75.81063829787234\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.8753490182444\n",
      "    mean_inference_ms: 11.889839708532891\n",
      "    mean_processing_ms: 21.717585588456604\n",
      "  time_since_restore: 30777.8498878479\n",
      "  time_this_iter_s: 65.95464134216309\n",
      "  time_total_s: 30777.8498878479\n",
      "  timestamp: 1575796034\n",
      "  timesteps_since_restore: 894000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 894000\n",
      "  training_iteration: 447\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30777 s, 447 iter, 894000 ts, 46.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-08-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.61802214382176\n",
      "  episode_reward_mean: 44.64486275092493\n",
      "  episode_reward_min: -15.550823798660236\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8179.305\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10296203941106796\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2087905406951904\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.1375340223312378\n",
      "        policy_loss: 0.035704225301742554\n",
      "        total_loss: 215.6641845703125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 215.61431884765625\n",
      "    load_time_ms: 7.135\n",
      "    num_steps_sampled: 896000\n",
      "    num_steps_trained: 896000\n",
      "    sample_time_ms: 58952.917\n",
      "    update_time_ms: 47.337\n",
      "  iterations_since_restore: 448\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.19270833333333\n",
      "    ram_util_percent: 75.92812500000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.85706683799015\n",
      "    mean_inference_ms: 11.886953065799089\n",
      "    mean_processing_ms: 21.71163730325704\n",
      "  time_since_restore: 30845.37553024292\n",
      "  time_this_iter_s: 67.52564239501953\n",
      "  time_total_s: 30845.37553024292\n",
      "  timestamp: 1575796102\n",
      "  timesteps_since_restore: 896000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 896000\n",
      "  training_iteration: 448\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30845 s, 448 iter, 896000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-09-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.39385531683664\n",
      "  episode_reward_mean: 45.40838455461543\n",
      "  episode_reward_min: -3.9943097675020405\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 8980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7980.34\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15444305539131165\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7571966648101807\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03546684980392456\n",
      "        policy_loss: -0.0063677760772407055\n",
      "        total_loss: 218.0408477783203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.0417022705078\n",
      "    load_time_ms: 6.324\n",
      "    num_steps_sampled: 898000\n",
      "    num_steps_trained: 898000\n",
      "    sample_time_ms: 59241.4\n",
      "    update_time_ms: 47.776\n",
      "  iterations_since_restore: 449\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.6536842105263\n",
      "    ram_util_percent: 76.02947368421053\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.26286309890673\n",
      "    mean_inference_ms: 11.910795024496476\n",
      "    mean_processing_ms: 21.723767006614356\n",
      "  time_since_restore: 30911.797483205795\n",
      "  time_this_iter_s: 66.42195296287537\n",
      "  time_total_s: 30911.797483205795\n",
      "  timestamp: 1575796168\n",
      "  timesteps_since_restore: 898000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 898000\n",
      "  training_iteration: 449\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30911 s, 449 iter, 898000 ts, 45.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-10-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.39385531683664\n",
      "  episode_reward_mean: 44.5697783764743\n",
      "  episode_reward_min: -9.642551814480418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8248.058\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15444305539131165\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3703720569610596\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0716678649187088\n",
      "        policy_loss: 0.00450136186555028\n",
      "        total_loss: 202.20196533203125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 202.1864013671875\n",
      "    load_time_ms: 6.372\n",
      "    num_steps_sampled: 900000\n",
      "    num_steps_trained: 900000\n",
      "    sample_time_ms: 59239.036\n",
      "    update_time_ms: 47.86\n",
      "  iterations_since_restore: 450\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.101\n",
      "    ram_util_percent: 76.15499999999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.62861025758752\n",
      "    mean_inference_ms: 11.934035164687762\n",
      "    mean_processing_ms: 21.730070934881088\n",
      "  time_since_restore: 30981.535236120224\n",
      "  time_this_iter_s: 69.73775291442871\n",
      "  time_total_s: 30981.535236120224\n",
      "  timestamp: 1575796238\n",
      "  timesteps_since_restore: 900000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 900000\n",
      "  training_iteration: 450\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 30981 s, 450 iter, 900000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-11-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.39385531683664\n",
      "  episode_reward_mean: 42.1904007981431\n",
      "  episode_reward_min: -9.642551814480418\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9020\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8440.212\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.23166459798812866\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.523721694946289\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.045179370790719986\n",
      "        policy_loss: 0.006810508202761412\n",
      "        total_loss: 205.78457641601562\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 205.767333984375\n",
      "    load_time_ms: 7.404\n",
      "    num_steps_sampled: 902000\n",
      "    num_steps_trained: 902000\n",
      "    sample_time_ms: 59025.172\n",
      "    update_time_ms: 48.34\n",
      "  iterations_since_restore: 451\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.75\n",
      "    ram_util_percent: 76.07659574468087\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.39734556565864\n",
      "    mean_inference_ms: 11.920651558464826\n",
      "    mean_processing_ms: 21.703221680995068\n",
      "  time_since_restore: 31047.42546105385\n",
      "  time_this_iter_s: 65.89022493362427\n",
      "  time_total_s: 31047.42546105385\n",
      "  timestamp: 1575796304\n",
      "  timesteps_since_restore: 902000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 902000\n",
      "  training_iteration: 451\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31047 s, 451 iter, 902000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-12-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.39385531683664\n",
      "  episode_reward_mean: 44.3734262170033\n",
      "  episode_reward_min: -16.65530870833946\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9040\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8182.449\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.347496896982193\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7007782459259033\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012267776764929295\n",
      "        policy_loss: -0.018498703837394714\n",
      "        total_loss: 220.58111572265625\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 220.59530639648438\n",
      "    load_time_ms: 7.305\n",
      "    num_steps_sampled: 904000\n",
      "    num_steps_trained: 904000\n",
      "    sample_time_ms: 59043.868\n",
      "    update_time_ms: 48.719\n",
      "  iterations_since_restore: 452\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.42210526315787\n",
      "    ram_util_percent: 76.19052631578948\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.40354925795182\n",
      "    mean_inference_ms: 11.923228646211848\n",
      "    mean_processing_ms: 21.696418017976224\n",
      "  time_since_restore: 31114.393711805344\n",
      "  time_this_iter_s: 66.96825075149536\n",
      "  time_total_s: 31114.393711805344\n",
      "  timestamp: 1575796371\n",
      "  timesteps_since_restore: 904000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 904000\n",
      "  training_iteration: 452\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31114 s, 452 iter, 904000 ts, 44.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-13-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 90.39385531683664\n",
      "  episode_reward_mean: 45.40762482728474\n",
      "  episode_reward_min: -16.65530870833946\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9060\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8061.086\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.347496896982193\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.625410556793213\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024600420147180557\n",
      "        policy_loss: 0.0006218948401510715\n",
      "        total_loss: 219.9214630126953\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 219.9123077392578\n",
      "    load_time_ms: 5.006\n",
      "    num_steps_sampled: 906000\n",
      "    num_steps_trained: 906000\n",
      "    sample_time_ms: 59181.202\n",
      "    update_time_ms: 49.38\n",
      "  iterations_since_restore: 453\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.22105263157894\n",
      "    ram_util_percent: 76.28315789473683\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.7731300735617\n",
      "    mean_inference_ms: 11.948552275897638\n",
      "    mean_processing_ms: 21.70445909223411\n",
      "  time_since_restore: 31181.14852809906\n",
      "  time_this_iter_s: 66.75481629371643\n",
      "  time_total_s: 31181.14852809906\n",
      "  timestamp: 1575796438\n",
      "  timesteps_since_restore: 906000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 906000\n",
      "  training_iteration: 453\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.3/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31181 s, 453 iter, 906000 ts, 45.4 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 04:15:07,129\tWARNING util.py:145 -- The `process_trial` operation took 0.1153554916381836 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-15-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.34350969188348\n",
      "  episode_reward_mean: 44.975279493327754\n",
      "  episode_reward_min: -16.65530870833946\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9080\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8274.497\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.347496896982193\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8323802947998047\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06804783642292023\n",
      "        policy_loss: 0.024645056575536728\n",
      "        total_loss: 206.89007568359375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 206.8417510986328\n",
      "    load_time_ms: 5.072\n",
      "    num_steps_sampled: 908000\n",
      "    num_steps_trained: 908000\n",
      "    sample_time_ms: 59120.108\n",
      "    update_time_ms: 46.043\n",
      "  iterations_since_restore: 454\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90408163265305\n",
      "    ram_util_percent: 76.38979591836735\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.91865779475498\n",
      "    mean_inference_ms: 11.89715290009262\n",
      "    mean_processing_ms: 21.65582527626359\n",
      "  time_since_restore: 31249.834581136703\n",
      "  time_this_iter_s: 68.68605303764343\n",
      "  time_total_s: 31249.834581136703\n",
      "  timestamp: 1575796507\n",
      "  timesteps_since_restore: 908000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 908000\n",
      "  training_iteration: 454\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31249 s, 454 iter, 908000 ts, 45 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 04:15:07,313\tWARNING util.py:145 -- The `on_step_begin` operation took 0.14707732200622559 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-16-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.34350969188348\n",
      "  episode_reward_mean: 44.16206091064825\n",
      "  episode_reward_min: -16.65530870833946\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9100\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8624.712\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5212453603744507\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.325767993927002\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0429525226354599\n",
      "        policy_loss: 0.008992427960038185\n",
      "        total_loss: 209.10305786132812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 209.07167053222656\n",
      "    load_time_ms: 5.197\n",
      "    num_steps_sampled: 910000\n",
      "    num_steps_trained: 910000\n",
      "    sample_time_ms: 58681.618\n",
      "    update_time_ms: 54.661\n",
      "  iterations_since_restore: 455\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.27422680412371\n",
      "    ram_util_percent: 76.51030927835052\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.53474835865836\n",
      "    mean_inference_ms: 11.873268587511312\n",
      "    mean_processing_ms: 21.64314312097651\n",
      "  time_since_restore: 31317.407893180847\n",
      "  time_this_iter_s: 67.57331204414368\n",
      "  time_total_s: 31317.407893180847\n",
      "  timestamp: 1575796574\n",
      "  timesteps_since_restore: 910000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 910000\n",
      "  training_iteration: 455\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31317 s, 455 iter, 910000 ts, 44.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-17-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.34350969188348\n",
      "  episode_reward_mean: 43.24173185819429\n",
      "  episode_reward_min: -28.165076738874806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9120\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8361.655\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.7818679809570312\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.7215189933776855\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.07280491292476654\n",
      "        policy_loss: 0.02559247985482216\n",
      "        total_loss: 226.5930633544922\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 226.51055908203125\n",
      "    load_time_ms: 5.142\n",
      "    num_steps_sampled: 912000\n",
      "    num_steps_trained: 912000\n",
      "    sample_time_ms: 58819.313\n",
      "    update_time_ms: 54.773\n",
      "  iterations_since_restore: 456\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.10729166666665\n",
      "    ram_util_percent: 76.35520833333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.11820079378847\n",
      "    mean_inference_ms: 11.849237818232949\n",
      "    mean_processing_ms: 21.62487993175648\n",
      "  time_since_restore: 31384.873121976852\n",
      "  time_this_iter_s: 67.46522879600525\n",
      "  time_total_s: 31384.873121976852\n",
      "  timestamp: 1575796642\n",
      "  timesteps_since_restore: 912000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 912000\n",
      "  training_iteration: 456\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31384 s, 456 iter, 912000 ts, 43.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-18-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.34350969188348\n",
      "  episode_reward_mean: 43.91574837394268\n",
      "  episode_reward_min: -28.165076738874806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9140\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8301.063\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1728019714355469\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.8417707681655884\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.031209247186779976\n",
      "        policy_loss: 0.01656404696404934\n",
      "        total_loss: 228.7274932861328\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 228.67434692382812\n",
      "    load_time_ms: 4.169\n",
      "    num_steps_sampled: 914000\n",
      "    num_steps_trained: 914000\n",
      "    sample_time_ms: 58942.432\n",
      "    update_time_ms: 55.138\n",
      "  iterations_since_restore: 457\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.03894736842106\n",
      "    ram_util_percent: 76.44842105263159\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.18000119172837\n",
      "    mean_inference_ms: 11.853770642908758\n",
      "    mean_processing_ms: 21.643296785365006\n",
      "  time_since_restore: 31451.442869901657\n",
      "  time_this_iter_s: 66.56974792480469\n",
      "  time_total_s: 31451.442869901657\n",
      "  timestamp: 1575796708\n",
      "  timesteps_since_restore: 914000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 914000\n",
      "  training_iteration: 457\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31451 s, 457 iter, 914000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-19-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.81199249826574\n",
      "  episode_reward_mean: 43.869286693779735\n",
      "  episode_reward_min: -28.165076738874806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9160\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8608.089\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1728019714355469\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.466477155685425\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03657543286681175\n",
      "        policy_loss: 0.016719182953238487\n",
      "        total_loss: 202.81044006347656\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 202.75076293945312\n",
      "    load_time_ms: 4.178\n",
      "    num_steps_sampled: 916000\n",
      "    num_steps_trained: 916000\n",
      "    sample_time_ms: 58876.178\n",
      "    update_time_ms: 56.142\n",
      "  iterations_since_restore: 458\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.42299999999999\n",
      "    ram_util_percent: 76.554\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.62588518622218\n",
      "    mean_inference_ms: 11.816668328476272\n",
      "    mean_processing_ms: 21.616840464587703\n",
      "  time_since_restore: 31521.388700962067\n",
      "  time_this_iter_s: 69.94583106040955\n",
      "  time_total_s: 31521.388700962067\n",
      "  timestamp: 1575796778\n",
      "  timesteps_since_restore: 916000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 916000\n",
      "  training_iteration: 458\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31521 s, 458 iter, 916000 ts, 43.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-20-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.81199249826574\n",
      "  episode_reward_mean: 44.61287247462509\n",
      "  episode_reward_min: -28.165076738874806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9180\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8797.219\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 1.1728019714355469\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.032207727432251\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0057024708949029446\n",
      "        policy_loss: -0.016446182504296303\n",
      "        total_loss: 206.50546264648438\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 206.51531982421875\n",
      "    load_time_ms: 5.559\n",
      "    num_steps_sampled: 918000\n",
      "    num_steps_trained: 918000\n",
      "    sample_time_ms: 58648.645\n",
      "    update_time_ms: 56.675\n",
      "  iterations_since_restore: 459\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 76.67021276595743\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.55784070880063\n",
      "    mean_inference_ms: 11.812191338646022\n",
      "    mean_processing_ms: 21.611220155960783\n",
      "  time_since_restore: 31587.442211151123\n",
      "  time_this_iter_s: 66.0535101890564\n",
      "  time_total_s: 31587.442211151123\n",
      "  timestamp: 1575796844\n",
      "  timesteps_since_restore: 918000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 918000\n",
      "  training_iteration: 459\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31587 s, 459 iter, 918000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-21-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 82.81199249826574\n",
      "  episode_reward_mean: 44.631860615208716\n",
      "  episode_reward_min: -28.165076738874806\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9200\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8544.233\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5864009857177734\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.326747179031372\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03690443933010101\n",
      "        policy_loss: 0.014382396824657917\n",
      "        total_loss: 201.11019897460938\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 201.07415771484375\n",
      "    load_time_ms: 5.59\n",
      "    num_steps_sampled: 920000\n",
      "    num_steps_trained: 920000\n",
      "    sample_time_ms: 58727.294\n",
      "    update_time_ms: 56.388\n",
      "  iterations_since_restore: 460\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.40309278350516\n",
      "    ram_util_percent: 76.76082474226804\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.557855344188\n",
      "    mean_inference_ms: 11.872205048400634\n",
      "    mean_processing_ms: 21.655647237947733\n",
      "  time_since_restore: 31655.4326877594\n",
      "  time_this_iter_s: 67.99047660827637\n",
      "  time_total_s: 31655.4326877594\n",
      "  timestamp: 1575796912\n",
      "  timesteps_since_restore: 920000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 920000\n",
      "  training_iteration: 460\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31655 s, 460 iter, 920000 ts, 44.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-23-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 79.5776114086189\n",
      "  episode_reward_mean: 45.28683812225458\n",
      "  episode_reward_min: -19.888865710927917\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9220\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8355.988\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5864009857177734\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4769132137298584\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05262330546975136\n",
      "        policy_loss: 0.022136341780424118\n",
      "        total_loss: 202.17300415039062\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 202.11996459960938\n",
      "    load_time_ms: 4.616\n",
      "    num_steps_sampled: 922000\n",
      "    num_steps_trained: 922000\n",
      "    sample_time_ms: 59195.516\n",
      "    update_time_ms: 56.625\n",
      "  iterations_since_restore: 461\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.59285714285714\n",
      "    ram_util_percent: 76.86326530612244\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.68585050443434\n",
      "    mean_inference_ms: 11.877077613425884\n",
      "    mean_processing_ms: 21.659867019727244\n",
      "  time_since_restore: 31724.109832048416\n",
      "  time_this_iter_s: 68.67714428901672\n",
      "  time_total_s: 31724.109832048416\n",
      "  timestamp: 1575796981\n",
      "  timesteps_since_restore: 922000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 922000\n",
      "  training_iteration: 461\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31724 s, 461 iter, 922000 ts, 45.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-24-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 78.79873095085325\n",
      "  episode_reward_mean: 44.6815542376153\n",
      "  episode_reward_min: -6.983794718632071\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9240\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8649.169\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.8796014785766602\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.926939010620117\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004399893805384636\n",
      "        policy_loss: -0.013043983839452267\n",
      "        total_loss: 207.01715087890625\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 207.0263214111328\n",
      "    load_time_ms: 4.639\n",
      "    num_steps_sampled: 924000\n",
      "    num_steps_trained: 924000\n",
      "    sample_time_ms: 59172.625\n",
      "    update_time_ms: 57.292\n",
      "  iterations_since_restore: 462\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.21900000000001\n",
      "    ram_util_percent: 76.82499999999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.86747860757868\n",
      "    mean_inference_ms: 11.887176227855866\n",
      "    mean_processing_ms: 21.65438045458833\n",
      "  time_since_restore: 31793.78609418869\n",
      "  time_this_iter_s: 69.67626214027405\n",
      "  time_total_s: 31793.78609418869\n",
      "  timestamp: 1575797051\n",
      "  timesteps_since_restore: 924000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 924000\n",
      "  training_iteration: 462\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31793 s, 462 iter, 924000 ts, 44.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-25-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 78.79873095085325\n",
      "  episode_reward_mean: 44.992735679800944\n",
      "  episode_reward_min: 1.4279401398518563\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9260\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8775.213\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4398007392883301\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.925990343093872\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01680295541882515\n",
      "        policy_loss: -0.026253869757056236\n",
      "        total_loss: 206.79327392578125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 206.81219482421875\n",
      "    load_time_ms: 4.616\n",
      "    num_steps_sampled: 926000\n",
      "    num_steps_trained: 926000\n",
      "    sample_time_ms: 59226.993\n",
      "    update_time_ms: 57.571\n",
      "  iterations_since_restore: 463\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.80927835051547\n",
      "    ram_util_percent: 76.78762886597937\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.7234206159887\n",
      "    mean_inference_ms: 11.880875610362013\n",
      "    mean_processing_ms: 21.64644498468617\n",
      "  time_since_restore: 31862.344744205475\n",
      "  time_this_iter_s: 68.55865001678467\n",
      "  time_total_s: 31862.344744205475\n",
      "  timestamp: 1575797119\n",
      "  timesteps_since_restore: 926000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 926000\n",
      "  training_iteration: 463\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31862 s, 463 iter, 926000 ts, 45 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-26-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84340331696554\n",
      "  episode_reward_mean: 43.03484125931629\n",
      "  episode_reward_min: -27.24766864512253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9280\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8576.15\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4398007392883301\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.5034379959106445\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01571487821638584\n",
      "        policy_loss: -0.00797349400818348\n",
      "        total_loss: 242.5916290283203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 242.59259033203125\n",
      "    load_time_ms: 4.564\n",
      "    num_steps_sampled: 928000\n",
      "    num_steps_trained: 928000\n",
      "    sample_time_ms: 59280.156\n",
      "    update_time_ms: 60.214\n",
      "  iterations_since_restore: 464\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56770833333333\n",
      "    ram_util_percent: 76.83854166666666\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.21982771159134\n",
      "    mean_inference_ms: 11.908440402227843\n",
      "    mean_processing_ms: 21.65871274895986\n",
      "  time_since_restore: 31929.439757585526\n",
      "  time_this_iter_s: 67.09501338005066\n",
      "  time_total_s: 31929.439757585526\n",
      "  timestamp: 1575797187\n",
      "  timesteps_since_restore: 928000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 928000\n",
      "  training_iteration: 464\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31929 s, 464 iter, 928000 ts, 43 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-27-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84340331696554\n",
      "  episode_reward_mean: 42.328001138283454\n",
      "  episode_reward_min: -27.24766864512253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9300\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8234.116\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4398007392883301\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.426619291305542\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013774304650723934\n",
      "        policy_loss: 0.004757919814437628\n",
      "        total_loss: 236.4959259033203\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 236.48513793945312\n",
      "    load_time_ms: 4.419\n",
      "    num_steps_sampled: 930000\n",
      "    num_steps_trained: 930000\n",
      "    sample_time_ms: 59557.804\n",
      "    update_time_ms: 52.579\n",
      "  iterations_since_restore: 465\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.19374999999998\n",
      "    ram_util_percent: 76.95104166666665\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.00979495602826\n",
      "    mean_inference_ms: 11.895955770842079\n",
      "    mean_processing_ms: 21.654462438286565\n",
      "  time_since_restore: 31996.28608727455\n",
      "  time_this_iter_s: 66.84632968902588\n",
      "  time_total_s: 31996.28608727455\n",
      "  timestamp: 1575797253\n",
      "  timesteps_since_restore: 930000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 930000\n",
      "  training_iteration: 465\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 31996 s, 465 iter, 930000 ts, 42.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-28-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84340331696554\n",
      "  episode_reward_mean: 42.21023627548609\n",
      "  episode_reward_min: -27.24766864512253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9320\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8506.415\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.4398007392883301\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.5569560527801514\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.005728273186832666\n",
      "        policy_loss: -0.009807225316762924\n",
      "        total_loss: 230.10198974609375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 230.10923767089844\n",
      "    load_time_ms: 4.422\n",
      "    num_steps_sampled: 932000\n",
      "    num_steps_trained: 932000\n",
      "    sample_time_ms: 59525.23\n",
      "    update_time_ms: 53.808\n",
      "  iterations_since_restore: 466\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.96060606060607\n",
      "    ram_util_percent: 77.08080808080807\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.65279507605777\n",
      "    mean_inference_ms: 11.940818610614878\n",
      "    mean_processing_ms: 21.688071905093388\n",
      "  time_since_restore: 32066.147242307663\n",
      "  time_this_iter_s: 69.86115503311157\n",
      "  time_total_s: 32066.147242307663\n",
      "  timestamp: 1575797323\n",
      "  timesteps_since_restore: 932000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 932000\n",
      "  training_iteration: 466\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32066 s, 466 iter, 932000 ts, 42.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-29-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84340331696554\n",
      "  episode_reward_mean: 41.698699375746635\n",
      "  episode_reward_min: -27.24766864512253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9340\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8508.409\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21990036964416504\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4167802333831787\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.024557219818234444\n",
      "        policy_loss: -0.02172423154115677\n",
      "        total_loss: 210.91546630859375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 210.93174743652344\n",
      "    load_time_ms: 4.454\n",
      "    num_steps_sampled: 934000\n",
      "    num_steps_trained: 934000\n",
      "    sample_time_ms: 59539.729\n",
      "    update_time_ms: 53.452\n",
      "  iterations_since_restore: 467\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.64526315789473\n",
      "    ram_util_percent: 77.14736842105262\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.8107045400659\n",
      "    mean_inference_ms: 11.949777218644618\n",
      "    mean_processing_ms: 21.701720280078025\n",
      "  time_since_restore: 32132.870929956436\n",
      "  time_this_iter_s: 66.7236876487732\n",
      "  time_total_s: 32132.870929956436\n",
      "  timestamp: 1575797390\n",
      "  timesteps_since_restore: 934000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 934000\n",
      "  training_iteration: 467\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32132 s, 467 iter, 934000 ts, 41.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-30-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 83.84340331696554\n",
      "  episode_reward_mean: 39.058538136480145\n",
      "  episode_reward_min: -27.24766864512253\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9360\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8214.935\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21990036964416504\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.3457462787628174\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02920251339673996\n",
      "        policy_loss: 0.004462318494915962\n",
      "        total_loss: 229.92819213867188\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 229.91729736328125\n",
      "    load_time_ms: 4.62\n",
      "    num_steps_sampled: 936000\n",
      "    num_steps_trained: 936000\n",
      "    sample_time_ms: 59574.8\n",
      "    update_time_ms: 52.299\n",
      "  iterations_since_restore: 468\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.50721649484537\n",
      "    ram_util_percent: 77.05154639175258\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.80620679013578\n",
      "    mean_inference_ms: 11.94969731507943\n",
      "    mean_processing_ms: 21.698861813104486\n",
      "  time_since_restore: 32200.224004030228\n",
      "  time_this_iter_s: 67.3530740737915\n",
      "  time_total_s: 32200.224004030228\n",
      "  timestamp: 1575797457\n",
      "  timesteps_since_restore: 936000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 936000\n",
      "  training_iteration: 468\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32200 s, 468 iter, 936000 ts, 39.1 rew\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-12-08 04:32:06,424\tWARNING util.py:145 -- The `process_trial` operation took 0.1490342617034912 seconds to complete, which may be a performance bottleneck.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-32-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.27364688211736\n",
      "  episode_reward_mean: 38.30264872393976\n",
      "  episode_reward_min: -18.21821840609351\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9380\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8163.833\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21990036964416504\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.8802742958068848\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.033574171364307404\n",
      "        policy_loss: 0.005935338791459799\n",
      "        total_loss: 213.27194213867188\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 213.25856018066406\n",
      "    load_time_ms: 3.142\n",
      "    num_steps_sampled: 938000\n",
      "    num_steps_trained: 938000\n",
      "    sample_time_ms: 59831.423\n",
      "    update_time_ms: 51.582\n",
      "  iterations_since_restore: 469\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.15257731958762\n",
      "    ram_util_percent: 77.10824742268042\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.89822146256574\n",
      "    mean_inference_ms: 11.958002691809734\n",
      "    mean_processing_ms: 21.709377629444607\n",
      "  time_since_restore: 32268.457513332367\n",
      "  time_this_iter_s: 68.23350930213928\n",
      "  time_total_s: 32268.457513332367\n",
      "  timestamp: 1575797526\n",
      "  timesteps_since_restore: 938000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 938000\n",
      "  training_iteration: 469\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32268 s, 469 iter, 938000 ts, 38.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-33-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 81.27364688211736\n",
      "  episode_reward_mean: 40.10958117641478\n",
      "  episode_reward_min: -18.21821840609351\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9400\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8437.077\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.21990036964416504\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.538940906524658\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.16547098755836487\n",
      "        policy_loss: 0.032810892909765244\n",
      "        total_loss: 210.5941925048828\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.52493286132812\n",
      "    load_time_ms: 4.054\n",
      "    num_steps_sampled: 940000\n",
      "    num_steps_trained: 940000\n",
      "    sample_time_ms: 59523.064\n",
      "    update_time_ms: 68.782\n",
      "  iterations_since_restore: 470\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.90412371134022\n",
      "    ram_util_percent: 77.24226804123714\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.45912432682718\n",
      "    mean_inference_ms: 11.930517731735572\n",
      "    mean_processing_ms: 21.675088447574243\n",
      "  time_since_restore: 32336.285028219223\n",
      "  time_this_iter_s: 67.82751488685608\n",
      "  time_total_s: 32336.285028219223\n",
      "  timestamp: 1575797594\n",
      "  timesteps_since_restore: 940000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 940000\n",
      "  training_iteration: 470\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32336 s, 470 iter, 940000 ts, 40.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-34-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.48546842227681\n",
      "  episode_reward_mean: 41.664874478495186\n",
      "  episode_reward_min: -18.21821840609351\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9420\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8447.32\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.32985055446624756\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 1.722135066986084\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.14894776046276093\n",
      "        policy_loss: 0.013969930820167065\n",
      "        total_loss: 198.88311767578125\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 198.8199005126953\n",
      "    load_time_ms: 4.046\n",
      "    num_steps_sampled: 942000\n",
      "    num_steps_trained: 942000\n",
      "    sample_time_ms: 59254.046\n",
      "    update_time_ms: 69.287\n",
      "  iterations_since_restore: 471\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.9276595744681\n",
      "    ram_util_percent: 77.32340425531916\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.72785712399903\n",
      "    mean_inference_ms: 11.943567295106277\n",
      "    mean_processing_ms: 21.67775936175732\n",
      "  time_since_restore: 32402.385351657867\n",
      "  time_this_iter_s: 66.10032343864441\n",
      "  time_total_s: 32402.385351657867\n",
      "  timestamp: 1575797660\n",
      "  timesteps_since_restore: 942000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 942000\n",
      "  training_iteration: 471\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32402 s, 471 iter, 942000 ts, 41.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-35-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.48546842227681\n",
      "  episode_reward_mean: 42.89957145134511\n",
      "  episode_reward_min: -18.21821840609351\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9440\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8139.754\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.49477583169937134\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.920457601547241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.01042907778173685\n",
      "        policy_loss: -0.012786424718797207\n",
      "        total_loss: 194.6710662841797\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 194.67868041992188\n",
      "    load_time_ms: 4.039\n",
      "    num_steps_sampled: 944000\n",
      "    num_steps_trained: 944000\n",
      "    sample_time_ms: 59343.503\n",
      "    update_time_ms: 67.905\n",
      "  iterations_since_restore: 472\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.28556701030928\n",
      "    ram_util_percent: 77.42680412371135\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.60784299541348\n",
      "    mean_inference_ms: 11.93555168482053\n",
      "    mean_processing_ms: 21.656856825550623\n",
      "  time_since_restore: 32469.874021291733\n",
      "  time_this_iter_s: 67.48866963386536\n",
      "  time_total_s: 32469.874021291733\n",
      "  timestamp: 1575797728\n",
      "  timesteps_since_restore: 944000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 944000\n",
      "  training_iteration: 472\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32469 s, 472 iter, 944000 ts, 42.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-36-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.48546842227681\n",
      "  episode_reward_mean: 44.745850297116014\n",
      "  episode_reward_min: -15.40923970694289\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9460\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8376.922\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.49477583169937134\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.334512948989868\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.004101714119315147\n",
      "        policy_loss: -0.010091583244502544\n",
      "        total_loss: 206.274169921875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 206.28219604492188\n",
      "    load_time_ms: 4.3\n",
      "    num_steps_sampled: 946000\n",
      "    num_steps_trained: 946000\n",
      "    sample_time_ms: 59540.621\n",
      "    update_time_ms: 67.477\n",
      "  iterations_since_restore: 473\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.91634615384615\n",
      "    ram_util_percent: 77.51346153846154\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 143.02366287700295\n",
      "    mean_inference_ms: 11.959851954127009\n",
      "    mean_processing_ms: 21.669500544604485\n",
      "  time_since_restore: 32542.776767015457\n",
      "  time_this_iter_s: 72.90274572372437\n",
      "  time_total_s: 32542.776767015457\n",
      "  timestamp: 1575797800\n",
      "  timesteps_since_restore: 946000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 946000\n",
      "  training_iteration: 473\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32542 s, 473 iter, 946000 ts, 44.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-37-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.48546842227681\n",
      "  episode_reward_mean: 47.34031764304812\n",
      "  episode_reward_min: -15.40923970694289\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9480\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8382.377\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.24738791584968567\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6383450031280518\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.05432140454649925\n",
      "        policy_loss: 0.0008470711763948202\n",
      "        total_loss: 210.52084350585938\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 210.50660705566406\n",
      "    load_time_ms: 4.31\n",
      "    num_steps_sampled: 948000\n",
      "    num_steps_trained: 948000\n",
      "    sample_time_ms: 59787.373\n",
      "    update_time_ms: 68.491\n",
      "  iterations_since_restore: 474\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.06969696969696\n",
      "    ram_util_percent: 77.39999999999999\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.49817612790753\n",
      "    mean_inference_ms: 11.928250607182749\n",
      "    mean_processing_ms: 21.64219497278711\n",
      "  time_since_restore: 32612.405224084854\n",
      "  time_this_iter_s: 69.62845706939697\n",
      "  time_total_s: 32612.405224084854\n",
      "  timestamp: 1575797870\n",
      "  timesteps_since_restore: 948000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 948000\n",
      "  training_iteration: 474\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.4/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32612 s, 474 iter, 948000 ts, 47.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-38-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.48546842227681\n",
      "  episode_reward_mean: 48.368891703396436\n",
      "  episode_reward_min: -6.503539645743669\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9500\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8386.008\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3710818886756897\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.88348126411438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.012856714427471161\n",
      "        policy_loss: -0.005345541052520275\n",
      "        total_loss: 211.9244384765625\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 211.9250030517578\n",
      "    load_time_ms: 4.306\n",
      "    num_steps_sampled: 950000\n",
      "    num_steps_trained: 950000\n",
      "    sample_time_ms: 59853.329\n",
      "    update_time_ms: 68.142\n",
      "  iterations_since_restore: 475\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.97291666666668\n",
      "    ram_util_percent: 77.475\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.38582340292945\n",
      "    mean_inference_ms: 11.921076785665878\n",
      "    mean_processing_ms: 21.641580620135773\n",
      "  time_since_restore: 32679.937852859497\n",
      "  time_this_iter_s: 67.53262877464294\n",
      "  time_total_s: 32679.937852859497\n",
      "  timestamp: 1575797938\n",
      "  timesteps_since_restore: 950000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 950000\n",
      "  training_iteration: 475\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32679 s, 475 iter, 950000 ts, 48.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-40-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.87476541188448\n",
      "  episode_reward_mean: 47.0861809819777\n",
      "  episode_reward_min: -11.91133698276239\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9520\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8387.959\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3710818886756897\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.2091140747070312\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.017546022310853004\n",
      "        policy_loss: -0.02156088314950466\n",
      "        total_loss: 235.88845825195312\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 235.9034881591797\n",
      "    load_time_ms: 4.371\n",
      "    num_steps_sampled: 952000\n",
      "    num_steps_trained: 952000\n",
      "    sample_time_ms: 59834.601\n",
      "    update_time_ms: 66.437\n",
      "  iterations_since_restore: 476\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.48299999999999\n",
      "    ram_util_percent: 77.567\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.9145134696876\n",
      "    mean_inference_ms: 11.892620075902858\n",
      "    mean_processing_ms: 21.617596775440106\n",
      "  time_since_restore: 32749.709181785583\n",
      "  time_this_iter_s: 69.77132892608643\n",
      "  time_total_s: 32749.709181785583\n",
      "  timestamp: 1575798007\n",
      "  timesteps_since_restore: 952000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 952000\n",
      "  training_iteration: 476\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32749 s, 476 iter, 952000 ts, 47.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-41-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 84.87476541188448\n",
      "  episode_reward_mean: 46.443975472929196\n",
      "  episode_reward_min: -11.91133698276239\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9540\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8679.465\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.3710818886756897\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6824324131011963\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0642012357711792\n",
      "        policy_loss: 0.021215740591287613\n",
      "        total_loss: 212.7781524658203\n",
      "        vf_explained_var: 1.1920928955078125e-07\n",
      "        vf_loss: 212.73306274414062\n",
      "    load_time_ms: 6.414\n",
      "    num_steps_sampled: 954000\n",
      "    num_steps_trained: 954000\n",
      "    sample_time_ms: 59532.117\n",
      "    update_time_ms: 72.841\n",
      "  iterations_since_restore: 477\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.97263157894737\n",
      "    ram_util_percent: 77.6957894736842\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.41804773507857\n",
      "    mean_inference_ms: 11.864489349556946\n",
      "    mean_processing_ms: 21.601382502737923\n",
      "  time_since_restore: 32816.409543037415\n",
      "  time_this_iter_s: 66.70036125183105\n",
      "  time_total_s: 32816.409543037415\n",
      "  timestamp: 1575798074\n",
      "  timesteps_since_restore: 954000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 954000\n",
      "  training_iteration: 477\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32816 s, 477 iter, 954000 ts, 46.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-42-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.70348199051205\n",
      "  episode_reward_mean: 47.51759906925761\n",
      "  episode_reward_min: -11.91133698276239\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9560\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8665.719\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5566228032112122\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9761805534362793\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.02167595736682415\n",
      "        policy_loss: -0.001211562193930149\n",
      "        total_loss: 216.66339111328125\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 216.65248107910156\n",
      "    load_time_ms: 6.301\n",
      "    num_steps_sampled: 956000\n",
      "    num_steps_trained: 956000\n",
      "    sample_time_ms: 59611.8\n",
      "    update_time_ms: 74.184\n",
      "  iterations_since_restore: 478\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.58041237113402\n",
      "    ram_util_percent: 77.80721649484536\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.68628438131077\n",
      "    mean_inference_ms: 11.882628975392386\n",
      "    mean_processing_ms: 21.612652552644068\n",
      "  time_since_restore: 32884.45096230507\n",
      "  time_this_iter_s: 68.04141926765442\n",
      "  time_total_s: 32884.45096230507\n",
      "  timestamp: 1575798142\n",
      "  timesteps_since_restore: 956000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 956000\n",
      "  training_iteration: 478\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32884 s, 478 iter, 956000 ts, 47.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.70348199051205\n",
      "  episode_reward_mean: 47.486643897698585\n",
      "  episode_reward_min: -11.91133698276239\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9580\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8519.266\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5566228032112122\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.105516195297241\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03460073843598366\n",
      "        policy_loss: 0.016197476536035538\n",
      "        total_loss: 209.28050231933594\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 209.24505615234375\n",
      "    load_time_ms: 6.299\n",
      "    num_steps_sampled: 958000\n",
      "    num_steps_trained: 958000\n",
      "    sample_time_ms: 59590.195\n",
      "    update_time_ms: 74.052\n",
      "  iterations_since_restore: 479\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.8\n",
      "    ram_util_percent: 77.80736842105262\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.61829457912296\n",
      "    mean_inference_ms: 11.878019303879757\n",
      "    mean_processing_ms: 21.60807128027108\n",
      "  time_since_restore: 32950.87185168266\n",
      "  time_this_iter_s: 66.420889377594\n",
      "  time_total_s: 32950.87185168266\n",
      "  timestamp: 1575798209\n",
      "  timesteps_since_restore: 958000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 958000\n",
      "  training_iteration: 479\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 32950 s, 479 iter, 958000 ts, 47.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-44-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.70348199051205\n",
      "  episode_reward_mean: 46.55894969178634\n",
      "  episode_reward_min: -11.91133698276239\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9600\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8586.073\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5566228032112122\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.934030532836914\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.03961186110973358\n",
      "        policy_loss: 0.00285943946801126\n",
      "        total_loss: 216.49681091308594\n",
      "        vf_explained_var: 2.384185791015625e-07\n",
      "        vf_loss: 216.4719696044922\n",
      "    load_time_ms: 5.282\n",
      "    num_steps_sampled: 960000\n",
      "    num_steps_trained: 960000\n",
      "    sample_time_ms: 59776.64\n",
      "    update_time_ms: 57.774\n",
      "  iterations_since_restore: 480\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56200000000001\n",
      "    ram_util_percent: 77.71900000000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.53772427010935\n",
      "    mean_inference_ms: 11.875806160287912\n",
      "    mean_processing_ms: 21.60177501530705\n",
      "  time_since_restore: 33021.06006979942\n",
      "  time_this_iter_s: 70.18821811676025\n",
      "  time_total_s: 33021.06006979942\n",
      "  timestamp: 1575798279\n",
      "  timesteps_since_restore: 960000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 960000\n",
      "  training_iteration: 480\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33021 s, 480 iter, 960000 ts, 46.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-45-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.92093934509465\n",
      "  episode_reward_mean: 46.43121648229901\n",
      "  episode_reward_min: -13.267613716338525\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9620\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8574.491\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5566228032112122\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.877192497253418\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.028156574815511703\n",
      "        policy_loss: 0.007678533438593149\n",
      "        total_loss: 224.92837524414062\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 224.9050750732422\n",
      "    load_time_ms: 5.687\n",
      "    num_steps_sampled: 962000\n",
      "    num_steps_trained: 962000\n",
      "    sample_time_ms: 59839.276\n",
      "    update_time_ms: 57.392\n",
      "  iterations_since_restore: 481\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 99.22526315789473\n",
      "    ram_util_percent: 77.86210526315789\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.95534553816196\n",
      "    mean_inference_ms: 11.84066089841825\n",
      "    mean_processing_ms: 21.573224664861154\n",
      "  time_since_restore: 33087.66318774223\n",
      "  time_this_iter_s: 66.60311794281006\n",
      "  time_total_s: 33087.66318774223\n",
      "  timestamp: 1575798346\n",
      "  timesteps_since_restore: 962000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 962000\n",
      "  training_iteration: 481\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33087 s, 481 iter, 962000 ts, 46.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-46-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.92093934509465\n",
      "  episode_reward_mean: 45.21094498594231\n",
      "  episode_reward_min: -13.267613716338525\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9640\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8571.487\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.5566228032112122\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4991226196289062\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.008599176071584225\n",
      "        policy_loss: -0.0009321918478235602\n",
      "        total_loss: 221.57708740234375\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 221.57322692871094\n",
      "    load_time_ms: 5.666\n",
      "    num_steps_sampled: 964000\n",
      "    num_steps_trained: 964000\n",
      "    sample_time_ms: 59809.87\n",
      "    update_time_ms: 57.102\n",
      "  iterations_since_restore: 482\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.61145833333335\n",
      "    ram_util_percent: 77.92083333333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.78830300874074\n",
      "    mean_inference_ms: 11.829525648693052\n",
      "    mean_processing_ms: 21.561090514182588\n",
      "  time_since_restore: 33154.87569785118\n",
      "  time_this_iter_s: 67.21251010894775\n",
      "  time_total_s: 33154.87569785118\n",
      "  timestamp: 1575798413\n",
      "  timesteps_since_restore: 964000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 964000\n",
      "  training_iteration: 482\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33154 s, 482 iter, 964000 ts, 45.2 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-48-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.92093934509465\n",
      "  episode_reward_mean: 43.270329137531725\n",
      "  episode_reward_min: -13.267613716338525\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9660\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8201.829\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2783114016056061\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6620471477508545\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009458244778215885\n",
      "        policy_loss: -0.017494382336735725\n",
      "        total_loss: 227.96014404296875\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 227.9750518798828\n",
      "    load_time_ms: 5.388\n",
      "    num_steps_sampled: 966000\n",
      "    num_steps_trained: 966000\n",
      "    sample_time_ms: 59634.638\n",
      "    update_time_ms: 56.878\n",
      "  iterations_since_restore: 483\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.36875000000002\n",
      "    ram_util_percent: 78.00208333333333\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.70065323690275\n",
      "    mean_inference_ms: 11.825156399394546\n",
      "    mean_processing_ms: 21.561248755017314\n",
      "  time_since_restore: 33222.31656384468\n",
      "  time_this_iter_s: 67.44086599349976\n",
      "  time_total_s: 33222.31656384468\n",
      "  timestamp: 1575798480\n",
      "  timesteps_since_restore: 966000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 966000\n",
      "  training_iteration: 483\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33222 s, 483 iter, 966000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-49-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.92093934509465\n",
      "  episode_reward_mean: 42.384588469274675\n",
      "  episode_reward_min: -13.267613716338525\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9680\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8492.016\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13915570080280304\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.045032262802124\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007463457062840462\n",
      "        policy_loss: -0.010808372870087624\n",
      "        total_loss: 216.1061248779297\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 216.11585998535156\n",
      "    load_time_ms: 5.406\n",
      "    num_steps_sampled: 968000\n",
      "    num_steps_trained: 968000\n",
      "    sample_time_ms: 59422.556\n",
      "    update_time_ms: 56.038\n",
      "  iterations_since_restore: 484\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.25049504950495\n",
      "    ram_util_percent: 78.10396039603958\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.69671907859836\n",
      "    mean_inference_ms: 11.82509430128116\n",
      "    mean_processing_ms: 21.559539843113193\n",
      "  time_since_restore: 33292.721353292465\n",
      "  time_this_iter_s: 70.40478944778442\n",
      "  time_total_s: 33292.721353292465\n",
      "  timestamp: 1575798551\n",
      "  timesteps_since_restore: 968000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 968000\n",
      "  training_iteration: 484\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33292 s, 484 iter, 968000 ts, 42.4 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-50-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.92093934509465\n",
      "  episode_reward_mean: 43.28812927950814\n",
      "  episode_reward_min: -13.267613716338525\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9700\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8480.466\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06957785040140152\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.4162216186523438\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.013828698545694351\n",
      "        policy_loss: -0.01622197777032852\n",
      "        total_loss: 217.3890380859375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 217.4042510986328\n",
      "    load_time_ms: 5.418\n",
      "    num_steps_sampled: 970000\n",
      "    num_steps_trained: 970000\n",
      "    sample_time_ms: 59341.548\n",
      "    update_time_ms: 55.725\n",
      "  iterations_since_restore: 485\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.56\n",
      "    ram_util_percent: 78.08000000000003\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.0126837985546\n",
      "    mean_inference_ms: 11.844609757964367\n",
      "    mean_processing_ms: 21.57848295364962\n",
      "  time_since_restore: 33359.33621454239\n",
      "  time_this_iter_s: 66.6148612499237\n",
      "  time_total_s: 33359.33621454239\n",
      "  timestamp: 1575798618\n",
      "  timesteps_since_restore: 970000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 970000\n",
      "  training_iteration: 485\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33359 s, 485 iter, 970000 ts, 43.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-51-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.98640099060972\n",
      "  episode_reward_mean: 43.56620450354129\n",
      "  episode_reward_min: -7.7752263299055695\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9720\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8211.014\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.06957785040140152\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.866928815841675\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.13866183161735535\n",
      "        policy_loss: -0.015126466751098633\n",
      "        total_loss: 198.81344604492188\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 198.81895446777344\n",
      "    load_time_ms: 5.377\n",
      "    num_steps_sampled: 972000\n",
      "    num_steps_trained: 972000\n",
      "    sample_time_ms: 59397.287\n",
      "    update_time_ms: 56.797\n",
      "  iterations_since_restore: 486\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.628125\n",
      "    ram_util_percent: 78.01145833333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.0078096445518\n",
      "    mean_inference_ms: 11.844207453087634\n",
      "    mean_processing_ms: 21.577617299635158\n",
      "  time_since_restore: 33426.89110946655\n",
      "  time_this_iter_s: 67.55489492416382\n",
      "  time_total_s: 33426.89110946655\n",
      "  timestamp: 1575798685\n",
      "  timesteps_since_restore: 972000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 972000\n",
      "  training_iteration: 486\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33426 s, 486 iter, 972000 ts, 43.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-52-32\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.98640099060972\n",
      "  episode_reward_mean: 43.453198130656205\n",
      "  episode_reward_min: -7.7752263299055695\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9740\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7921.539\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.10436677932739258\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.240481376647949\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.06053822487592697\n",
      "        policy_loss: 0.019133003428578377\n",
      "        total_loss: 210.14596557617188\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 210.12059020996094\n",
      "    load_time_ms: 3.341\n",
      "    num_steps_sampled: 974000\n",
      "    num_steps_trained: 974000\n",
      "    sample_time_ms: 59683.09\n",
      "    update_time_ms: 50.751\n",
      "  iterations_since_restore: 487\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.94\n",
      "    ram_util_percent: 78.08842105263162\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.07762438476718\n",
      "    mean_inference_ms: 11.845311094540039\n",
      "    mean_processing_ms: 21.581338835523134\n",
      "  time_since_restore: 33493.47173810005\n",
      "  time_this_iter_s: 66.58062863349915\n",
      "  time_total_s: 33493.47173810005\n",
      "  timestamp: 1575798752\n",
      "  timesteps_since_restore: 974000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 974000\n",
      "  training_iteration: 487\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33493 s, 487 iter, 974000 ts, 43.5 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-53-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 88.98640099060972\n",
      "  episode_reward_mean: 45.34185188611654\n",
      "  episode_reward_min: -7.7752263299055695\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9760\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8186.071\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15655016899108887\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.923891067504883\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.029031986370682716\n",
      "        policy_loss: -0.023630822077393532\n",
      "        total_loss: 218.95071411132812\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 218.96981811523438\n",
      "    load_time_ms: 3.377\n",
      "    num_steps_sampled: 976000\n",
      "    num_steps_trained: 976000\n",
      "    sample_time_ms: 59618.041\n",
      "    update_time_ms: 49.744\n",
      "  iterations_since_restore: 488\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.417\n",
      "    ram_util_percent: 78.25000000000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.25599412954176\n",
      "    mean_inference_ms: 11.852937425972952\n",
      "    mean_processing_ms: 21.575370684342083\n",
      "  time_since_restore: 33563.48055744171\n",
      "  time_this_iter_s: 70.00881934165955\n",
      "  time_total_s: 33563.48055744171\n",
      "  timestamp: 1575798822\n",
      "  timesteps_since_restore: 976000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 976000\n",
      "  training_iteration: 488\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33563 s, 488 iter, 976000 ts, 45.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 85.90838883170593\n",
      "  episode_reward_mean: 44.818855925540326\n",
      "  episode_reward_min: -12.652566635619182\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9780\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8235.373\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.15655016899108887\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.7339823246002197\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.1361236572265625\n",
      "        policy_loss: 0.06299866735935211\n",
      "        total_loss: 226.8639373779297\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 226.77964782714844\n",
      "    load_time_ms: 3.428\n",
      "    num_steps_sampled: 978000\n",
      "    num_steps_trained: 978000\n",
      "    sample_time_ms: 59670.894\n",
      "    update_time_ms: 51.237\n",
      "  iterations_since_restore: 489\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.14999999999999\n",
      "    ram_util_percent: 78.32395833333334\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.81455691926928\n",
      "    mean_inference_ms: 11.884527376149972\n",
      "    mean_processing_ms: 21.591898831953458\n",
      "  time_since_restore: 33630.93964886665\n",
      "  time_this_iter_s: 67.45909142494202\n",
      "  time_total_s: 33630.93964886665\n",
      "  timestamp: 1575798889\n",
      "  timesteps_since_restore: 978000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 978000\n",
      "  training_iteration: 489\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33630 s, 489 iter, 978000 ts, 44.8 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-55-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.33175307217213\n",
      "  episode_reward_mean: 42.68962416400654\n",
      "  episode_reward_min: -15.325522262388589\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9800\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7899.257\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2348252534866333\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.147029399871826\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009541207924485207\n",
      "        policy_loss: -0.012873392552137375\n",
      "        total_loss: 246.2548370361328\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 246.26544189453125\n",
      "    load_time_ms: 3.46\n",
      "    num_steps_sampled: 980000\n",
      "    num_steps_trained: 980000\n",
      "    sample_time_ms: 59946.996\n",
      "    update_time_ms: 52.477\n",
      "  iterations_since_restore: 490\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 97.633\n",
      "    ram_util_percent: 78.399\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.40426996847256\n",
      "    mean_inference_ms: 11.855426685629936\n",
      "    mean_processing_ms: 21.564478556590828\n",
      "  time_since_restore: 33700.53604745865\n",
      "  time_this_iter_s: 69.59639859199524\n",
      "  time_total_s: 33700.53604745865\n",
      "  timestamp: 1575798959\n",
      "  timesteps_since_restore: 980000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 980000\n",
      "  training_iteration: 490\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33700 s, 490 iter, 980000 ts, 42.7 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-57-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.33175307217213\n",
      "  episode_reward_mean: 44.319054187310115\n",
      "  episode_reward_min: -15.325522262388589\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9820\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8182.12\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.11741262674331665\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3934884071350098\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.2530965209007263\n",
      "        policy_loss: 0.05911115184426308\n",
      "        total_loss: 219.63446044921875\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 219.54566955566406\n",
      "    load_time_ms: 2.953\n",
      "    num_steps_sampled: 982000\n",
      "    num_steps_trained: 982000\n",
      "    sample_time_ms: 59968.767\n",
      "    update_time_ms: 52.609\n",
      "  iterations_since_restore: 491\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.5959595959596\n",
      "    ram_util_percent: 78.3040404040404\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.7557817004146\n",
      "    mean_inference_ms: 11.87503823444663\n",
      "    mean_processing_ms: 21.57764980522592\n",
      "  time_since_restore: 33770.197234869\n",
      "  time_this_iter_s: 69.66118741035461\n",
      "  time_total_s: 33770.197234869\n",
      "  timestamp: 1575799029\n",
      "  timesteps_since_restore: 982000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 982000\n",
      "  training_iteration: 491\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33770 s, 491 iter, 982000 ts, 44.3 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-58-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.33175307217213\n",
      "  episode_reward_mean: 43.02888535657229\n",
      "  episode_reward_min: -15.325522262388589\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9840\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8254.305\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17611894011497498\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.137784719467163\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011353868059813976\n",
      "        policy_loss: -0.007929937914013863\n",
      "        total_loss: 250.18882751464844\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 250.19479370117188\n",
      "    load_time_ms: 4.376\n",
      "    num_steps_sampled: 984000\n",
      "    num_steps_trained: 984000\n",
      "    sample_time_ms: 59755.989\n",
      "    update_time_ms: 53.119\n",
      "  iterations_since_restore: 492\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 78.37340425531913\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 142.21129763259032\n",
      "    mean_inference_ms: 11.907328511305154\n",
      "    mean_processing_ms: 21.598445577940993\n",
      "  time_since_restore: 33835.97647356987\n",
      "  time_this_iter_s: 65.7792387008667\n",
      "  time_total_s: 33835.97647356987\n",
      "  timestamp: 1575799094\n",
      "  timesteps_since_restore: 984000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 984000\n",
      "  training_iteration: 492\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33835 s, 492 iter, 984000 ts, 43 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_04-59-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.33175307217213\n",
      "  episode_reward_mean: 41.145643254881044\n",
      "  episode_reward_min: -24.526693350053712\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9860\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8257.166\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.17611894011497498\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6537303924560547\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.18201856315135956\n",
      "        policy_loss: 0.03877080976963043\n",
      "        total_loss: 225.39598083496094\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 225.32518005371094\n",
      "    load_time_ms: 4.386\n",
      "    num_steps_sampled: 986000\n",
      "    num_steps_trained: 986000\n",
      "    sample_time_ms: 59740.341\n",
      "    update_time_ms: 52.207\n",
      "  iterations_since_restore: 493\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.36458333333333\n",
      "    ram_util_percent: 78.42812500000001\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.49281967635198\n",
      "    mean_inference_ms: 11.865685619192494\n",
      "    mean_processing_ms: 21.57375789354086\n",
      "  time_since_restore: 33903.2898516655\n",
      "  time_this_iter_s: 67.31337809562683\n",
      "  time_total_s: 33903.2898516655\n",
      "  timestamp: 1575799162\n",
      "  timesteps_since_restore: 986000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 986000\n",
      "  training_iteration: 493\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33903 s, 493 iter, 986000 ts, 41.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-00-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 91.33175307217213\n",
      "  episode_reward_mean: 41.87486168828751\n",
      "  episode_reward_min: -24.526693350053712\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9880\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 7958.799\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.26417839527130127\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6561977863311768\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0124458447098732\n",
      "        policy_loss: -0.015039273537695408\n",
      "        total_loss: 231.52825927734375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 231.5399627685547\n",
      "    load_time_ms: 4.454\n",
      "    num_steps_sampled: 988000\n",
      "    num_steps_trained: 988000\n",
      "    sample_time_ms: 59651.273\n",
      "    update_time_ms: 52.48\n",
      "  iterations_since_restore: 494\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59473684210526\n",
      "    ram_util_percent: 78.44526315789474\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 140.98897505469304\n",
      "    mean_inference_ms: 11.837895149118035\n",
      "    mean_processing_ms: 21.55691131665918\n",
      "  time_since_restore: 33969.8240506649\n",
      "  time_this_iter_s: 66.53419899940491\n",
      "  time_total_s: 33969.8240506649\n",
      "  timestamp: 1575799228\n",
      "  timesteps_since_restore: 988000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 988000\n",
      "  training_iteration: 494\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 33969 s, 494 iter, 988000 ts, 41.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-01-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.29900571776318\n",
      "  episode_reward_mean: 43.01606956394305\n",
      "  episode_reward_min: -24.526693350053712\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9900\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8287.853\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.26417839527130127\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.3870933055877686\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.007712013088166714\n",
      "        policy_loss: -0.015798956155776978\n",
      "        total_loss: 215.16082763671875\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 215.17457580566406\n",
      "    load_time_ms: 4.515\n",
      "    num_steps_sampled: 990000\n",
      "    num_steps_trained: 990000\n",
      "    sample_time_ms: 59705.335\n",
      "    update_time_ms: 52.77\n",
      "  iterations_since_restore: 495\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.035\n",
      "    ram_util_percent: 78.585\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.25618296487173\n",
      "    mean_inference_ms: 11.855486087690242\n",
      "    mean_processing_ms: 21.564175851905688\n",
      "  time_since_restore: 34040.2758204937\n",
      "  time_this_iter_s: 70.45176982879639\n",
      "  time_total_s: 34040.2758204937\n",
      "  timestamp: 1575799299\n",
      "  timesteps_since_restore: 990000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 990000\n",
      "  training_iteration: 495\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 34040 s, 495 iter, 990000 ts, 43 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-02-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 87.29900571776318\n",
      "  episode_reward_mean: 41.62424101481066\n",
      "  episode_reward_min: -24.526693350053712\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9920\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8511.46\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.13208919763565063\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.413194179534912\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.0676737055182457\n",
      "        policy_loss: 0.015436380170285702\n",
      "        total_loss: 200.38958740234375\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 200.36521911621094\n",
      "    load_time_ms: 6.067\n",
      "    num_steps_sampled: 992000\n",
      "    num_steps_trained: 992000\n",
      "    sample_time_ms: 59317.656\n",
      "    update_time_ms: 53.517\n",
      "  iterations_since_restore: 496\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 100.0\n",
      "    ram_util_percent: 78.7223404255319\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.30362918087653\n",
      "    mean_inference_ms: 11.858650218273388\n",
      "    mean_processing_ms: 21.563442255035316\n",
      "  time_since_restore: 34106.20701766014\n",
      "  time_this_iter_s: 65.93119716644287\n",
      "  time_total_s: 34106.20701766014\n",
      "  timestamp: 1575799365\n",
      "  timesteps_since_restore: 992000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 992000\n",
      "  training_iteration: 496\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 34106 s, 496 iter, 992000 ts, 41.6 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-03-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 86.02619557976871\n",
      "  episode_reward_mean: 43.08455114363106\n",
      "  episode_reward_min: -24.526693350053712\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9940\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8504.201\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.19813381135463715\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.141747236251831\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.1140773668885231\n",
      "        policy_loss: 0.03502427414059639\n",
      "        total_loss: 194.42864990234375\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 194.37103271484375\n",
      "    load_time_ms: 6.113\n",
      "    num_steps_sampled: 994000\n",
      "    num_steps_trained: 994000\n",
      "    sample_time_ms: 59408.738\n",
      "    update_time_ms: 54.368\n",
      "  iterations_since_restore: 497\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.66391752577319\n",
      "    ram_util_percent: 78.59381443298969\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.1019272253285\n",
      "    mean_inference_ms: 11.843083711622432\n",
      "    mean_processing_ms: 21.54790658377918\n",
      "  time_since_restore: 34173.64934921265\n",
      "  time_this_iter_s: 67.4423315525055\n",
      "  time_total_s: 34173.64934921265\n",
      "  timestamp: 1575799432\n",
      "  timesteps_since_restore: 994000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 994000\n",
      "  training_iteration: 497\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 34173 s, 497 iter, 994000 ts, 43.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-05-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.91144342402252\n",
      "  episode_reward_mean: 42.926171817147754\n",
      "  episode_reward_min: -11.505472649135383\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9960\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8240.972\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.2972007095813751\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 3.0981228351593018\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.009976359084248543\n",
      "        policy_loss: -0.010160344652831554\n",
      "        total_loss: 202.53488159179688\n",
      "        vf_explained_var: 5.960464477539063e-08\n",
      "        vf_loss: 202.5420684814453\n",
      "    load_time_ms: 6.428\n",
      "    num_steps_sampled: 996000\n",
      "    num_steps_trained: 996000\n",
      "    sample_time_ms: 59406.86\n",
      "    update_time_ms: 56.127\n",
      "  iterations_since_restore: 498\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.59479166666667\n",
      "    ram_util_percent: 78.67291666666667\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.92373775928615\n",
      "    mean_inference_ms: 11.892297254012085\n",
      "    mean_processing_ms: 21.590972420943462\n",
      "  time_since_restore: 34241.030432224274\n",
      "  time_this_iter_s: 67.3810830116272\n",
      "  time_total_s: 34241.030432224274\n",
      "  timestamp: 1575799500\n",
      "  timesteps_since_restore: 996000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 996000\n",
      "  training_iteration: 498\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 34241 s, 498 iter, 996000 ts, 42.9 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-06-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.91144342402252\n",
      "  episode_reward_mean: 45.12654226284929\n",
      "  episode_reward_min: -9.494040040873568\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 9980\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8505.507\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.14860035479068756\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.6134886741638184\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.020202498883008957\n",
      "        policy_loss: -0.010931429453194141\n",
      "        total_loss: 204.96449279785156\n",
      "        vf_explained_var: -1.1920928955078125e-07\n",
      "        vf_loss: 204.97242736816406\n",
      "    load_time_ms: 6.45\n",
      "    num_steps_sampled: 998000\n",
      "    num_steps_trained: 998000\n",
      "    sample_time_ms: 59484.881\n",
      "    update_time_ms: 55.739\n",
      "  iterations_since_restore: 499\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.1118811881188\n",
      "    ram_util_percent: 78.7950495049505\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.86700735670786\n",
      "    mean_inference_ms: 11.888516060755988\n",
      "    mean_processing_ms: 21.58904592284085\n",
      "  time_since_restore: 34311.907148599625\n",
      "  time_this_iter_s: 70.87671637535095\n",
      "  time_total_s: 34311.907148599625\n",
      "  timestamp: 1575799571\n",
      "  timesteps_since_restore: 998000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 998000\n",
      "  training_iteration: 499\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 8/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.6/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'RUNNING': 1})\n",
      "RUNNING trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tRUNNING, [8 CPUs, 0 GPUs], [pid=23396], 34311 s, 499 iter, 998000 ts, 45.1 rew\n",
      "\n",
      "Result for PPO_LaneChangeAccelEnv1-v0_0:\n",
      "  custom_metrics: {}\n",
      "  date: 2019-12-08_05-07-19\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_reward_max: 89.91144342402252\n",
      "  episode_reward_mean: 44.294161961400384\n",
      "  episode_reward_min: -9.494040040873568\n",
      "  episodes_this_iter: 20\n",
      "  episodes_total: 10000\n",
      "  experiment_id: 1b1c2dbd7f554809834cc3d9e101cde6\n",
      "  hostname: osboxes\n",
      "  info:\n",
      "    grad_time_ms: 8513.999\n",
      "    learner:\n",
      "      default_policy:\n",
      "        cur_kl_coeff: 0.14860035479068756\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 2.9328548908233643\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.23098483681678772\n",
      "        policy_loss: 0.07624702155590057\n",
      "        total_loss: 206.96206665039062\n",
      "        vf_explained_var: 0.0\n",
      "        vf_loss: 206.85150146484375\n",
      "    load_time_ms: 6.423\n",
      "    num_steps_sampled: 1000000\n",
      "    num_steps_trained: 1000000\n",
      "    sample_time_ms: 59385.917\n",
      "    update_time_ms: 53.985\n",
      "  iterations_since_restore: 500\n",
      "  node_ip: 192.168.1.188\n",
      "  num_healthy_workers: 7\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 98.10510204081632\n",
      "    ram_util_percent: 78.89897959183673\n",
      "  pid: 23396\n",
      "  policy_reward_mean: {}\n",
      "  sampler_perf:\n",
      "    mean_env_wait_ms: 141.69085640401846\n",
      "    mean_inference_ms: 11.880737052428822\n",
      "    mean_processing_ms: 21.586496291304087\n",
      "  time_since_restore: 34380.57443737984\n",
      "  time_this_iter_s: 68.6672887802124\n",
      "  time_total_s: 34380.57443737984\n",
      "  timestamp: 1575799639\n",
      "  timesteps_since_restore: 1000000\n",
      "  timesteps_this_iter: 2000\n",
      "  timesteps_total: 1000000\n",
      "  training_iteration: 500\n",
      "  trial_id: 42ddae0c\n",
      "  \n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tTERMINATED, [8 CPUs, 0 GPUs], [pid=23396], 34380 s, 500 iter, 1000000 ts, 44.3 rew\n",
      "\n",
      "== Status ==\n",
      "Using FIFO scheduling algorithm.\n",
      "Resources requested: 0/8 CPUs, 0/0 GPUs\n",
      "Memory usage on this node: 6.5/8.3 GB\n",
      "Result logdir: /home/osboxes/ray_results/FigureEightNetwork\n",
      "Number of trials: 1 ({'TERMINATED': 1})\n",
      "TERMINATED trials:\n",
      " - PPO_LaneChangeAccelEnv1-v0_0:\tTERMINATED, [8 CPUs, 0 GPUs], [pid=23396], 34380 s, 500 iter, 1000000 ts, 44.3 rew\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 1,  # number of iterations between checkpoints\n",
    "        \"checkpoint_at_end\": True,  # generate a checkpoint at the end\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {  # stopping conditions\n",
    "            \"training_iteration\": 500,  # number of iterations to stop after\n",
    "        },\n",
    "    },\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
